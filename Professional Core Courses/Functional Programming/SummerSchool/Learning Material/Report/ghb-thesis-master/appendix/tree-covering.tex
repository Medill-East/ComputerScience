% Copyright (c) 2017-2018, Gabriel Hjort Blindell <ghb@kth.se>
%
% This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0
% International License (see LICENSE file or visit
% <http://creativecommons.org/licenses/by-nc-nd/4.0/> for details).

\chapter{Tree Covering}
\labelAppendix{tree-covering}

This appendix considers techniques based on \gls{tree covering}, which is the
most common \gls{principle} of techniques found in the current literature.
%
First, we introduce the principle in \refSection{tc-principle}.
%
We then describe the first \gls{tree}-based approaches in
\refSection{first-techniques-to-use-tree-pattern-matching}.
%
In \refSection{tc-lr-parsing-based-approaches} we describe parser-based
approaches, where methods typically used for \gls{syntactic analysis} is
reinstrumented for \gls{instruction selection}.
%
The techniques thus far are all bottom-up-oriented, and in
\refSection{tc-top-down-approaches} we describe the first top-down-oriented
approaches.
%
In \refSection{tc-separating-matching-and-selection} we describe the first
techniques that separate the \glsshort{matching problem} and \glspl{selection
  problem}, allowing the latter to be solved \gls{optimal.ps}[ly].
%
In \refSection{tc-other-tree-based-approaches} we describe other
\gls{tree}-based approaches that do not fit into any of the sections above.
%
Limitations of this principle are discussed in \refSection{tc-limitations}, and
we summarize in \refSection{tc-summary}.

The appendix is based on material presented in
\cite[Chap.\thinspace3]{HjortBlindell:2016:Survey} that has been adapted for
this dissertation.
%
To not disturb the flow of reading, material already presented in
\refChapter{existing-isel-techniques-and-reps} is duplicated in this appendix.


\section{The Principle}
\labelSection{tc-principle}

As we saw in \refAppendix{macro-expansion} on \refPageOfSection{me-limitations},
the main limitation of most \glspl{instruction selector} based on \gls{macro
  expansion} is that the scope of expansion is restricted to a single \gls{AST}
or \gls{IR} \gls{node}.
%
Hence exploitation of many \glspl{instruction} is excluded, resulting in low
code quality.
%
Another problem is that \gls{macro}-expanding \glspl{instruction selector}
typically combine \glsshort{matching problem} and \glsshort{selection problem}
into a single step, thus making it very difficult to consider combinations of
\glspl{instruction} and then pick the one that yields the best \gls{assembly
  code}.
%
These problems can be addressed by employing \gls{tree covering}.

First, the \gls{IR} code is transformed into an \gls{expression tree}, as we saw
in \refAppendix{macro-expansion} (see \refSection{me-expression-tree} on
\refPageOfSection{me-expression-tree}).
%
Corresponding \glspl{data-flow graph}, called \glspl!{pattern tree}, are also
built to represent the \gls{instruction} provided by the \gls{target machine}.
%
When the shape is clear from the context, they are simply called
\glspl!{pattern}.
%
The set of \glspl{pattern} for a particular \gls{target machine} constitute a
\gls!{pattern set}.

The \gls{matching problem} can be reduced to finding all instances where a
\gls{pattern} from the \gls{pattern set} is \gls{subgraph} isomorphic to the
\gls{expression tree}.
%
Each such instance is called a \gls!{match}, and the set of all \glspl{match}
constitute a \gls!{match set}.
%
Hence, in this context the \gls{matching problem} is referred to as
\gls!{pattern matching}.
%
In many contexts, \glspl{match} and \glspl{pattern} can be used interchangeably.

Having found the \gls{match set}, the \gls{selection problem} -- which in this
context is referred to as \gls!{pattern selection} -- can be reduced to
selecting a set of \glspl{match} that \gls{cover}[s] the \gls{expression tree}.
%
A subset~\mbox{$M' \subseteq M$}, where $M$ is a \gls{match set},
\gls!{cover}[s] a \gls{data-flow graph}~$G$, derived from a \gls{function}, if
every \gls{node} in $G$ appears in at least one match in~$M'$\!.
%
Such a subset is called a \gls!{cover}.
%
A \gls{cover} is an \gls!{exact.c}[ \gls{cover}] if every \gls{node} in $G$
appears in exactly one match in the \gls{cover}.
%
Most \gls{instruction selection} approaches assume \gls{exact.c}
\gls{cover}[age].
%
Examples of \glspl{cover} are shown in \refFigure{p-match-sel-example-2}.

\begin{filecontents*}{p-match-sel-example.c}
x = A[i + 1];
\end{filecontents*}

\begin{figure}
  \centering%
  \subcaptionbox{C code\labelFigure{p-match-sel-example-2-c}}%
                {%
                  \begin{lstpage}{2.6cm}%
                    \lstinputlisting[language=c]{p-match-sel-example.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \subcaptionbox{%
                  Instructions.
                  %
                  The $*s$ \mbox{notation} means ``get value at address $s$ in
                  memory''%
                  \labelFigure{p-match-sel-example-2-instrs}%
                }{%
                  \figureFontSize%
                  \begin{tabular}{%
                                   >{\instrFont{}}r@{\hspace{4pt}}%
                                   >{$}l<{$}@{ $\leftarrow$ }%
                                   >{$}l<{$}%
                                 }
                    \toprule
                    mv     & r & \mathit{var}\\
                    add    & r & s + t\\
                    mul    & r & s \times t\\
                    muladd & r & s \times t + u\\
                    load   & r & *s\\
                    maload & r & *(s \times t + u)\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \subcaptionbox{%
                  Expression tree and its matches%
                  \labelFigure{p-match-sel-example-2-tree}%
                }{%
                  \input{figures/tree-covering/p-match-sel-example-tree}%
                }

  \caption[Example of the pattern matching and selection problem]%
          {%
            Example demonstrating the pattern matching and selection problem for
            a function that loads a value from integer array \irVar*{A} at
            offset \mbox{\irVar*{i} \irCode*{\irAddText{}} \irVar*{1}}.
            %
            It is assumed that \irVar*{i} is stored in register, that \irVar*{A}
            is stored in memory, and that an integer is four~bytes.
            %
            Exact covers are \mbox{$\mSet{m_1, \ldots, m_7, m_9}$},
            \mbox{$\mSet{m_1, \ldots, m_5, m_8, m_9}$}, \mbox{$\mSet{m_1,
                \ldots, m_5, m_{10}}$}, \mbox{$\mSet{m_1, \ldots, m_5, m_8,
                m_9}$} (for brevity, non-exact covers are ignored).
            %
            Variable assignments need not be explicitly represented as nodes
            since this information can be propagated from the root node after
            having found a cover%
          }
  \labelFigure{p-match-sel-example-2}
\end{figure}

For most \glspl{target machine} there will be a tremendous amount of overlap
among the \glspl{pattern}, meaning that one \gls{pattern} may match (either
partially or fully) the \glspl{node} matched by another \gls{pattern} in the
\gls{expression tree}.
%
Typically we want to use as few \glspl{pattern} as possible to cover the
\gls{expression tree}.
%
This is for two reasons:
%
\begin{itemize}
  \item Striving for the smallest number of \glspl{pattern} means favoring
    larger \glspl{pattern} over smaller ones.
    %
    This in turn leads to the use of more complex \glspl{instruction} which
    typically yield higher code quality.
  \item The amount of overlap between the selected \glspl{pattern} is limited,
    which means that the same values will be computed multiple times only when
    necessary.
    %
    Keeping redundant work to a minimum is another crucial factor for
    performance as well as for reducing code size.
\end{itemize}

In general, an \gls!{optimal.ps} solution to the \gls{pattern selection} problem
is not defined as the one that minimizes the \emph{number} of selected
\glspl{pattern}, but as the one that minimizes the \emph{total cost} of the
selected \glspl{pattern}.
%
This allows the \gls{pattern} costs to be chosen such that they fit the desired
optimization criteria, although there is usually a strong correlation between
the number of \glspl{pattern} and the total cost.
%
Note, however, that an \gls{optimal.ps} solution to the \gls{pattern selection}
problem need not necessarily be an \gls{optimal.ps} solution for the final
\gls{assembly code}.

Finding the \gls{optimal.ps} solution to a \gls{pattern selection} problem is
not a trivial task, and it becomes even less so if only certain combinations of
\glspl{pattern} are allowed.
%
To be sure, most would be hard-pressed just to come up with an efficient method
that finds all valid \glspl{match} of the entire \gls{pattern set}.
%
We therefore begin by exploring the first methods that address the \gls{pattern
  matching} problem, but do not necessarily address the \gls{pattern selection}
problem, and then gradually transition to those that do.


\section{First Techniques to Use Tree-Based Pattern Matching}
\labelSection{first-techniques-to-use-tree-pattern-matching}

In 1972 and 1973, the first \gls{code generation} techniques known to use
\gls{tree}-based \gls{pattern matching} were introduced by
\textcite{Wasilew:1972} and \textcite{Weingart:1973}, respectively.
%
Unfortunately only \citeauthor{Weingart:1973}'s work appears to be recognized by
other literature, even though \citeauthor{Wasilew:1972}'s ideas have more in
common with later \gls{tree}-based \gls{instruction selection} techniques.
%
We will briefly cover both in this dissertation, as described by
\textcite{Lunell:1983}, who gives a more detailed account in his doctoral
dissertation.


\paragraph{Wasilew's Design}

To begin with, \citeauthor{Wasilew:1972} devised an \glsdesc{IR} where the
\glspl{function} are represented using \gls!{postfix notation} (or
\gls!{reverse.pn}[ \gls{Polish notation}] as this is also called; we will
discuss more on \gls{Polish notation} in \refSection{tc-glanville-graham}).
%
An example is shown in \refFigure{wasilew-code-example}.
%
\begin{filecontents*}{wasilew-code-example.c}
AWAY m YHPASS assign
K AMA m PMFI 7 - assign
Z K AMA m ANS assign assign
X 8 + m HEAD X 6 + m I1 + m X 6 + m I2 + m assign
X Y FR AA transfer assign
X INC if-AZ BB transfer
OR m MAJ 4FCOID 4FCOIN if-equal2 1 J + transfer
\end{filecontents*}
%
\begin{figure}
  \centering%
  \begin{lstpage}{86.5mm}
    \lstset{
      keywordstyle=\bfseries,
      keywords={assign, m, transfer},
      otherkeywords={if-AZ, if-equal2, -, +},
    }%
    \lstinputlisting{wasilew-code-example.c}%
  \end{lstpage}

  \caption[%
            Example of a function expressed using \citeauthor{Wasilew:1972}'s
            IR%
          ]%
          {%
            Example of a function expressed using \citeauthor{Wasilew:1972}'s
            IR~\cite{Lunell:1983}%
          }
  \labelFigure{wasilew-code-example}
\end{figure}
%
\citeauthor{Wasilew:1972} also developed his own programming language, which is
transformed into \gls{IR} code as part of compilation.
%
The \glspl{instruction} of the \gls{target machine} are described in a table,
where each \gls{instruction} comprises execution time and code size information,
a string constituting the \gls{assembly code}, and the \gls{pattern} to be
matched against the \gls{function}.
%
For each line in the \gls{function}, \gls{pattern matching} is done starting at
a \gls{leaf} in the \gls{tree} corresponding to the current line.
%
For this \gls{subtree}, all \glspl{match} are found by comparing it against all
\glspl{pattern} in the \gls{pattern set}.
%
The \gls{subtree} is then grown to include its \gls{parent}, and the new
\gls{subtree} is again compared against the \glspl{pattern}.
%
This continues until no new \glspl{match} are found.
%
Once the largest \gls{match} has been found, the \gls{subtree} is replaced with
the result of the \gls{pattern}, and the process is repeated for the remaining
parts in the \gls{tree}.
%
If multiple largest \glspl{match} are found for any \gls{subtree}, the process
is repeated for each such \gls{match}.
%
This results in an exhaustive search that finds all combinations of
\glspl{pattern} for a given \gls{tree}.
%
Once all combinations have been found, the cheapest combination -- whose cost is
based on the \glspl{instruction}' execution time and code size -- is selected.

Compared to the early macro-expanding \glspl{instruction selector} (at least
those prior to \glsshort{Davidson-Fraser approach}), \citeauthor{Wasilew:1972}'s
design had a more extensive \gls{instruction} support as it could include
\glspl{pattern} that extend over multiple\gls{IR} \glspl{node}.
%
However, its exhaustive nature makes it considerably more expensive in terms of
compilation time.
%
In addition, the notations used by \citeauthor{Wasilew:1972} are difficult to
read and write.


\paragraph{Weingart's Design}

In comparison to \citeauthor{Wasilew:1972}, \citeauthor{Weingart:1973}'s design
is centered around a single \gls{tree} of \glspl{pattern} --
\citeauthor{Weingart:1973} called this a \gls!{discrimination net} -- which is
automatically derived from a declarative \gls{machine description}.
%
Using a single \gls{tree} of \glspl{pattern}, \citeauthor{Weingart:1973} argued,
allows for a compact and efficient means of representing the \gls{pattern set}.
%
The process of building the \gls{AST} is then extended to simultaneously push
each new \gls{AST} \gls{node} onto a stack.
%
In tandem, the \gls{discrimination net} is progressively traversed by comparing
the \glspl{node} on the stack against the \glspl{child} of the current
\gls{node} in the net.
%
A \gls{match} is found when the process reaches a \gls{leaf} in the
\gls{discrimination net}, whereupon the \gls{instruction} associated with the
\gls{match} is emitted.

Like \citeauthor{Wasilew:1972}'s design, \citeauthor{Weingart:1973}'s had a more
extensive \gls{instruction} support compared to the contemporary techniques as
it could include \glspl{pattern} extending over multiple \gls{AST} \glspl{node}.
%
However, when applied in practice, the design suffered from several problems.
%
First, structuring the \gls{discrimination net} to support efficient
\gls{pattern matching} proved difficult for certain \glspl{target machine}; it
is known that \citeauthor{Weingart:1973} struggled in particular with the
\gls{PDP-11}.
%
Second, the design assumes that there exists at least one \gls{instruction} on
the \gls{target machine} that corresponds to a particular \gls{node} type of
the~\gls{AST}, which turned out to not always be the case.
%
\citeauthor{Weingart:1973} partly addressed this problem by introducing
\glspl!{conversion pattern}, which could transform mismatched parts of the
\gls{AST} into another form that hopefully would be matched by some
\gls{pattern} at a later stage.
%
However, these had to be added manually and could potentially cause the
\gls{compiler} to get stuck in an infinite loop.
%
Third, like its \glshyphened{macro expander}[ding] predecessors, the process
immediately selects a \gls{pattern} as soon as a \gls{match} is found.


\paragraph{PCC}

Another early \gls{pattern matching} technique was developed by
\textcite{Johnson:1978}, which was implemented in the \gls!{PCC} -- a renowned
system that was the first standard \gls{C}~\gls{compiler} to be shipped with
\gls{Unix}.
%
\citeauthor{Johnson:1978} based his design on the earlier work by
\textcite{Snyder:1975} (which we discussed in
\refSection{separating-macros-and-machine-description}), but replaced the use of
\gls{macro expansion} with a method that performs \gls{tree rewriting}.
%
For each \gls{instruction}, a \gls{expression tree} is formed together with a
\gls{rewrite.r} \gls{rule}, subgoals, resource requirements, and an assembly
string which is emitted verbatim.
%
This information is given in a \gls{machine description} format that allows
multiple, similar \glspl{pattern} to be condensed into a single declaration.
%
An example is shown in \refFigure{pcc-machine-description-sample}.

\begin{filecontents*}{pcc-machine-description-sample.c}
ASG PLUS,   INAREG,
            SAREG,   TINT,
            SNAME,   TINT,
                     0,      RLEFT,
                     "       add      AL,AR\n",
...
ASG OPSIM,   INAREG|FORCC,
             SAREG,      TINT|TUNSIGNED|TPOINT,
             SAREG|SNAME|SOREG|SCON,   TINT|TUNSIGNED|TPOINT,
                         0,            RLEFT|RESCC
                         "             OI            AL,AR\n"
\end{filecontents*}

\begin{figure}
  \centering%
  \begin{lstpage}{106.5mm}%
    \lstinputlisting{pcc-machine-description-sample.c}%
  \end{lstpage}

  \caption[Example of a machine description for \glsentrytext{PCC}]%
          {%
            A machine description sample for \glsentrytext{PCC}, consisting of
            two patterns~\cite{Johnson:1981}.
            %
            The first line specifies the node type of the root (\cCode*{+=}, for
            the first pattern) together with its cookie (``result must appear in
            an \cCode*{A}-type register'').
            %
            The second and third lines specify the left and right descendants,
            respectively, of the root.
            %
            The left subtree of the first pattern must be an \cCode*{int}
            allocated in an \cCode*{A}-type register, and the right subtree must
            be a \cCode*{NAME} node, also of type \cCode*{int}.
            %
            The fourth line indicates that no registers or temporaries are
            required and that the matched part in the expression tree is to be
            replaced by the left descendant of the pattern's root.
            %
            The fifth and last line declares the assembly string, where
            lowercase letters are output verbatim and uppercase words indicate a
            macro invocation -- \cCode*{AL} stands for ``Address form of Left
            operand'', and likewise for \cCode*{AR} -- whose result is then put
            into the assembly string.
            %
            In the second pattern we see that multiple restrictions can be
            or'ed together, thus allowing multiple patterns to be
            expressed in a more concise manner%
          }
          \labelFigure{pcc-machine-description-sample}
\end{figure}

The \gls{pattern matching} process is then relatively straightforward.
%
For a given \gls{node} in the \gls{expression tree}, the \gls{node} is compared
against the \gls{root} of each \gls{pattern}.
%
If these match, a similar check is done for each corresponding \gls{subtree} in
the \gls{pattern}.
%
Once all \glspl{leaf} in the \gls{pattern} are reached, a \gls{match} has been
found.
%
As this algorithm -- whose pseudo-code is given in
\refAlgorithm{tree-based-pattern-matching} -- exhibits quadratic time
complexity, it is desirable to minimize the number of redundant checks.
%
\begin{algorithm}[t]
  \DeclFunction{FindMatchSet}%
               {expression tree $T$\!, pattern set $P$}%
  {%
    $M$ \Assign array of size $|T|$, initialized to $\mEmptySet$\;
    \ForEach{node $n_T \in T$}{%
      \ForEach{pattern $p \in P$}{%
        $n_P$ \Assign root node of $P$\;
        \If{\Call{Matches}{$n$, $n_P$}}{%
          $M[n_T]$ \Assign $M[n_T] \cup \mSet{p}$\;
        }
      }
    }
    \Return{$M$}\;

    \BlankLine
    \DeclFunction{IsMatch}{expression tree rooted at node $n_T$,
                           pattern tree rooted at node $n_P$}{%
      $|n_T|$ \Assign number of children for $n_T$\;
      $|n_P|$ \Assign number of children for $n_P$\;
      \If{$n_T \mNodeMatchRel n_P$ \And $|n_T| = |n_P|$}{%
        \ForEach{child $n_T', n_P'$ of $n_T, n_P$}{%
          \If{\Not \Call{IsMatch}{$n_T'$, $n_P'$}}{%
            \Return{false}\;
          }
        }
        \Return{true}\;
      }
      \Else{%
        \Return{false}\;
      }
    }
  }

  \caption[Straightforward algorithm for pattern-matching trees]%
          {%
            A straightforward algorithm for pattern-matching trees.
            %
            The algorithm has \mbox{$\mBigO(n^2 p)$} time complexity, where $n$
            is the number of nodes in the expression tree and $p$ is the number
            patterns in the pattern set.
            %
            The relation $n_1 \mNodeMatchRel n_2$ holds for two nodes~$n_1$
            and~$n_2$ if both are of the same type%
          }
    \labelAlgorithm{tree-based-pattern-matching}
\end{algorithm}
%
This is done by maintaining a set of \gls{code generation} goals which are
encoded into the \gls{instruction selector} as an integer.
%
For historical reasons this integer is called a \gls!{cookie}, and each
\gls{pattern} has a corresponding \gls{cookie} indicating the situations in
which the \gls{pattern} may be useful.
%
If both the \glspl{cookie} and the \gls{pattern} match, an attempt is made to
allocate whatever resources are demanded by the \gls{pattern} (for example, a
\gls{pattern} may require a certain number of \glspl{register}).
%
If successful, the corresponding assembly string is emitted, and the matched
\gls{subtree} in the \gls{expression tree} is replaced by a single \gls{node} as
specified by the \gls{rewrite.r} \gls{rule}.
%
This process of matching and rewriting repeats until the \gls{expression tree}
consists of only a single \gls{node}, meaning that the entire \gls{expression
  tree} has been successfully converted into \gls{assembly code}.
%
If no \gls{pattern} matches, the \gls{instruction selector} enters a heuristic
mode where the \gls{expression tree} is partially rewritten until a \gls{match}
is found.
%
For example, to match an \cCode*{a = reg + b} \gls{pattern}, an \cCode*{a += b}
expression could first be rewritten into \cCode*{a = a + b} and then another
\gls{rule} could try to force operand \cCode*{a} into a \gls{register}.

Although successful for its time, \gls{PCC} had several disadvantages.
%
Like \citeauthor{Weingart:1973}, \citeauthor{Johnson:1981} used heuristic
\gls{rewrite.r} \glspl{rule} to handle mismatching situations.
%
Without formal methods of verification there was always the risk that the
current set of \glspl{rule} would be inadequate and potentially cause the
\gls{compiler} to never terminate for certain \glspl{function}.
%
\textcite{Reiser:1981} also noted that the investigated version of \gls{PCC}
only supported unary and binary \glspl{pattern} with a maximum height
of~\num{1}, thus excluding many \glspl{instruction}, such as those with complex
\glspl{addressing mode}.
%
Lastly, \gls{PCC} -- and all other techniques discussed so far -- still adhered
to the \emph{first-matched-first-served} approach when selecting
\glspl{pattern}.


\section{Using LR Parsing to Cover Trees Bottom-Up}
\labelSection{tc-lr-parsing-based-approaches}

As already noted, a common flaw among the first designs is that they
%
\begin{inlinelist}[itemjoin={, }, itemjoin*={, and}]
  \item apply the greediest form of \gls{pattern selection}
  \item typically lack a formal methodology
\end{inlinelist}.
%
In contrast, \gls!{syntactic analysis} -- which is the task of parsing the
source code -- is arguably the best understood area of compilation, and its
methods also produce completely table-driven parsers that are very fast and
resource-efficient.

In 1978, \textcite{GlanvilleGraham:1978} presented a seminal paper that
describes how techniques of \gls{syntactic analysis} can be adapted to
\gls{instruction selection}.\!%
%
\footnote{%
  This had also been vaguely hinted at ten years earlier in an article by
  \textcite{FeldmanGries:1968}.%
}
%
Due to its pioneers, we refer to this as the \gls!{Glanville-Graham approach}.
%
We first describe \glspl{grammar}, which is the representation used by
\citeauthor{GlanvilleGraham:1978} for modeling the \glspl{instruction}.


\subsection{Modeling Instructions as Machine Grammars}

\begin{inParFigure}{25mm}[r]
  \centering

  % LAYOUT FIX:
  % Make the figure as high as the entire paragraph
  \vspace*{.5\baselineskip}

  \begin{minipage}{\linewidth}
    \centering%
    \input{figures/tree-covering/tree-linearize-example}

    \vspace{.5em}

    $\Downarrow$

    \vspace{.5em}

    \figureFontSize%
    \cCode{\irMulText{} \irAddText{} \irVar{a} \irVar{b} \irVar{2}}%
  \end{minipage}

  % LAYOUT FIX:
  % Make the figure as high as the entire paragraph
  \vspace*{1.5\baselineskip}
\end{inParFigure}%
\noindent
To begin with, a well-known method of removing the need for parentheses in
arithmetic expressions without making them ambiguous is to use \gls!{Polish
  notation}.
%
For example, \mbox{$1 + (2 + 3)$} can be written as \mbox{$+ \; 1 + 2 \; 3$} and
still represent the same expression.
%
\citeauthor{GlanvilleGraham:1978} recognized that by using this form the
\glspl{instruction} can be expressed as a \gls{context-free grammar}.
%
This concept is already well described in most \gls{compiler} textbooks (see for
example~\cite{AhoEtAl:2006}), so we will proceed with only a brief introduction.

A \gls!{context-free grammar} (or simply \gls!{grammar}) consists of
\glspl{terminal}, \glspl{nonterminal}, and \glspl{rule}.
%
In this context, a \gls!{terminal} is a \gls{symbol} representing an
\gls{operation} (e.g.\ \irAddText, \irLTText, \irLoadText), and a
\gls!{nonterminal} is a \gls{symbol} representing an abstract result
(e.g.\ $\mNT{Reg}$) produced by the \gls{instruction}.
%
To distinguish between the two, \glspl{terminal} are written entirely in lower
case whereas \glspl{nonterminal} start with a capital letter and are set in
italics.
%
A \gls!{rule} describes the behavior of an \gls{instruction} and consists of a
\gls{production}, a non-negative cost, and an \gls{action}.
%
\Glspl!{production} describe how to derive \glspl{nonterminal}, and are written
as
%
\begin{displaymath}
  \alpha \rightarrow \beta \gamma \ldots
\end{displaymath}
%
where the left-hand side is a single \gls{nonterminal} and the right-hand side
is a sequence of \glspl{terminal} and \glspl{nonterminal}.
%
Each \gls{instruction} therefore gives rise to one or more \glspl{production},
where the right-hand side of a \gls{production} captures a \gls{pattern} of the
\gls{instruction} and the left-hand side denotes the result produced by the
\gls{instruction}.
%
Hence the left-hand and right-hand sides of a \gls{rule} are referred to as the
\gls{rule}'s \glsshort!{rule result} and \glsshort!{rule pattern}, respectively.
%
The cost should be self-explanatory at this point, and the \gls!{action} is the
activity to perform when the \gls{rule} is selected (typically, this is to emit
a string of \gls{assembly code}).
%
The \gls{rule} structure is also illustrated in
\refFigure{machine-grammar-rule-anatomy-2}, and an example of a few
\glspl{rule} is given in \refTable{grammar-rules-example-2}.
%
\begin{figure}
  \centering%
  \figureFont\figureFontSize%
  \begin{displaymath}
    \underbrace{
      \overbrace{
        \overbrace{\mNT{Reg}[1]}^{\text{result}}
        \rightarrow \;
        \overbrace{%
          \irCode{\irAddText} \; \mNT{Reg}[2] \; \irCode{const}%
        }^{\text{pattern}}%
      }^{\text{production}}
      \qquad
      \overbrace{\text{4}}^{\text{cost}}
      \qquad
      \overbrace{
        \text{emit}~
        \text{\instrFont{} add \$$\mNT{Reg}[1]$, \$$\mNT{Reg}[2]$, \#const}%
      }^{\text{action}}
    }_{\text{rule}}
  \end{displaymath}

  \vspace*{-\baselineskip}

  \caption{Anatomy of a rule in a machine grammar}
  \labelFigure{machine-grammar-rule-anatomy-2}
\end{figure}
%
\begin{table}
  \centering%
  \figureFontSize%
  \begin{tabular}{cl@{ $\rightarrow$ }lcl}
    \toprule
        \tabhead \#
      & \multicolumn{2}{c}{\tabhead production}
      & \tabhead cost
      & \multicolumn{1}{c}{\tabhead action}\\
    \midrule
    1 & $\mNT{Reg}[1]$
      & $\irCode{\irLoadText} \; \irCode{\irAddText} \; \mNT{Reg}[2] \;
         \irCode{const}$
      & 1
      & emit {\instrFont load \$$\mNT{Reg}[1]$, const(\$$\mNT{Reg}[2]$)}\\
    2 & $\mNT{Reg}[1]$
      & $\irCode{\irLoadText} \; \irCode{\irAddText} \; \irCode{const} \;
         \mNT{Reg}[2]$
      & 1
      & emit {\instrFont load \$$\mNT{Reg}[1]$, const(\$$\mNT{Reg}[2]$)}\\
    3 & $\mNT{Reg}[1]$
      & $\irCode{\irLoadText} \; \mNT{Reg}[2]$
      & 1
      & emit {\instrFont load \$$\mNT{Reg}[1]$, 0(\$$\mNT{Reg}[2]$)}\\
    \bottomrule
  \end{tabular}

  \caption[Example of grammar rules]%
          {%
            Example of grammar rules corresponding to a
            \mbox{\instrFont*load \$t, o(\$s)} \gls{instruction} that loads a
            value from memory at the address given in register~\instrCode*{s},
            offset by an immediate value~\instrCode*{o}, and stores the loaded
            value in register~\instrCode*{t}, in one cycle.
            %
            The subscripts are only needed for referencing the right
            nonterminal in the action%
          }
  \labelTable{grammar-rules-example-2}
\end{table}
%
The collection of \glspl{rule} for a particular \gls{target machine} is called
the \gls!{machine grammar} of that machine.

In most literature, \glspl{rule} and \glspl{pattern} usually have the same
connotations.
%
\labelPage{rule-terms}
%
In this dissertation, however, in the context of \glspl{grammar} a \gls{rule}
refers to a tuple of \gls{production}, cost, and \gls{action}, and a
\gls{pattern} refers to the right-hand side of the \gls{production} appearing in
a \gls{rule}.

Not shown in the example, a \gls{machine grammar} also consists of a
\gls!{goal.s}[ \gls{symbol}], whose purpose will be explained shortly.


\paragraph{Normal Form}

To simplify \gls{pattern matching} and \gls{pattern selection}, a \gls{grammar}
can be rewritten into \gls!{normal form.g}~\cite{BalachandranEtAl:1990}.
%
A \gls{grammar} is in \gls{normal form.g} if every \gls{rule} in the
\gls{grammar} has a \gls{production} in one of the following forms:
%
\begin{enumerate}
  \item \mbox{$\mNT{N} \rightarrow \irCode*{op} \; \mNT{A}[1] \; \mNT{A}[2]
    \ldots \mNT{A}[n]$}, where \irCode*{op} is a \gls{terminal}, representing an
    \gls{operation} that takes $n$ arguments, and all $\mNT{A}[i]$ are
    \glspl{nonterminal}.
    %
    Such rules are called \gls!{base.r}[ \glspl{rule}].
  \item \mbox{$\mNT{N} \rightarrow \irCode*{t}$}, where \irCode*{t} is a
    \gls{terminal}.
    %
    Such rules are also called \gls{base.r} \glspl{rule}.
  \item \mbox{$\mNT{N} \rightarrow \mNT{A}$}, where $\mNT{A}$ is a
    \gls{nonterminal}.
    %
    Such rules are called \gls!{chain.r}[ \glspl{rule}].
\end{enumerate}

A \gls{grammar} can be mechanically rewritten into \gls{normal form.g} by
introducing new \glspl{nonterminal} and breaking down illegal \glspl{rule} into
multiple, smaller \glspl{rule} until the \gls{grammar} is in \gls{normal
  form.g}.
%
For example, rewriting the \gls{grammar} shown in
\refTable{grammar-rules-example} into \gls{normal form.g} results in the
\gls{grammar} shown in \refTable{normal-form-grammar-example-2}.
%
\begin{table}[t]
  \centering%
  \figureFontSize%
  \begin{tabular}{cr@{ $\rightarrow$ }lcl}
    \toprule
    \tabhead \# & \multicolumn{2}{c}{\tabhead production} & \tabhead cost
      & \multicolumn{1}{c}{\tabhead action}\\
    \midrule
    1 & $\mNT{Reg}$ & $\irCode{\irLoadText} \; \mNT{A}$
      & 1
      & emit {\instrFont load \$$\mNT{Reg}$,
                               $A.C.\text{const}$(\$$A.\mNT{Reg}$)}\\
    4 & $\mNT{A}$ & $\irCode{\irAddText} \; \mNT{Reg} \; \mNT{C}$
      & 0
      & \\
    2 & $\mNT{Reg}$ & $\irCode{\irLoadText} \; \mNT{B}$
      & 1
      & emit {\instrFont load \$$\mNT{Reg}$,
                               $B.C.\text{const}$(\$$B.\mNT{Reg}$)}\\
    5 & $\mNT{B}$ & $\irCode{\irAddText} \; \mNT{C} \; \mNT{Reg}$
      & 0
      & \\
    6 & $\mNT{C}$ & $\irCode{const}$
      & 0
      & \\
    3 & $\mNT{Reg}[1]$ & $\irCode{\irLoadText} \; \mNT{Reg}[2]$
      & 1
      & emit {\instrFont load \$$\mNT{Reg}[1]$, 0(\$$\mNT{Reg}[2]$)}\\
    \bottomrule
  \end{tabular}%

  \caption[Example of a grammar in normal form]%
          {%
            The grammar from \refTable{grammar-rules-example} in normal form.
            %
            Nonterminals~$\mNT{A}$,~$\mNT{B}$ and~$\mNT{C}$ and
            rules~\mbox{\num{4}--\num{6}} are introduced in order to transform
            rules~\num{1} and~\num{2} into base rules%
          }
  \labelTable{normal-form-grammar-example-2}
\end{table}
%
Note that the new \glspl{rule} have zero cost and no \gls{action} as these are
only intermediary steps towards enabling reduction of the original \gls{rule}.

Since all \glspl{production} in a \glsshort{normal form.g} \gls{grammar} have at
most one \gls{terminal}, the \gls{pattern matching} problem becomes trivial
(simply match the \gls{node} type against the \gls{terminal} in all \gls{base.r}
\glspl{rule}).
%
Otherwise another bottom-up traversal of the \gls{expression tree} would have to
be made in order to find all \glspl{match}, which can be done in linear time for
most reasonable \glspl{grammar}~\cite{HoffmannODonnell:1982}.
%
As we will see, this also simplifies \gls{pattern selection} as the
\glspl{pattern} on the right-hand side in all \glspl{production} have uniform
height.


\subsection{The Glanville-Graham Approach}
\labelSection{tc-glanville-graham}

The \gls{machine grammar} provides us with a formal methodology for modeling
\glspl{instruction}, but it does not address the problems of \gls{pattern
  matching} and \gls{pattern selection}.
%
For that, \citeauthor{GlanvilleGraham:1978} applied an already-known technique
called \gls!{LR parsing}~\cite{Knuth:1965}.
%
Because this technique is mostly associated with \gls{syntactic analysis}, the
same application on \glspl{tree} is commonly referred to as \gls!{tree parsing}.
%
An extremely deep and thorough account of the theory and practice of this
approach is given by \textcite{Henry:1984}.


\paragraph{Tree Parsing}

As an example, let us use the \gls{machine grammar} given in
\refFigure{tree-parsing-example-machine-grammar} to generate \gls{assembly code}
for the \gls{expression tree} given in
\refFigure{tree-parsing-example-expression-tree} such that the result ends up in
a \gls{register}.
%
We assume that \cVar*{a} and \cVar*{b} are variables already stored in
\glspl{register} and that \cVar*{2} is an integer constant.
%
Hence each \gls{node} in the \gls{expression tree} is either of type
\irCode*{\irAddText}, \irCode*{\irMulText}, \irCode*{reg}, or \irCode*{const}.
%
These will be our \glspl{terminal}.
%
Because the result should be end up in a \gls{register}, we say that $\mNT{Reg}$
is our \gls{goal.s} \gls{symbol}.

After transforming the \gls{expression tree} into a sequence of \glspl{terminal}
(as in \refFigure{tree-parsing-example-terminal-sequence}), we traverse the
sequence from left to right.
%
\newcommand{\gS}{\mbox{\figureFont s}}
\newcommand{\gR}[1]{\mbox{\figureFont r$_{\text{#1}}$}}
%
\begin{filecontents*}{tree-parsing-example-expression-tree.c}
$\irAddText$ a $\irMulText$ b 2
\end{filecontents*}
%
\begin{filecontents*}{tree-parsing-example-terminal-sequence.c}
$\irAddText$ $\cVar{reg}[a]$ $\irMulText$ $\cVar{reg}[b]$ $\cVar{const}[2]$
\end{filecontents*}
%
\begin{filecontents*}{tree-parsing-example-assembly-code.c}
mv  $\$\mNT{Reg}[\irCode{2}]$, #2
mul $\$\mNT{Reg}[\irCode{x}]$, $\$\mNT{Reg}[\irCode{b}]$, $\$\mNT{Reg}[\irCode{2}]$
add $\$\mNT{Reg}[\irCode{y}]$, $\$\mNT{Reg}[\irCode{a}]$, $\$\mNT{Reg}[\irCode{x}]$
\end{filecontents*}
%
\begin{figure}
  \centering%
  \subcaptionbox{%
                  Machine grammar%
                  \labelFigure{tree-parsing-example-machine-grammar}%
                }%
                {%
                  \figureFontSize%
                  \begin{tabular}{cl@{ $\rightarrow$ }lcl}
                    \toprule
                        \tabhead \#
                      & \multicolumn{2}{c}{\tabhead production}
                      & \tabhead cost
                      & \multicolumn{1}{c}{\tabhead action}\\
                    \midrule
                        1
                      & $\mNT{Reg}[1]$
                      & $\irCode{\irAddText} \; \mNT{Reg}[2] \; \mNT{Reg}[3]$
                      & 1
                      & emit {\instrFont add \$$\mNT{Reg}[1]$,
                                              \$$\mNT{Reg}[2]$,
                                              \$$\mNT{Reg}[3]$}\\
                        2
                      & $\mNT{Reg}[1]$ & $\irCode{\irMulText} \;
                                          \mNT{Reg}[2] \;
                                          \mNT{Reg}[3]$
                      & 1
                      & emit {\instrFont mul \$$\mNT{Reg}[1]$,
                                              \$$\mNT{Reg}[2]$,
                                              \$$\mNT{Reg}[3]$}\\
                        3
                      & $\mNT{Reg}$ & \irCode{const}
                      & 1
                      & emit {\instrFont mv  \$$\mNT{Reg}$, \#\irCode{const}}\\
                        4
                      & $\mNT{Reg}$ & \irCode{reg}
                      & 1
                      & \\
                    \bottomrule
                  \end{tabular}%
                }

  \vspace{\betweensubfigures}

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Expression tree%
                  \labelFigure{tree-parsing-example-expression-tree}%
                }%
                [28mm]%
                {%
                  \begin{lstpage}{19mm}%
                    \lstinputlisting[mathescape]%
                                    {tree-parsing-example-expression-tree.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \subcaptionbox{%
                  Sequence of terminals%
                  \labelFigure{tree-parsing-example-terminal-sequence}%
                }%
                [35mm]%
                {%
                  \begin{lstpage}{31mm}%
                    \lstinputlisting[mathescape]%
                                    {tree-parsing-example-terminal-sequence.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \mbox{}

  \vspace{\betweensubfigures}

  \subcaptionbox{%
                  Sequence of shifts and rule reductions%
                  \labelFigure{tree-parsing-example-shift-rule-reductions}%
                }%
                [\textwidth]%
                {%
                  \gS{} \, \gS{} \, \gR{4} \, \gS{} \, \gS{} \, \gR{4} \,
                  \gS{} \, \gR{3} \, \gR{2} \, \gR{1}
                }

  \vspace{\betweensubfigures}

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Parse tree.
                  %
                  The rule numbers are shown in parentheses.
                  %
                  The subscripts denote book keeping data%
                  \labelFigure{tree-parsing-example-parse-tree}%
                }{%
                  \input{figures/tree-covering/parse-tree-example}%
                }%
  \hfill%
  \subcaptionbox{%
                  Assembly code%
                  \labelFigure{tree-parsing-example-assembly-code}%
                }{%
                  \begin{lstpage}{38mm}%
                    \lstinputlisting[mathescape]%
                                    {tree-parsing-example-assembly-code.c}%
                  \end{lstpage}%
                }%
  \hfill%
  \mbox{}

  \caption{Example of tree parsing}
  \labelFigure{tree-parsing-example}
\end{figure}
%
When doing so, we either \gls!{shift} the just-traversed \gls{symbol} onto a
stack, or replace \glspl{symbol} currently on the stack via a \gls!{rule
  reduction}.
%
We denote a \gls{shift} by~\gS{} and a \gls{rule reduction} by~\gR{$x$}, where
$x$ is the number of the reduced \gls{rule}.
%
A \gls{rule reduction} consists of two steps.
%
First, the \glspl{symbol} are popped according to those that appear on the
\gls{pattern} of the \gls{rule}.
%
The number and order of \glspl{symbol} popped must match exactly for a valid
\gls{rule reduction}.
%
Once popped, the \gls{nonterminal} appearing on the left-hand side is pushed
onto the stack.
%
When the last \gls{rule reduction} has been performed, the stack must contain
only the \gls{goal.s} \gls{symbol}.
%
For our example, a valid sequence of \glspl{shift} and \glspl{rule reduction}
is given in \refFigure{tree-parsing-example-shift-rule-reductions}.
%
The performed \glspl{rule reduction} can also be represented as a \gls!{parse
  tree}, illustrating the \glspl{terminal} and \glspl{nonterminal} which were
used to parse the sequence of \glspl{terminal}.
%
\RefFigure{tree-parsing-example-parse-tree} shows the corresponding \gls{parse
  tree} for the sequence shown in
\refFigure{tree-parsing-example-shift-rule-reductions}.
%
Lastly, when performing a \gls{rule reduction} we also execute the \gls{action}
associated with the \gls{rule}.
%
For our example, the resulting \gls{assembly code} is shown in
\refFigure{tree-parsing-example-assembly-code}.

The problem that remains is how to know when to \gls{shift} and when to reduce.
%
This can be addressed by consulting a state table which has been
generated for a specific \gls{grammar}.
%
How this table is produced is out of scope for this dissertation, but an
example generated from the \gls{machine grammar} shown in
\refFigure{glanville-graham-machine-grammar} is given in
\refFigure{glanville-graham-table}.
%
A walk-through of executing an \gls{instruction selector} with this state table
using an \gls{LR parser} is provided in \refFigure{glanville-graham-execution}.

\begin{figure}
  \figureFont\figureFontSize\relsize{-.5}%
  \begin{minipage}{66mm}%
    \centering%
    \begin{tabular}{rl@{$\; \rightarrow \;$}ll}
      \toprule
          \multicolumn{1}{c}{\tabhead \#}
        & \multicolumn{2}{c}{\tabhead production}
        & \multicolumn{1}{c}{\tabhead action}\\
      \midrule
          1
        & $\mNT{R}[2]$
        & \cCode{$+$ ld $+$ c} $\mNT{R}[1]$ $\mNT{R}[2]$
        & \instrCode{add $\mNT{R}[2]$,c,$\mNT{R}[1]$}\\
          2
        & $\mNT{R}[1]$
        & \cCode{$+$} $\mNT{R}[1]$ \cCode{ld $+$ c} $\mNT{R}[2]$
        & \instrCode{add $\mNT{R}[1]$,c,$\mNT{R}[2]$}\\
          3
        & $\mNT{R}$
        & \cCode{$+$ ld c} $\mNT{R}$
        & \instrCode{add $\mNT{R}$,c}\\
          4
        & $\mNT{R}$
        & \cCode{$+$} $\mNT{R}$ \cCode{ld c}
        & \instrCode{add $\mNT{R}$,c}\\
          5
        & $\mNT{R}[1]$
        & \cCode{$+$} $\mNT{R}[1]$ $\mNT{R}[2]$
        & \instrCode{add $\mNT{R}[1]$,$\mNT{R}[2]$}\\
          6
        & $\mNT{R}[2]$
        & \cCode{$+$} $\mNT{R}[1]$ $\mNT{R}[2]$
        & \instrCode{add $\mNT{R}[2]$,$\mNT{R}[1]$}\\
          7
        & & \cCode{$=$ ld $+$ c} $\mNT{R}[1]$ $\mNT{R}[2]$
        & \instrCode{store $\mNT{R}[2]$,*c,$\mNT{R}[1]$}\\
          8
        & & \cCode{$=$ $+$ c} $\mNT{R}[1]$ $\mNT{R}[2]$
        & \instrCode{store $\mNT{R}[2]$,c,$\mNT{R}[1]$}\\
          9
        & & \cCode{$=$ ld c} $\mNT{R}$
        & \instrCode{store $\mNT{R}$,*c}\\
          10
        & & \cCode{$=$ c} $\mNT{R}$
        & \instrCode{store $\mNT{R}$,c}\\
          11
        & & \cCode{$=$} $\mNT{R}[1]$ $\mNT{R}[2]$
        & \instrCode{store $\mNT{R}[2]$,$\mNT{R}[1]$}\\
          12
        & $\mNT{R}[2]$
        & \cCode{ld $+$ c} $\mNT{R}[1]$
        & \instrCode{load $\mNT{R}[2]$,c,$\mNT{R}[1]$}\\
          13
        & $\mNT{R}[2]$
        & \cCode{$+$ c} $\mNT{R}[1]$
        & \instrCode{load $\mNT{R}[2]$,$=\!c$,$\mNT{R}[1]$}\\
          14
        & $\mNT{R}[2]$
        & \cCode{$+$} $\mNT{R}[1]$ \cCode{c}
        & \instrCode{load $\mNT{R}[2]$,=c,$\mNT{R}[1]$}\\
          15
        & $\mNT{R}[2]$
        & \cCode{ld} $\mNT{R}[1]$
        & \instrCode{load $\mNT{R}[2]$,*$\mNT{R}[1]$}\\
          16
        & $\mNT{R}$
        & \cCode{ld c}
        & \instrCode{load $\mNT{R}$,=c}\\
          17
        & $\mNT{R}$
        & \cVar{c}
        & \instrCode{mv $\mNT{R}$,c}\\
      \bottomrule
    \end{tabular}

    \caption[Example of a machine grammar]%
            {%
              Example of a machine grammar~\cite{GlanvilleGraham:1978}.
              %
              \cCode*{c} (``const''), \cCode*{ld} (``load''), \cCode*{$+$}, and
              \cCode*{$=$} are all terminals, $\mNT{R}$ is a nonterminal
              indicating that the result will be stored in a register, and
              subscripts denote the semantic
              qualifiers.
              %
              All rules have the same unit cost.
              %
              Rules \mbox{\num{7}--\num{11}} do not have a nonterminal on the
              left-hand side as memory store instructions do not produce
              anything.
              %
              A dummy nonterminal can also be used if needed%
            }
    \labelFigure{glanville-graham-machine-grammar}
  \end{minipage}%
  \hfill%
  \begin{minipage}{55mm}
    \centering%
    \begin{tabular}{r@{\hspace{.6em}}c@{\hspace{.6em}}c*{4}{@{\hspace{1.2em}}c}}
      \toprule
          \multicolumn{1}{c}{\tabhead \#}
        & \tabhead\cCode{\$}
        & \tabhead$\mathtabhead{\mNTFont{R}}$
        & \tabhead\cCode{c}
        & \tabhead$\mathtabhead{+}$
        & \tabhead\cCode{ld}
        & \tabhead$\mathtabhead{=}$\\
      \midrule
         0 & accept &       &       &       &       &    s1\\
         1 &        &    s2 &    s3 &    s4 &    s5 &      \\
         2 &        &    s6 &    s7 &    s8 &    s9 &      \\
         3 &        &   s10 &    s7 &    s8 &    s9 &      \\
         4 &        &   s11 &   s12 &    s8 &   s13 &      \\
         5 &        &   s14 &   s15 &   s16 &    s9 &      \\
         6 &    r11 &   r11 &   r11 &   r11 &   r11 &   r11\\
         7 &    r17 &   r17 &   r17 &   r17 &   r17 &   r17\\
         8 &        &   s11 &   s17 &    s8 &   s13 &      \\
         9 &        &   s14 &   s18 &   s19 &    s9 &      \\
        10 &    r10 &   r10 &   r10 &   r10 &   r10 &   r10\\
        11 &        &   s20 &   s21 &    s8 &   s22 &      \\
        12 &        &   s23 &    s7 &    s8 &    s9 &      \\
        13 &        &   s14 &   s24 &   s25 &    s9 &      \\
        14 &    r15 &   r15 &   r15 &   r15 &   r15 &   r15\\
        15 &        &   s26 &    s7 &    s8 &    s9 &      \\
        16 &        &   s11 &   s27 &    s8 &   s13 &      \\
        17 &        &   s28 &    s7 &    s8 &    s9 &      \\
        18 &    r16 &   r16 &   r16 &   r16 &   r16 &   r16\\
        19 &        &   s11 &   s29 &    s8 &   s13 &      \\
        20 &   r5/6 &  r5/6 &  r5/6 &  r5/6 &  r5/6 &  r5/6\\
        21 &    r14 &   r14 &   r14 &   r14 &   r14 &   r14\\
        22 &        &   s14 &   s30 &   s31 &    s9 &      \\
        23 &        &   s32 &    s7 &    s8 &    s9 &      \\
        24 &        &   s33 &    s7 &    s8 &    s9 &      \\
        25 &        &   s11 &   s34 &    s8 &   s13 &      \\
        26 &     r9 &    r9 &    r9 &    r9 &    r9 &    r9\\
        27 &        &   s35 &    s7 &    s8 &    s9 &      \\
        28 &    r13 &   r13 &   r13 &   r13 &   r13 &   r13\\
        29 &        &   s36 &    s7 &    s8 &    s9 &      \\
        30 &     r4 &    r4 &    r4 &    r4 &    r4 &    r4\\
        31 &        &   s11 &   s37 &    s8 &   s13 &      \\
        32 &     r8 &    r8 &    r8 &    r8 &    r8 &    r8\\
        33 &     r3 &    r3 &    r3 &    r3 &    r3 &    r3\\
        34 &        &   s38 &    s7 &    s8 &    s9 &      \\
        35 &        &   s39 &    s7 &    s8 &    s9 &      \\
        36 &    r12 &   r12 &   r12 &   r12 &   r12 &   r12\\
        37 &        &   s40 &    s7 &    s8 &    s9 &      \\
        38 &        &   s41 &    s7 &    s8 &    s9 &      \\
        39 &     r7 &    r7 &    r7 &    r7 &    r7 &    r7\\
        40 &     r2 &    r2 &    r2 &    r2 &    r2 &    r2\\
        41 &     r1 &    r1 &    r1 &    r1 &    r1 &    r1\\
      \bottomrule
    \end{tabular}

    \caption[Example of a state table for a machine grammar]%
            {%
              State table generated from the machine grammar given in
              \refFigure{glanville-graham-machine-grammar}%
              ~\cite{GlanvilleGraham:1978}.
              %
              {\figureFont s$i$} indicates a shift to the next state $i$,
              {\figureFont r$j$} indicates the reduction of
              rule~\mbox{$j$\hspace{-1pt},} and a blank entry indicates an
              error%
            }
    \labelFigure{glanville-graham-table}
  \end{minipage}
\end{figure}

\begin{figure}
  \centering%
  \figureFont\figureFontSize\relsize{-.5}%
  \begin{adjustbox}{rotate=90, center}
    \begin{minipage}{\textheight}
      \begin{tabular}{rll@{\hspace{-2mm}}rl}
        \toprule
             \tabhead step
           & \tabhead state stack
           & \tabhead symbol
           & \tabhead input
           & \tabhead action\\
        \midrule
             1
           & 0
           &
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$}
             \cVar{c}[b] \cCode{ld} $\mNT{R}[7]$ \cCode{ld} \cVar{c}[c]
             \cCode{\$}
           & shift to 1\\
             2
           & 0 1
           & \cCode{$=$}
           & \cCode{$+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$}
             \cVar{c}[b] \cCode{ld} $\mNT{R}[7]$ \cCode{ld} \cVar{c}[c]
             \cCode{\$}
           & shift to 4\\
             3
           & 0 1 4
           & \cCode{$=$ $+$}
           & \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$} \cVar{c}[b] \cCode{ld}
             $\mNT{R}[7]$ \cCode{ld} \cVar{c}[c] \cCode{\$}
           & shift to 12\\
             4
           & 0 1 4 12
           & \cCode{$=$ $+$} \cVar{c}[a]
           & $\mNT{R}[7]$ \cCode{$+$ ld $+$} \cVar{c}[b] \cCode{ld} $\mNT{R}[7]$
             \cCode{ld} \cVar{c}[c] {\$}
           & shift to 23\\
             5
           & 0 1 4 12 23
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$
           & \cCode{$+$ ld $+$} \cVar{c}[b] \cCode{ld} $\mNT{R}[7]$ \cCode{ld}
             \cVar{c}[c] \cCode{\$}
           & shift to 8\\
             6
           & 0 1 4 12 23 8
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$}
           & \cCode{ld $+$} \cVar{c}[b] \cCode{ld} $\mNT{R}[7]$ \cCode{ld}
             \cVar{c}[c] \cCode{\$}
           & shift to 13\\
             7
           & 0 1 4 12 23 8 13
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld}
           & \cCode{$+$} \cVar{c}[b] \cCode{ld} $\mNT{R}[7]$ \cCode{ld}
             \cVar{c}[c] \cCode{\$}
           & shift to 25\\
             8
           & 0 1 4 12 23 8 13 25
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$}
           & \cVar{c}[b] \cCode{ld} $\mNT{R}[7]$ \cCode{ld} \cVar{c}[c]
             \cCode{\$}
           & shift to 34\\
             9
           & 0 1 4 12 23 8 13 25 34
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$}
             \cVar{c}[b]
           & \cCode{ld} $\mNT{R}[7]$ \cCode{ld} \cVar{c}[c] \cCode{\$}
           & shift to 9\\
             10
           & 0 1 4 12 23 8 13 25 34 9
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$}
             \cVar{c}[b] \cCode{ld}
           & $\mNT{R}[7]$ \cCode{ld} \cVar{c}[c] \cCode{\$}
           & shift to 14\\
             11
           & 0 1 4 12 23 8 13 25 34 9 14
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$}
             \cVar{c}[b] \cCode{ld} $\mNT{R}[7]$
           & \cCode{ld} \cVar{c}[c] \cCode{\$}
           & reduce rule 15 ($\mNT{R}[2]$ $\rightarrow$ \cCode{ld}
             $\mNT{R}[1]$)\\
           & & & & assign result to register 8\\
           & & & & emit \instrCode{load $\mNT{R}[8]$,*$\mNT{R}[7]$}\\
           & & & & shift to 38\\
             12
           & 0 1 4 12 23 8 13 25 34 38
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$}
             \cVar{c}[b] $\mNT{R}[8]$
           & \cCode{ld} \cVar{c}[c] \cCode{\$}
           & shift to 9\\
             13
           & 0 1 4 12 23 8 13 25 34 38 9
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$}
             \cVar{c}[b] $\mNT{R}[8]$ \cCode{ld}
           & \cVar{c}[c] \cCode{\$}
           & shift to 18\\
             14
           & 0 1 4 12 23 8 13 25 34 38 9 18
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$}
             \cVar{c}[b] $\mNT{R}[8]$ \cCode{ld} \cVar{c}[c]
           & \cCode{\$}
           & reduce rule 16 ($\mNT{R}$ $\rightarrow$ \cCode{ld} \cVar{c})\\
           & & & & assign result to register 9\\
           & & & & emit \instrCode{load r9,c}\\
           & & & & shift to 41\\
             15
           & 0 1 4 12 23 8 13 25 34 38 41
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ \cCode{$+$ ld $+$}
             \cVar{c}[b] $\mNT{R}[8]$ $\mNT{R}[9]$
           & \cCode{\$}
           & reduce rule 1 ($\mNT{R}[2]$ $\rightarrow$ \cCode{$+$ ld $+$ c}
             $\mNT{R}[1]$ $\mNT{R}[2]$)\\
           & & & & emit \instrCode{add $\mNT{R}[9]$,b,$\mNT{R}[8]$}\\
           & & & & shift to 32\\
             16
           & 0 1 4 12 23 32
           & \cCode{$=$ $+$} \cVar{c}[a] $\mNT{R}[7]$ $\mNT{R}[2]$
           & \cCode{\$}
           & reduce rule 8 (\quad $\rightarrow$ \cCode{$=$ $+$ c} $\mNT{R}[1]$
             $\mNT{R}[2]$)\\
           & & & & emit \instrCode{store $\mNT{R}[9]$,a,$\mNT{R}[7]$}\\
             17
           & 0
           &
           & \cCode{\$}
           & accept\\
        \bottomrule
      \end{tabular}

      \caption[Execution walk-through of the Glanville-Graham approach]%
        {%
          A walk-through of executing the table from
          \refFigure{glanville-graham-table} on a sequence \mbox{\cCode*{$=$
              $+$} \cVar*{c}[a] $\mNT{R}[7]$ \cCode*{$+$} \cCode*{ld $+$}
            \cVar*{c}[b] \cCode*{ld} $\mNT{R}[7]$ \cCode*{ld} \cVar*{c}[c]} (for
          simplicity, registers in the sequence are represented directly as
          nonterminals).
          %
          Note that a rule reduction may involve two operations:
          %
          \begin{inlinelist}[itemjoin={, }, itemjoin*={, }]
            \item the mandatory reduce operation
            \item followed by an optional operation which may be a shift or
              another rule reduction
          \end{inlinelist}.
          %
          For example, let us examine step~\num{11}.
          %
          First, a reduce is executed using rule~\num{15}, which pops
          \mbox{\cCode{ld} $\mNT{R}[7]$} from the symbol stack.
          %
          This is followed by pushing the result of the rule, $\mNT{R}[8]$, on
          top.
          %
          At the same time, states~\num{9} and~\num{14} are popped from the
          stack, which leaves state~\num{34} on top.
          %
          The top elements of both stacks are now used to consult the state
          table for inferring the next, additional action (if any).
          %
          In this case, input symbol $\mNT{R}[8]$ at state~\num{34} leads to a
          shift to state~\num{38}%
        }%
      \labelFigure{glanville-graham-execution}
    \end{minipage}%
  \end{adjustbox}
\end{figure}

The subscripts that appear in some of the \glspl{production} in
\refFigure{glanville-graham-machine-grammar} are \glspl{semantic qualifier},
which are used to express restrictions that may appear for some of the
\glspl{instruction}.
%
For example, all two-address arithmetic \glspl{instruction} store the result in
one of the \glspl{register} provided as input.
%
Using semantic quantifiers, this could be expressed as \mbox{$\mNT{R}[1]$
  $\rightarrow$ \irCode*{$+$} $\mNT{R}[1]$ $\mNT{R}[2]$}, indicating that the
destination \gls{register} must be the same as that of the first operand.
%
To make this information available during parsing, the parser pushes it onto the
stacking along with its corresponding \gls{terminal} or \gls{nonterminal}
\gls{symbol}.
%
\citeauthor{GlanvilleGraham:1978} also incorporated a \gls{register allocator}
into their parser, thus constituting an entire \gls{code generator}.


\paragraph{Resolving Conflicts and Avoiding Blocking}

As \glspl{instruction set} are rarely orthogonal, most \glspl{machine
  grammar} are \gls!{ambiguous.g}, meaning multiple valid \glspl{parse tree} may
exist for the same \gls{expression tree}.
%
This causes the \gls{instruction selector} to have the option of performing
either a \gls{shift} or a \gls{rule reduction}, which is known as
\gls!{shift-reduce conflict}.
%
To solve this kind of conflict, \citeauthor{GlanvilleGraham:1978}'s state table
generator always decides to \gls{shift}.
%
The intuition is that this will favor larger \glspl{pattern} over smaller ones
as a \gls{shift} postpones a decision to \gls{pattern} select while allowing
more information about the \gls{expression tree} to accumulate on the
stack.\!%
%
\footnote{%
  The approach of always selecting the largest possible \gls{pattern} is a
  scheme commonly known as \gls!{maximum munch}, which was coined by
  \citeauthor{Cattell:1978} in his doctoral dissertation~\cite{Cattell:1978}.%
}

Unfortunately, this scheme can cause the \gls{instruction selector} to fail even
though a valid \gls{parse tree} exists.
%
This is called \gls!{syntactic blocking}.
%
To avoid such situations, the \gls{grammar} designer must augment the
\gls{machine grammar} with auxiliary \glspl{rule} that patch the top of the
stack when necessary.
%
This allows the parser to recover from situations when it greedily decides to
\gls{shift} instead of applying a necessary \gls{rule reduction}.

Likewise, there is also the possibility of \glspl!{reduce-reduce conflict},
where the parser has the option of choosing between two or more \glspl{rule} in
a \glsshort{rule reduction}.
%
\citeauthor{GlanvilleGraham:1978} resolved these by selecting the \gls{rule}
with the longest \gls{pattern}.
%
If the \gls{grammar} contains \glspl{rule} that differ only in their semantic
quantifiers, then there may still exist more than one \gls{rule} to reduce (in
\refFigure{glanville-graham-machine-grammar}, \glspl{rule}~\num{5} and~\num{6}
are two such \glspl{rule}).
%
These are resolved at parse time by checking the semantic restrictions in the
order in which they appear in the \gls{grammar} (see for example state~\num{20}
in \refFigure{glanville-graham-table}).

If all \glspl{rule} in this set are semantically constrained, then situations
can arise where the parser is unable to apply any \gls{rule} due to semantic
mismatch.
%
This is called \gls!{semantic blocking} and can be resolved by always providing
a default \gls{rule} that can be invoked when all other semantically constrained
\glspl{rule} fail.
%
This fallback \gls{rule} typically uses multiple, shorter \glspl{instruction} to
simulate the effect of the more complex \gls{rule}, and
\citeauthor{GlanvilleGraham:1978} devised a clever trick to infer them
automatically.
%
For every semantically constrained \gls{rule}~$r$\!, \gls{tree parsing} is then
performed over the \gls{tree} representing the pattern of~$r$\!.
%
The \glspl{instruction} selected to implement this \gls{tree} thus constitute
the implementation of the fallback \gls{rule} for~$r$\!.


\paragraph{Advantages}

Subsequent experiments and evaluations showed that this design proved simpler
and more general than contemporary designs~\cite{Graham:1980,
  GanapathiEtAl:1982:Survey, GrahamEtAl:1982, LandwehrEtAl:1982,
  AigrainEtAl:1984}.
%
Due to this, the \gls{Glanville-Graham approach} has been acknowledged as one of
the most significant breakthroughs in this field which has influenced many later
techniques in one way or another.
%
In addition, by relying on a state table a \glsshort{Glanville-Graham
  approach}-style \gls{instruction selector} is completely table-driven since it
is implemented by a core that basically consists of a series of table
lookups.\!%
%
\footnote{%
  \textcite{Pennello:1986} developed a technique to express the state table
  directly as \gls{assembly code}, thus eliminating even the need to perform
  table lookups.
  %
  This was reported to improve the efficiency of \gls{LR parsing} by six to ten
  times.%
}
%
Hence the time it takes for the \gls{instruction selector} to generate the
\gls{assembly code} is linearly proportional to the size of the \gls{expression
  tree}.
%
Although the idea of table-driven code generation was not novel in itself -- we
have seen several examples of it in \refAppendix{macro-expansion} -- earlier
attempts had all failed to provide an automated procedure for producing the
tables.
%
In addition, many decisions regarding \gls{pattern selection} are precomputed by
resolving \glsshort{shift-reduce conflict} and \glspl{reduce-reduce conflict} at
the time that the state table is generated, thus reducing compilation time.

Another advantage of the \gls{Glanville-Graham approach} is its formal
foundation, which enables means of automatic verification.
%
For instance, \textcite{Emmelmann:1992:Testing} presented one of the first
methods of proving the completeness of an \gls{machine grammar}.\!%
%
\footnote{%
  Note, however, that even though an \gls{machine grammar} has been proven to be
  complete, a greedy \gls{instruction selector} may still fail to use a
  necessary \gls{rule}.
  %
  Consequently, \citeauthor{Emmelmann:1992:Testing}'s checker assumes that an
  optimal \gls{instruction selector} will be used for the proven \gls{machine
    grammar}.%
}
%
The intuition behind \citeauthor{Emmelmann:1992:Testing}'s automatic prover is
to find all \glspl{expression tree} that can appear in the \gls{function} but
cannot be handled by the \gls{instruction selector}.
%
Let us denote an \gls{machine grammar} by $G$ and a \gls{grammar} describing the
\glspl{expression tree} by $T$\!.
%
If we further use $L(X)$ to represent the set of all \glspl{tree} accepted by a
\gls{grammar}~$X$, we can then determine whether the \gls{machine grammar} is
incomplete by checking if $L(T) \setminus L(G)$ yields a nonempty set.
%
\citeauthor{Emmelmann:1992:Testing} recognized that this intersection can be
computed by creating a \gls{product automaton} which essentially implements the
language that accepts only the \glspl{tree} in this set of counterexamples.
%
From this automaton it is also possible to derive the \glspl{rule} that are
missing from the \gls{machine grammar}.
%
\textcite{BrandnerEtAl:2010} recently extended this method to handle
\glspl{production} that contain \glspl{predicate} -- we will discuss these
shortly when exploring \glspl{attribute grammar} -- by splitting
\glspl{terminal} to expose these otherwise-hidden characteristics.


\paragraph{Disadvantages}

Although it addressed several of the problems with contemporary \gls{instruction
  selection} techniques, the \gls{Glanville-Graham approach} also had
disadvantages of its own.
%
First, since an \gls{LR parser} can only reason on syntax, any restrictions
regarding specific values or ranges must be captured by its own
\gls{nonterminal}.
%
In conjunction with the limitation that each \gls{production} can match only a
single \gls{pattern}, this typically meant that \glspl{rule} for versatile
\glspl{instruction} with several \glsshort{addressing mode} or operand modes had
to be duplicated for each such mode.
%
For most \glspl{target machine} this turned out to be impracticable.
%
For example, in the case of the \gls{VAX}~machine -- a \gls!{CISC} architecture
from the 1980s, where each \gls{instruction} accepted a multitude of operand
modes~\cite{Ceruzzi:2003} -- the \gls{machine grammar} would contain over eight
million \glspl{rule}~\cite{GrahamEtAl:1982}.
%
By introducing auxiliary \glspl{nonterminal} to combine features shared among
the \glspl{rule} -- a task called \gls!{refactoring} -- the number was brought
down to about a thousand \glspl{rule}, but this had to be done carefully to not
have a negative impact on code quality.
%
Second, since the parser traverses from left to right without backtracking,
\gls{assembly code} regarding one operand has to be emitted before any other
operand can be observed.
%
This can potentially lead to poor decisions which later have to be undone by
emitting additional code, as in the case of recovering from \gls{syntactic
  blocking}.
%
Hence, to design an \gls{machine grammar} that was both compact and yielded good
code quality, the developer had to possess extensive knowledge about the
implementation of the \gls{instruction selector}.


\subsection{Extending Grammars with Semantic Handling}
\labelSection{attribute-grammars}

In purely \glspl{context-free grammar} there is just no way to handle semantic
information.
%
For example, the exact \gls{register} represented by a
$\mNT{reg}$~\gls{nonterminal} is not available.
%
\citeauthor{GlanvilleGraham:1978} worked around this limitation by pushing the
information onto the stack, but even then their modified \gls{LR parser} could
reason upon it using only simple equality comparisons.
%
\citeauthor{GanapathiEtAl:1982:AttrGr}~\cite{Ganapathi:1980,
  GanapathiEtAl:1982:AttrGr, GanapathiFischer:1984, GanapathiFischer:1985}
addressed this problem by replacing the use of traditional, \glspl{context-free
  grammar} with the use of a more powerful set of \glspl{grammar} known as
\glspl!{attribute grammar}.
%
There are also \glspl!{affix grammar}, which can be thought of as a subset of
\glspl{attribute grammar}.
%
In this dissertation, however, we will only consider \glspl{attribute grammar},
and, as with the \gls{Glanville-Graham approach}, we will discuss how they work
only at a high level.


\paragraph{Attribute Grammars}

\Glspl{attribute grammar} were introduced in 1968 by \textcite{Knuth:1968}, who
extended \glspl{context-free grammar} with \glspl{attribute}.
%
\Glspl!{attribute} are used to store, manipulate, and propagate additional
information about individual \glspl{terminal} and \glspl{nonterminal} during
parsing, and an \gls{attribute} is either \gls!{synthesized.a} or
\gls!{inherited.a}.
%
Using \glspl{parse tree} as the point of reference, a \gls{node} with a
\gls{synthesized.a} \gls{attribute} forms its value from the \glspl{attribute}
of its \glspl{child}, and a \gls{node} with an \gls{inherited.a} \gls{attribute}
copies the value from the \gls{parent}.
%
 Consequently, information derived from \gls{synthesized.a} \glspl{attribute}
 flows \emph{upwards} along the \gls{tree} while information derived from
 \gls{inherited.a} \glspl{attribute} flows \emph{downwards}.
%
We therefore distinguish between \gls{synthesized.a} and \gls{inherited.a}
\glspl{attribute} by a $\uparrow$ or~$\downarrow$, respectively, which will be
prefixed to the \gls{attribute} of the concerned \gls{symbol}.
%
For example, the \gls{synthesized.a} attribute~$\mAttr{x}$ of a
$\mNT{Reg}$~\gls{nonterminal} is written as \mbox{$\mNT{Reg}\uparrow\mAttr{x}$}.

The \glspl{attribute} are then used within \glspl{predicate} and \glspl{action}.
%
\Glspl!{predicate} are used for checking the applicability of a \gls{rule}, and,
in addition to emitting \gls{assembly code}, \glspl{action} are used to produce
new \gls{synthesized.a} \glspl{attribute}.
%
Hence, when modeling \glspl{instruction} we can use \glspl{predicate} to express
the constraints, and \glspl{action} to indicate effects, such as code emission,
and which \gls{register} the result will be stored in.
%
Let us look at an example.

In \refTable{ganapathi-fischer-machine-grammar} we see a set of \glspl{rule} for
modeling three byte-adding \glspl{instruction}:
%
\begin{table}
  \centering%
  \figureFont\figureFontSize%
  \def\mSynAttr#1{\uparrow\mAttr{#1}}%
  \def\mInhAttr#1{\downarrow\mAttr{#1}}%
  \def\mByteSynAttr#1{\mNT{Byte}\mSynAttr{#1}}%
  \begin{tabular}{rl@{$\; \rightarrow \;$}lll}
    \toprule
        \tabhead \#
      & \multicolumn{2}{c}{\tabhead production}
      & \multicolumn{1}{c}{\tabhead predicates}
      & \multicolumn{1}{c}{\tabhead actions}\\
    \midrule
        1
      & $\mByteSynAttr{r}$
      & \cCode{$+$} $\mByteSynAttr{a}$ $\mByteSynAttr{r}$
      & $\mPredicate{IsOne}(\mInhAttr{a})$, $\mPredicate{NotBusy}(\mInhAttr{r})$
      & emit \instrCode{incb $\mInhAttr{r}$}\\
        2
      & $\mByteSynAttr{r}$
      & \cCode{$+$} $\mByteSynAttr{r}$ $\mByteSynAttr{a}$
      & $\mPredicate{IsOne}(\mInhAttr{a})$, $\mPredicate{NotBusy}(\mInhAttr{r})$
      & emit \instrCode{incb $\mInhAttr{r}$}\\
        3
      & $\mByteSynAttr{r}$
      & \cCode{$+$} $\mByteSynAttr{a}$ $\mByteSynAttr{r}$
      & $\mPredicate{TwoOp}(\mInhAttr{a}, \mInhAttr{r})$
      & emit \instrCode{addb2 $\mInhAttr{a}$, $\mInhAttr{r}$}\\
        4
      & $\mByteSynAttr{r}$
      & \cCode{$+$} $\mByteSynAttr{r}$ $\mByteSynAttr{a}$
      & $\mPredicate{TwoOp}(\mInhAttr{a}, \mInhAttr{r})$
      & emit \instrCode{addb2 $\mInhAttr{a}$, $\mInhAttr{r}$}\\
        5
      & $\mByteSynAttr{r}$
      & \cCode{$+$} $\mByteSynAttr{a}$ $\mByteSynAttr{b}$
      &
      & get register $\mSynAttr{r}$\\
      & \multicolumn{3}{c}{}
      & emit \instrCode{addb3 $\mInhAttr{r}$, $\mInhAttr{a}$, $\mInhAttr{b}$}\\
    \bottomrule
  \end{tabular}

  \caption[Example of an instruction set expressed as attribute grammar]
          {%
            Example of an instruction set expressed as attribute
            grammar~\cite{GanapathiEtAl:1982:AttrGr}%
          }
  \labelTable{ganapathi-fischer-machine-grammar}
\end{table}
%
\begin{enumerate*}[label=(\roman*), itemjoin={;\ }, itemjoin*={; and\ }]
  \item an increment version \cCode*{incb} (increments a \gls{register}
    by~\num{1}, modeled by \glspl{rule}~\num{1} and~\num{2})
  \item a two-address version \cCode*{add2b} (adds two \glspl{register} and
    stores the result in one of the operands, modeled by \glspl{rule}~\num{3}
    and~\num{4})
  \item a three-address version \cCode*{add3b} (the result can be stored
    elsewhere, modeled by rule~\num{5})
\end{enumerate*}.
%
Naturally, the \cCode*{incb} \gls{instruction} can only be used when one of the
operands is a constant of value~\num{1}, which is checked by the
$\mPredicate{IsOne}$~\gls{predicate}.
%
In addition, since this \gls{instruction} destroys the previous value of the
\gls{register}, it can only be used when no subsequent operation uses the old
value (meaning the \gls{register} is not ``busy''), which is checked by the
$\mPredicate{NotBusy}$~\gls{predicate}.
%
The emit~\gls{action} then emits the corresponding \gls{assembly code}.
%
Since addition is commutative, we require two \glspl{rule} to make the
\gls{instruction} applicable in both cases.
%
Similarly, we have two \glspl{rule} for the \cCode*{add2b}~\gls{instruction},
but the \glspl{predicate} have been replaced by a $\mPredicate{TwoOp}$, which
checks if one of the operands is the target of assignment or if the value is not
needed afterwards.
%
Since the last \gls{rule} does not have any \glspl{predicate}, it also acts as
the default \gls{rule}, thus preventing situations of \gls{semantic blocking}
which we discussed when covering the \gls{Glanville-Graham approach}.


\paragraph{Advantages}

The use of \glspl{predicate} removes the need of introducing new
\glspl{nonterminal} for expressing specific values and ranges, resulting in a
more concise \gls{machine grammar} compared to a \gls{context-free grammar}.
%
For example, for the \gls{VAX}~machine, the use of \glspl{attribute} leads to a
\gls{grammar} half the size (around \num{600}~\glspl{rule}) compared to that
required for the \gls{Glanville-Graham approach}, even without applying any
extensive \gls{refactoring}~\cite{GanapathiFischer:1985}.
%
\Glspl{attribute grammar} also facilitate incremental development of the
\glspl{machine description}.
%
One can start by implementing the most general \glspl{rule} to achieve an
\gls{machine grammar} that produces correct but inefficient code.
%
\Glspl{rule} for handling more complex \glspl{instruction} can then can be added
incrementally, making it possible to balance implementation effort against code
quality.
%
Another useful feature is that other \gls{program} optimization routines, such
as \gls{constant folding}, can be expressed as part of the \gls{grammar} instead
of as a separate component.
%
\textcite{Farrow:1982} even made an attempt at deriving an entire \gls{Pascal}
\gls{compiler} from an \gls{attribute grammar}.


\paragraph{Disadvantages}

To permit \glspl{attribute} to be used together with \gls{LR parsing}, the
properties of the \gls{machine grammar} must be restricted.
%
First, only \gls{synthesized.a} \glspl{attribute} may appear in
\glspl{nonterminal}.
%
This is because an \gls{LR parser} constructs the \gls{parse tree} bottom-up and
left-to-right, starting from the \glspl{leaf} and working its way up towards the
\gls{root}.
%
Hence an \gls{inherited.a} value only becomes available after the \gls{subtree}
of its \gls{nonterminal} has been constructed.
%
Second, since \glspl{predicate} may render a \gls{rule} as semantically invalid
for \gls{rule reduction}, all \glspl{action} must appear last in the
\glspl{rule}.
%
Otherwise they may cause effects that must be undone after a \gls{predicate}
fails its check.
%
Third, as with the \gls{Glanville-Graham approach}, the parser has to take
decisions regarding one \gls{subtree} without any consideration of sibling
\glspl{subtree} that may appear to the right.
%
This can result in \gls{assembly code} that could have been improved if all
\glspl{subtree} had been available beforehand, and this is again a limitation
due to the use of \gls{LR parsing}.
%
\textcite{Ganapathi:1989} later made an attempt to resolve this problem by
implementing an \gls{instruction selector} in \gls{Prolog} -- a logic-based
programming language -- but this incurred exponential worst-case time complexity
of the \gls{instruction selector}.


\subsection{Maintaining Multiple Parse Trees for Better Code Quality}

Since \glspl{LR parser} make a single pass over the \glspl{expression tree} --
and thus only produce one out of many possible \glspl{parse tree} -- the quality
of the produced \gls{assembly code} is heavily dependent on the \gls{machine
  grammar} to guide the parser in finding a ``good'' \gls{parse tree}.

\textcite{ChristopherEtAl:1984} attempted to address this concern by using the
concepts of the \gls{Glanville-Graham approach} but extending the parser to
produce \emph{all} \glspl{parse tree}, and then select the one that yields the
best \gls{assembly code}.
%
This was achieved by replacing the original \gls{LR parser} with an
implementation of \citeauthor{Earley:1970}'s algorithm~\cite{Earley:1970}, and
although this scheme certainly improves code quality -- at least in theory -- it
does so at the cost of enumerating all \glspl{parse tree}, which is often too
expensive in practice.

In 2000, \textcite{MadhavanEtAl:2000} extended the \gls{Glanville-Graham
  approach} to achieve \gls{optimal.ps} selection of \glspl{pattern} while
allegedly retaining the linear time complexity of \gls{LR parsing}.
%
By incorporating a new version of \gls{LR parsing}~\cite{ShankarEtAl:2000},
\glsshort{rule reduction}s that were previously executed directly as matching
\glspl{rule} were found are now allowed to be postponed by an arbitrary number
of steps.
%
Hence the \gls{instruction selector} essentially keeps track of multiple
\glspl{parse tree}, allowing it to gather enough information about the
\gls{function} before committing to a decision that could turn out to be
suboptimal.
%
In other words, as in the case of \citeauthor{ChristopherEtAl:1984} the design
by \citeauthor{MadhavanEtAl:2000} also covers all \glspl{parse tree} but it
immediately discards those determined to result in less efficient \gls{assembly
  code}
%
Hence the scheme resembles the \gls{branch and bound}~\gls{search} strategy (see
\refChapter{constraint-programming} on \refPage{cp-branch-and-bound}).

To do this efficiently, the design also incorporates \gls{offline cost
  analysis}, which we will explore later in
\refSection{precomputing-cost-computations}.
%
More recently, \textcite{Yang:2010} proposed a similar technique involving the
use of \glspl!{parser cactus}, where deviating \glspl{parse tree} are branched
off a common trunk to reduce space requirements.
%
In both designs, however, the underlying \gls{principle} still prohibits the
modeling of many typical \gls{target machine} features such as
\gls{multi-output.ic} \glspl{instruction} since their \glspl{grammar} only allow
\glspl{rule} that produce a single result.


\section{Using Recursion to Cover Trees Top-Down}
\labelSection{tc-top-down-approaches}

The \gls{tree covering} techniques we have examined so far -- in particular
those based on \gls{LR parsing} -- all operate \emph{bottom-up}.
%
The \gls{instruction selector} begins to cover the \glspl{leaf} in the
\gls{expression tree}.
%
Based on the decisions taken for the \glspl{subtree}, it then progressively
works upwards along the \gls{tree} until it reaches the \gls{root}, continually
matching and selecting applicable \glspl{pattern} along the way.
%
This is by no means the only method of covering, as it can also be done
\emph{top-down}.
%
In such designs, the \gls{instruction selector} covers the \gls{expression tree}
starting from the \gls{root}, and then recursively works its way downwards.
%
Consequently, the flow of semantic information, such as the particular
\gls{register} in which a result will be stored, is also different.
%
A bottom-up \gls{instruction selector} lets this information trickle upwards
along the \gls{expression tree} -- either via auxiliary data structures or
through \gls{tree rewriting} -- whereas a top-down implementation decides upon
this beforehand and pushes this information downwards.
%
The latter is therefore said to be \gls!{goal-driven.ps}, as \gls{pattern
  selection} is guided by a set of additional requirements which must be
fulfilled by the selected \gls{pattern}.
%
Since this in turn will incur new requirements for the \glspl{subtree}, most
top-down techniques are implemented recursively.
%
This also enables backtracking, which is a necessary feature, as selection of
certain \glspl{pattern} can cause the lower parts of the \gls{expression tree}
to become uncoverable.


\subsection{First Applications}
\labelSection{tree-covering-first-approaches}

\paragraph{Using Means-End Analysis to Guide Instruction Selection}
\labelSection{tc-means-end-analysis-approach}

To the best of the author's knowledge, \textcite{Newcomer:1975} was the first to
develop a scheme that uses top-down \gls{tree covering} to address
\gls{instruction selection}.
%
In his 1975 doctoral dissertation, \citeauthor{Newcomer:1975} proposes a design
that exhaustively finds all combinations of \glspl{pattern} that cover a given
\gls{expression tree}, and then selects the one with lowest cost.
%
\textcite{Cattell:1977} also describes this in his survey paper, which is the
main source for the discussion of \citeauthor{Newcomer:1975}'s design.

The \glspl{instruction} are modeled as \glspl!{T-operator}, which are basically
\glspl{pattern tree} with costs and \glspl{attribute} attached.
%
The \glspl{attribute} describe various restrictions, such as which
\glspl{register} can be used for the operands.
%
There is also a set of \glspl{T-operator} that the \gls{instruction selector}
uses to perform necessary transformations of the \gls{function} -- its need will
become clear as the discussion continues.
%
The scheme takes an \gls{AST} as expected input and then covers it following the
aforementioned top-down approach.
%
The \gls{instruction selector} first attempts to find all matching
\glspl{pattern} for the \gls{root} of the~\gls{AST}, and then proceeds to
recursively cover the remaining \glspl{subtree} for each \gls{match}.
%
\Gls{pattern matching} is done using a straightforward technique that we know
from before (see \refAlgorithm{tree-based-pattern-matching} on
\refAlgorithm{tree-based-pattern-matching}).
%
For efficiency, all \glspl{pattern} are indexed according to the type of their
\gls{root}.
%
The result of this procedure is thus a set of \gls{pattern} sequences each of
which covers the entire~\gls{AST}.
%
Afterwards, each sequence is checked for whether the \glspl{attribute} of its
\glspl{pattern} are equal to those of a \gls!{PAS}, which corresponds to a goal.
%
If not, the \gls{instruction selector} will attempt to rewrite the \gls{subtree}
using the transformation \glspl{T-operator} until the \glspl{attribute} match.
%
To guide this process, \citeauthor{Newcomer:1975} applied a heuristic search
strategy known as \gls!{means-end analysis}, which was introduced by
\textcite{NewellSimon:1959} in~1959.
%
The intuition behind \gls{means-end analysis} is to recursively minimize the
quantitative difference between the current state (that is, what the
\gls{subtree} looks like \emph{now}) and a goal state (what it \emph{should}
look like).
%
How to calculate this quantitative difference, however, is not mentioned
in~\cite{Cattell:1977}.
%
To avoid infinite looping, the transformation process stops once it reaches a
certain depth in the search space.
%
If successful, the applied transformations are inserted into the \gls{pattern}
sequence; if not, the sequence is dropped.
%
From the found \gls{pattern} sequences the one with the lowest total cost is
selected, followed by \gls{assembly code} emission.

\citeauthor{Newcomer:1975}'s design was pioneering as its application of
\gls{means-end analysis} made it possible to guide the process of modifying the
\gls{function}, without having to resort to target-specific mechanisms, until it
could be implemented on the \gls{target machine}.
%
But the design also had several significant flaws.
%
First, it had little practical application, as \citeauthor{Newcomer:1975}'s
implementation only handled arithmetic expressions.
%
Second, the \glspl{T-operator} used for modeling the \glspl{instruction} as well
as transformations had to be constructed by hand -- a task that was far from
trivial -- which hindered \gls{compiler} retargetability.
%
Third, the process of transforming the \gls{function} could end prematurely due
to the search space cut-off, causing the \gls{instruction selector} to fail to
generate any \gls{assembly code} whatsoever.
%
Lastly, the search strategy proved much too expensive to be usable in practice
except for very small \glspl{expression tree}.


\paragraph{Making Means-End Analysis Work in Practice}

\citeauthor{CattellEtAl:1979}~\cite{CattellEtAl:1979, Cattell:1980,
  LeverettEtAl:1980} later improved and extended \citeauthor{Newcomer:1975}'s
work into a more practical framework which was implemented in the \gls!{PQCC}, a
derivation of the \gls!{Bliss-11} \gls{compiler} originally written by
\textcite{WulfEtAl:1975}.
%
Instead of performing the \gls{means-end analysis} as the \gls{function} is
compiled, their design does it as a preprocessing step when generating the
\gls{compiler} itself -- much as with the \gls{Glanville-Graham approach}.

The \glspl{pattern} are expressed as a set of \glspl{template} which are formed
using recursive composition, and are thus similar to the \glspl{production}
found in \glspl{machine grammar}.
%
But unlike \citeauthor{GlanvilleGraham:1978}'s and
\citeauthor{GanapathiEtAl:1982:AttrGr}'s designs -- where the \glspl{grammar}
were written by hand -- the \glspl{template} in \gls{PQCC} are derived
automatically from a \gls{machine description}.
%
Each \gls{instruction} is modeled as a set of \glspl!{machine operation} that
describe the effects of the \gls{instruction}.
%
The \glspl{machine operation} are thus akin to the \glspl{RTL} introduced by
\textcite{Fraser:1979} in \refAppendix{macro-expansion} on
\refPageOfSection{me-register-transfer-lists}.
%
These effects are then used by a separate tool, called the \gls!{CGG}, to create
the \glspl{template} which will be used by the \gls{instruction selector}.

In addition to producing the trivial \glspl{template} corresponding directly to
an \gls{instruction}, \gls{CGG} also produces a set of single-\gls{node}
\glspl{pattern} as well as a set of larger \glspl{pattern} that combine several
\glspl{instruction}.
%
The former ensures that the \gls{instruction selector} is capable of generating
\gls{assembly code} for all \glspl{function} (since any \gls{expression tree}
can thereby be trivially covered), while the latter reduces compilation time as
it is quicker to match a large \gls{pattern} than many smaller ones.
%
To do this, \gls{CGG} uses a combination of \gls{means-end analysis} and
heuristic \glspl{rule} which apply a set of axioms (such as \mbox{$\mNot \mNot E
  \Leftrightarrow E$}, \mbox{$E + 0 \Leftrightarrow E$}, and \mbox{$\mNot (E_1
  \geq E_2) \Leftrightarrow E_1 < E_2$}) to manipulate and combine existing
\glspl{pattern} into new ones.
%
However, there are no guarantees that these ``interesting'' \glspl{pattern} will
ever be applicable in practice.
%
Once generated, \gls{instruction selection} is performed in a greedy, top-down
fashion that always selects the lowest-cost \gls{template} matching the current
\gls{node} in the \gls{expression tree}.
%
\Gls{pattern matching} is done using a scheme identical to that of
\citeauthor{Newcomer:1975}.
%
If there is a tie, the \gls{instruction selector} picks the \gls{template} with
the least number of memory loads and stores.

Compared to the \gls{LR parsing}-based methods discussed previously, the design
by \citeauthor{CattellEtAl:1979} has both advantages and disadvantages.
%
The main advantage is that the \glspl{instruction selector} is less at risk of
failing to generate \gls{assembly code} for some \gls{function}.
%
There is the possibility that the set of predefined \glspl{template} is
insufficient to produce all necessary single-node \glspl{pattern}.
%
In such cases, however, \gls{CGG} can at least issue a warning (in
\citeauthor{GanapathiEtAl:1982:AttrGr}'s design this correctness has to be
ensured by the \gls{grammar} designer).
%
The disadvantage is that it is relatively slow.
%
Whereas the \gls{tree parsing}-based \glspl{instruction selector} exhibit linear
time complexity -- both for \gls{pattern matching} and selection -- the
\gls{instruction selector} by Cattell~\etal has to match each \gls{template}
individually, which could take quadratic time in the worst case.


\paragraph{Recent Designs}

\glsunset{BURS}

To the best of the author's knowledge, the only recent technique (less than
\num{20}~years old) to use this kind of recursive top-down methodology for
\gls{tree covering} is that of
\citeauthor{NymeyerEtAl:1996}~\cite{NymeyerEtAl:1996, NymeyerKatoen:1997}.
%
In two papers from 1996 and~1997, \citeauthor{NymeyerEtAl:1996} introduce a
method where \gls{A* search} -- another strategy for exploring the search space
(see~\cite{RussellNorvig:2010}) -- is combined with \gls{BURS} theory.
%
We will discuss \gls{BURS} theory in more detail later in this appendix (the
eager reader can skip directly to \refSection{precomputing-cost-computations}).
%
Until then, let it for now be sufficient to say that \glspl{grammar} based on
\gls{BURS} allow transformation \glspl{rule}, such as rewriting \mbox{$X + Y$}
into \mbox{$Y + X$}, to be included as part of the \gls{machine grammar}.
%
This potentially simplifies and reduces the number of \glspl{rule} required for
expressing the \glspl{instruction}, but unfortunately the authors did not
publish any experimental results.
%
Hence it is difficult to judge whether the \mbox{\glsshort{A*
    search}-\gls{BURS}} theory combination would be an applicable technique in
practice.

\glsreset{BURS}


\subsection{A Note on Tree Rewriting \versus Tree Covering}

At this point some readers may feel that \gls{tree rewriting} -- where
\glspl{pattern} are iteratively selected for rewriting the \gls{expression tree}
until it consists of a single \gls{node} of some goal type -- is something
entirely different compared to \gls{tree covering} -- where compatible
\glspl{pattern} are selected for covering all \glspl{node} in the
\gls{expression tree}.
%
The same argument applies to \glsshort{DAG covering} and \gls{graph covering},
although rewriting-based techniques are less common for those \glspl{principle}.
%
Indeed, there appears to be a subtle difference, but a valid solution to a
problem expressed using \gls{tree rewriting} is also a valid solution to the
equivalent problem expressed using \gls{tree covering}, and vice versa.
%
It could therefore be argued that the two are interchangeable, but we regard
\gls{tree rewriting} as a \emph{means} to solving the \gls{tree covering}
problem, which we regard as the fundamental \emph{\gls{principle}}.


\paragraph{Handling Chain Rules in Purely Coverage-Driven Designs}

Another objection that may arise is how \gls{tree covering}, as a
\gls{principle}, can support \gls{chain.r} \glspl{rule}.
%
A \gls{chain.r} \gls{rule} is a \gls{rule} whose \gls{pattern} consists of a
single \gls{nonterminal}, and the name comes from the fact that \glsshort{rule
  reduction}s using these \glspl{rule} can be chained together one after
another.
%
Consequently, \gls{chain.r} \glspl{rule} are often used to represent data
transfers and other value-preserving transformations (an example of this is
given in \refAppendix{graph-covering}).

Let us first assume that we have as input the \gls{expression tree} shown in
\refFigure{tree-covering-dilemma-input}, which will be covered using the
\gls{machine grammar} shown in \refFigure{tree-covering-dilemma-grammar}.
%
Let us further assume that we have an \gls{instruction selector} where
\gls{pattern matching} is performed strictly through \gls{node} comparison.
%
This \gls{instruction selector} is clearly based on \gls{tree covering}, but it
will fail to find a valid cover for the aforementioned \gls{expression tree} as
it will not be able to match and select the necessary \gls{chain.r} \glspl{rule}
(see \refFigure{tree-covering-dilemma-invalid-cover}).

\begin{figure}
  \centering%

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Expression tree to cover.
                  %
                  Variables~\cVar*{a} and~\cVar*{b} are assumed to be stored
                  in A-type registers%
                  \labelFigure{tree-covering-dilemma-input}%
                }%
                [62mm]%
                {%
                  \input{figures/tree-covering/tree-covering-dilemma-input}
                }%
  \hfill%
  \subcaptionbox{%
                  Machine grammar%
                  \labelFigure{tree-covering-dilemma-grammar}%
                }%
                {%
                  \figureFontSize%
                  \begin{tabular}{cl@{ $\rightarrow$ }l}
                    \toprule
                        \tabhead \#
                      & \multicolumn{2}{c}{\tabhead production}\\
                    \midrule
                        1
                      & $\mNT{RegA}$
                      & \irCode{reg}\\
                        2
                      & $\mNT{RegB}$
                      & $\mNT{RegA}$\\
                        3
                      & $\mNT{RegB}$
                      & $\irCode{\irAddText} \; \mNT{RegB} \; \mNT{RegB}$\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \mbox{}

  \vspace{\betweensubfigures}

  \subcaptionbox{%
                  Invalid cover due to incompatibilities between the selected
                  patterns%
                  \labelFigure{tree-covering-dilemma-invalid-cover}%
                }%
                [36mm]%
                {%
                  \input{%
                    figures/tree-covering/tree-covering-dilemma-invalid-cover%
                  }%
                }%
  \hfill%
  \subcaptionbox{%
                  Covering using the transitive closure method%
                  \labelFigure{tree-covering-dilemma-combined-cover}%
                }%
                {%
                  \input{%
                    figures/tree-covering/tree-covering-dilemma-combined-cover%
                  }%
                }%
  \hfill%
  \subcaptionbox{%
                  Covering using the augmentation method%
                  \labelFigure{tree-covering-dilemma-augmented-cover}%
                }%
                [34mm]%
                {%
                  \input{%
                    figures/tree-covering/tree-covering-dilemma-augmented-cover%
                  }%
                }

  \caption[Examples illustrating how chain rules can be supported]%
          {%
            Examples illustrating how chain rules can be supported by tree
            covering-based techniques.
            %
            The numbers represent rule numbers%
          }%
  \labelFigure{tree-covering-dilemma}
\end{figure}

There are three ways of solving this problem.
%
The simplest method is to simply ignore the incompatibilities during
\gls{pattern selection}, and then inject the \gls{assembly code} for the
necessary \gls{chain.r} \glspl{rule} afterwards.
%
But this obviously compromises code quality as the cost of the \gls{chain.r}
\glspl{rule} is not taken into account.
%
A better approach is to consider all \gls{chain.r} \gls{rule} applications
during \gls{pattern matching}, thus essentially combining regular
\glspl{pattern} with \gls{chain.r} \glspl{rule} to yield new \glspl{pattern}
(see \refFigure{tree-covering-dilemma-combined-cover}).
%
Finding all such combinations is known as computing the \gls!{transitive
  closure}.
%
The third and last approach is to augment the \gls{expression tree} by inserting
auxiliary \glspl{node}, each of which each represents the application of a
\gls{chain.r} \gls{rule} (see
\refFigure{tree-covering-dilemma-augmented-cover}).

The \gls{transitive closure} and augmentation methods both come with certain
benefits and drawbacks.
%
The former method allows \gls{chain.r} \glspl{rule} to be applied in any
combination and of any length, but it complicates the tasks of \gls{pattern
  matching} and \gls{pattern selection}.
%
The latter method requires no change in the \gls{pattern matcher} and
\gls{pattern selector}.
%
However, it enlarges the \gls{expression tree} and requires an additional dummy
\gls{rule} to indicate that no \gls{chain.r} \gls{rule} is applied.
%
If more than one \gls{chain.r} \gls{rule} needs to be applied, then several
auxiliary \glspl{node} must be inserted one after another.
%
As we have seen, several designs ignore this problem by assuming a homogeneous
target architecture, and a few techniques apply the inefficient idea of code
injection.
%
The \gls{transitive closure} approach is typically limited to \gls{tree
  covering}-based methods, while the augmentation method is mostly applied when
covering more general forms such as \glsdesc{DAG}s and \glspl{graph} (which we
will discuss in the coming appendices).


\section{Separating Pattern Matching from Pattern Selection}
\labelSection{tc-separating-matching-and-selection}

In the previously discussed techniques based on \gls{tree covering}, the tasks
of \gls{pattern matching} and \gls{pattern selection} are unified into a single
step.
%
Although this enables single-pass \gls{code generation}, it typically also
prevents the \gls{instruction selector} from considering the impact of certain
combinations of \glspl{pattern}.
%
By separating these two concerns and allowing the \gls{instruction selector} to
make multiple passes over the \gls{expression tree}, it can gather enough
information about all applicable \glspl{pattern} before having to commit to
premature decisions.

But the \glspl{pattern matcher} we have seen so far -- excluding those based on
\gls{LR parsing} -- have all been implementations of algorithms with quadratic
time complexity.
%
Fortunately, we can do better.


\subsection{Algorithms for Linear-Time, Tree-Based Pattern Matching}

Over the years many algorithms have been discovered for finding all
\glspl{match} given a subject \gls{tree} and a set of \glspl{pattern tree} (see
for example~\cite{KarpEtAl:1972, HoffmannODonnell:1982, PurdomBrown:1985,
  WeisgerberWilhelm:1988, RameshRamakrishnan:1992, DubinerEtAl:1994,
  ChenEtAl:1995, ColeHariharan:1997, ShamirTsur:1999, WuuYang:2000}).
%
For \gls{tree covering}, most \gls{pattern matching} algorithms have been
derived from methods of string-based \gls{pattern matching}.
%
This was first discovered by \textcite{KarpEtAl:1972} in~1972, and their ideas
were later extended by \textcite{HoffmannODonnell:1982} to form the algorithms
most applied by \gls{tree}-based \gls{instruction selection} techniques.
%
Hence, in order to understand \gls{pattern matching} with \glspl{tree}, let us
first explore how this is done with strings.


\paragraph{Matching Trees Is Equivalent to Matching Strings}

The algorithms most commonly used for string matching were introduced by
\textcite{AhoCorasick:1975} and \textcite{KnuthEtAl:1977} (also known as the
\gls!{Knuth-Morris-Pratt algorithm}) in 1975 and~1977, respectively.
%
Independently discovered from one another, both algorithms operate in the same
fashion and are thus nearly identical in their approach.
%
The intuition is that when a partial \gls{match} of a \gls{pattern} with a
repetitive substring fails, the \gls{pattern matcher} does not need to return
all the way to the input character where the matching initially started.
%
This is illustrated in \refTable{string-matching-example}, where the
\gls{pattern} string \cCode*{abcabd} is matched against the input string
\cCode*{abcabcabd}.
%
\begin{table}
  \centering%
  \figureFont\figureFontSize%
  \begin{tabular}{rccccccccc}
    \toprule
      & \tabhead 0
      & \tabhead 1
      & \tabhead 2
      & \tabhead 3
      & \tabhead 4
      & \tabhead 5
      & \tabhead 6
      & \tabhead 7
      & \tabhead 8\\
    \midrule
       \tabhead input string
      & a & b & c & a & b & c & a & b & d\\
       \tabhead pattern string
      & a & b & c & a & b & d &   &   &  \\
      &   &   &   &   &   & $\uparrow$
                              &   &   &  \\
      &   &   &   & a & b & c & a & b & d\\
      &   &   &   &   &   &   & $\uparrow$
                                  &   &  \\
    \bottomrule
  \end{tabular}

  \caption{Example of string matching without full backtracking}
  \labelTable{string-matching-example}
\end{table}
%
The arrow indicates the current character under consideration.
%
At first, the \gls{pattern} matches the beginning of the input string up until
the last character (position~\num{5}).
%
When this fails, instead of returning to position~\num{1} and restarting the
matching from scratch, the matcher remembers that the first three characters of
the \gls{pattern} (\cCode*{abc}) have already been matched at this point.
%
Therefore, it continues to position~\num{6} and attempts to match the fourth
character in the \gls{pattern}.
%
Hence all occurrences of the \gls{pattern} can be found in linear time.
%
We continue our discussion with \citeauthor{AhoCorasick:1975}'s design as it is
capable of matching multiple \glspl{pattern} whereas the algorithm of
\citeauthor{KnuthEtAl:1977} only considers a single \gls{pattern} (although it
can easily be extended to handle multiple \glspl{pattern} as well).

\citeauthor{AhoCorasick:1975}'s algorithm relies on three functions --
$\mFunFont{goto}$, $\mFunFont{failure}$, and $\mFunFont{output}$ -- where the
first function is implemented as a \gls{state machine} and the two latter ones
are implemented as simple table lookups.
%
How these are constructed is out of scope for our purpose -- the interested
reader can consult the referenced paper -- and we will instead illustrate how
the algorithm works on an example.
%
In \refFigure{string-matching-example} we see the corresponding functions for
matching the strings \cCode*{he}, \cCode*{she}, \cCode*{his}, and~\cCode*{hers}.
%
\begin{figure}
  \centering%

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Goto function, represented as a state machine%
                  \labelFigure{string-matching-example-goto}%
                }%
                [68mm]%
                {%
                  \input{%
                    figures/tree-covering/string-matching-goto-state-machine%
                  }%
                }%
  \hfill%
  \subcaptionbox{%
                  Output function%
                  \labelFigure{string-matching-state-example-output}%
                }%
                [30mm]%
                {%
                  \figureFont\figureFontSize%
                  \begin{tabular}{cc}
                    \toprule
                        $\mathtabhead{i}$
                      & $\mathtabhead{\mFunFont{output}(i)}$\\
                    \midrule
                        2
                      & $\mSet{\text{he}}$\\
                        5
                      & $\mSet{\text{she}, \text{he}}$\\
                        7
                      & $\mSet{\text{his}}$\\
                        9
                      & $\mSet{\text{hers}}$\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \mbox{}

  \vspace{\betweensubfigures}

  \subcaptionbox{%
                  Failure function%
                  \labelFigure{string-matching-state-example-failure}%
                }{%
                  \figureFont\figureFontSize%
                  \begin{tabular}{ccccccccccc}
                    \toprule
                        $\mathtabhead{i}$
                      & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
                        $\mathtabhead{\mFunFont{failure}(i)}$
                      & 0 & 0 & 0 & 0 & 1 & 2 & 0 & 3 & 0 & 3\\
                    \bottomrule
                  \end{tabular}%
                }

  \caption[Example of a string-matching machine]%
          {Example of a string-matching machine~\cite{AhoCorasick:1975}}
  \labelFigure{string-matching-example}
\end{figure}
%
As a character is read from an input string, say \cCode*{shis}, it is first
given as argument to the $\mFunFont{goto}$ function.
%
Having initialized to \gls{state machine} to state~\num{0},
$\mFunFont{goto}(\cVar*{s})$ first causes a transition to state~\num{3}, and
$\mFunFont{goto}(\cVar*{h})$ causes a subsequent transition to state~\num{4}.
%
For each successful transition to some state~$i$ we invoke
$\mFunFont{output}(i)$ to check whether some \gls{pattern} string has been
matched, but so far no \gls{match} has been found.
%
For the next input character \cVar*{i}, however, there exists no corresponding
\gls{edge} from the current state (that is, $\mFunFont{goto}(\cVar*{i})$ causes
a failure).
%
At this point $\mFunFont{failure}(4)$ is invoked, which dictates that the
\gls{state machine} should fall back to state~\num{1}.
%
We then retry $\mFunFont{goto}(\cVar*{i})$, which takes us to state~\num{6}.
%
With the last input character, $\mFunFont{goto}(\cVar*{s})$ causes a transition
to state~\num{7}, where $\mFunFont{output}(7)$ indicates a \gls{match} with the
\gls{pattern} string \cCode*{his}.


\paragraph{The Hoffmann-O'Donnell Algorithm}

\textcite{HoffmannODonnell:1982} developed two algorithms incorporating the
ideas of \citeauthor{AhoCorasick:1975} and \citeauthor{KnuthEtAl:1977}.
%
In a paper from~1982, \citeauthor{HoffmannODonnell:1982} first present an
\mbox{$\mBigO(np)$} algorithm that matches \glspl{pattern tree} in a top-down
fashion.
%
In the same paper, \citeauthor{HoffmannODonnell:1982} then present an
\mbox{$\mBigO(n+m)$} bottom-up algorithm that trades linear-time \gls{pattern
  matching} for longer preprocessing times ($n$ is the size of the
\gls{expression tree}, $p$~is the number of \glspl{pattern}, and $m$ is the
number of \glspl{match} found).

The bottom-up algorithm is outlined in
\refAlgorithm{hoffmann-odonnell-labeling}.
%
\begin{algorithm}[t]
  \DeclFunction{LabelTree}{expression tree $E$, set $T$ of lookup tables}%
  {%
    $n_E$ \Assign root node of $E$\;
    \Call{LabelNode}{$n_E$}\;

    \BlankLine
    \DeclFunction{LabelNode}{expression tree rooted at $n$}%
    {%
      \ForEach{child $m_i$ of $n$}{%
        \Call{LabelNode}{$m_i$}\;
      }
      $t$ \Assign node type of $n$\;
      label $n$ with $T_{t}[\text{labels of $m_1, \ldots, m_k$}]$\;
    }%
  }

  \caption[Hoffmann-O'Donnell tree labeling algorithm]%
          {%
            Hoffmann-O'Donnell algorithm for labeling expression
            trees~\cite{HoffmannODonnell:1982}%
          }%
  \labelAlgorithm{hoffmann-odonnell-labeling}
\end{algorithm}
%
Starting at the \glspl{leaf}, each \gls{node} is labeled with an identifier
denoting the set of \glspl{pattern} that match the \gls{subtree} rooted at that
\gls{node}.
%
We call this set the \gls!{match set}.
%
The label to assign a particular \gls{node} is retrieved by using the labels of
the \glspl{child} as indices in a table that is specific to the type of the
current \gls{node}.
%
For example, label lookups for \glspl{node} representing addition are done using
one table, while lookups for \glspl{node} representing subtraction are done
using another table.
%
The dimension of the table is equal to the number of \glspl{child} that the
\gls{node} may have.
%
For example, binary operation \glspl{node} have two-dimensional tables while
\glspl{node} representing constant values have \mbox{0-dimensional} tables,
which simply consist of a single value.
%
A fully labeled example is shown in
\refFigure{hoffmann-odonnell-example-labeled-exp-tree}, and the \glspl{match
  set} are then retrieved via a subsequent top-down traversal of the labeled
\gls{tree}.

Since the bottom-up algorithm introduced by \citeauthor{HoffmannODonnell:1982}
has had a historical impact on \gls{instruction selection}, we will spend some
time discussing the details of how the lookup tables are produced.


\paragraph{Definitions}

\def\patI{$\textsc{i}$}
\def\patII{$\textsc{ii}$}

We begin by introducing a few definitions.
%
To our aid, we will use two \glspl{pattern tree}~\patI{} and \patII, shown in
\refFigureList{hoffmann-odonnell-example-pattern-1,
  hoffmann-odonnell-example-pattern-2}, respectively.
%
The patterns in our \gls{pattern set} thus consist of \glspl{node} with
\glspl{symbol}~$a$, \mbox{$b$\hspace{-1pt},} $c$, or~\mbox{$v$\hspace{-1pt},}
where an \mbox{$a$-node} always has exactly two \glspl{child},
and~\mbox{$b$\hspace{-1pt},} $c$, and \mbox{$v$-nodes} always have no
\glspl{child}.
%
The \mbox{$v$-symbol} is a special \gls!{nullary symbol}, as such \glspl{node}
represent placeholders that can match any \gls{subtree}.
%
We say that these \glspl{symbol} collectively constitute the
\gls!{alphabet}~$\mSigma$ of our \gls{pattern set}.
%
The \gls{alphabet} needs to be finite and \gls!{ranked.a}, meaning that each
\gls{symbol} in $\mSigma$ has a ranking function that gives the number of
\glspl{child} for a given \gls{symbol}.
%
Hence, in our case \mbox{$\mRank(a) = 2$} and \mbox{$\mRank(b) = \mRank(c) =
  \mRank(v) = 0$}.

Following the terminology used in \citeauthor{HoffmannODonnell:1982}'s paper, we
also introduce the notion of a \gls!{sigma term} and define it as follows:
%
\begin{enumerate}
  \item Each \mbox{$i \in \mSigma$} with \mbox{$\mRank(i) = 0$} is a \gls{sigma
    term}.
  \item If \mbox{$i \in \mSigma$} and \mbox{$\mRank(i) > 0$}, then
    \mbox{$\mSigmaTree{i}{t_1, \ldots, t_{\mRank(i)}}$} is a \gls{sigma term}
    provided every $t_i$ is a \gls{sigma term}.
  \item Nothing else is a \gls{sigma term}.
\end{enumerate}
%
A \gls{pattern tree} is therefore a \gls{sigma term}, allowing us to write
\glspl{pattern}~\patI{} and \patII{} as
\mbox{$\mSigmaTree{a}{\mSigmaTree{a}{v\hspace{-1pt}, v}, b}$} and
\mbox{$\mSigmaTree{a}{b\hspace{-1pt}, v}$,} respectively.
%
\Glspl{sigma term} are also \gls!{ordered.st}, meaning \mbox{$\mSigmaTree{a}{b,
    v}$} for example is different from \mbox{$\mSigmaTree{a}{v\hspace{-1pt},
    b}$.}
%
Consequently, commutative operations, such as addition, must be handled through
\gls{pattern} duplication (as in the \gls{Glanville-Graham approach}).

\begin{figure}
  \centering%

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Pattern \patI%
                  \labelFigure{hoffmann-odonnell-example-pattern-1}%
                }{%
                  \input{%
                    figures/tree-covering/hoffmann-odonnell-example-pattern-1%
                  }%
                }%
  \hfill%
  \subcaptionbox{%
                  Pattern \patII%
                  \labelFigure{hoffmann-odonnell-example-pattern-2}%
                }%
                [20mm]%
                {%
                  \input{%
                    figures/tree-covering/hoffmann-odonnell-example-pattern-2%
                  }%
                }%
  \hfill%
  \subcaptionbox{%
                  Subsumption graph, excluding loop edges%
                  \labelFigure{hoffmann-odonnell-example-subsumption-graph}%
                }%
                [34mm]%
                {%
                  \clap{%
                    \input{%
                      figures/tree-covering/%
                      hoffmann-odonnell-example-subsumption-graph%
                    }%
                  }%
                }%
  \hfill%
  \mbox{}

  \vspace{\betweensubfigures}

  \subcaptionbox{%
                  Lookup table for symbol $a$%
                  \labelFigure{hoffmann-odonnell-example-lookup-table-a}%
                }%
                [42mm]%
                {%
                  \figureFont\figureFontSize%
                  \begin{tabular}{cccccccc}
                    \toprule
                        $\mathtabhead{T_a}$
                      & \tabhead 0
                      & \tabhead 1
                      & \tabhead 2
                      & \tabhead 3
                      & \tabhead 4\\
                    \midrule
                        \tabhead 0
                      & 2 & 2 & 2 & 2 & 2\\
                        \tabhead 1
                      & 3 & 3 & 3 & 3 & 3\\
                        \tabhead 2
                      & 2 & 4 & 2 & 2 & 2\\
                        \tabhead 3
                      & 2 & 4 & 2 & 2 & 2\\
                        \tabhead 4
                      & 2 & 4 & 2 & 2 & 2\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \begin{minipage}[b]{23mm}%
    \centering%
    \subcaptionbox{%
                    Lookup table for symbol $b$%
                    \labelFigure{hoffmann-odonnell-example-lookup-table-b}%
                  }%
                  [23mm]%
                  {%
                    \figureFont\figureFontSize%
                    \begin{tabular}{cc}
                      \toprule
                        $\mathtabhead{T_a}$ & 1\\
                      \bottomrule
                    \end{tabular}%
                  }

    \vspace{\betweensubfigures}

    \subcaptionbox{%
                    Lookup table for symbol $v$%
                    \labelFigure{hoffmann-odonnell-example-lookup-table-v}%
                  }%
                  [23mm]%
                  {%
                    \figureFont\figureFontSize%
                    \begin{tabular}{cc}
                      \toprule
                        $\mathtabhead{T_v}$ & 0\\
                      \bottomrule
                    \end{tabular}%
                  }%
  \end{minipage}%
  \hfill%
  \subcaptionbox{%
                  Labeled expression tree%
                  \labelFigure{hoffmann-odonnell-example-labeled-exp-tree}%
                }%
                [40mm]%
                {%
                  \input{%
                    figures/tree-covering/%
                    hoffmann-odonnell-example-labeled-exp-tree%
                  }%
                }%
  \mbox{}

  \caption[Example of tree pattern matching using Hoffmann-O'Donnell]%
          {%
            Example of tree pattern matching using
            Hoffmann-O'Donnell~\cite{HoffmannODonnell:1982}.
            %
            Nullary nodes~$v$ are indicated with a dashed border.
            %
            The subpatterns \mbox{$v$\hspace{-1pt},} \mbox{$b$\hspace{-1pt},}
            \mbox{$\mSigmaTree{a}{v\hspace{-1pt}, v}$},
            \mbox{$\mSigmaTree{a}{b\hspace{-1pt}, v}$}, and
            \mbox{$\mSigmaTree{a}{\mSigmaTree{a}{v\hspace{-1pt}, v}, b}$} are
            labeled \num{0}, \num{1}, \num{2}, \num{3}, and \num{4},
            respectively%
          }
  \labelFigure{hoffmann-odonnell-example}
\end{figure}

We continue with some definitions concerning \glspl{pattern}.
%
First, let us denote by $\mMTrees(p)$ the set of trees that can be matched by
the \gls{pattern}~$p$ at the \gls{root} of any valid tree.\!%
%
\footnote{%
  This definition is not used by \citeauthor{HoffmannODonnell:1982} in their
  paper, but having it will simplify the discussion to come.%
}
%
Depending on the \gls{alphabet}, this set could be infinite.
%
Then, a \gls{pattern}~$p$ is said to \gls!{subsume} another \gls{pattern}~$q$
(written \mbox{$p \mSubsumes q$}) if and only if any \gls{match set} including
$p$ always also includes~$q$ (hence \mbox{$\mMTrees(q) \subseteq \mMTrees(p)$}).
%
For example, given two \glspl{pattern}~\mbox{$\mSigmaTree{a}{b\hspace{-1pt},
    b}$} and \mbox{$\mSigmaTree{a}{v\hspace{-1pt}, v}$}, we have that
\mbox{$\mSigmaTree{a}{b, b} \mSubsumes \mSigmaTree{a}{v\hspace{-1pt}, v}$},
since the \mbox{$v$-nodes} must obviously also match whenever the
\mbox{$b$-nodes} match.
%
By this definition every \gls{pattern} also \gls{subsume}[s] itself.
%
Furthermore, $p$ \gls!{strictly subsume}[s]~$q$ (written \mbox{$p
  \mStrictSubsumes q$}) if and only if \mbox{$p \mSubsumes q$} and \mbox{$p \not
  = q$}, and $p$ \gls!{immediately subsume}[s]~$q$ (written \mbox{$p
  \mImmSubsumes q$}) iff \mbox{$p \mStrictSubsumes q$} and there exists no other
\gls{pattern}~$r$ such that \mbox{$p \mStrictSubsumes r$} and \mbox{$r
  \mStrictSubsumes q$\hspace{-1pt}.}

We also say that two \glspl{pattern}~$p$ and $q$ are \gls!{inconsistent.pt} if
and only if both \glspl{pattern} never appear in the same \gls{match set} (hence
\mbox{$\mMTrees(q) \cap \mMTrees(p) = \emptyset$}).
%
Lastly, $p$ and $q$ are \gls!{independent.pt} iff there exist three distinct
\glspl{tree}~\mbox{$t$\hspace{-1pt},} \mbox{$t'$\!\!,} and $t''$ (that is,
\mbox{$t \not = t' \not = t''$}), such that
%
\begin{inlinelist}[itemjoin={, }, itemjoin*={, and}]
  \item $t$ is matched by $p$ but not $q$ (hence \mbox{$\mMTrees(p) \nsubseteq
    \mMTrees(q)$})
  \item $t'$ is matched by $q$ but not $p$ (hence \mbox{$\mMTrees(q) \nsubseteq
    \mMTrees(p)$})
  \item $t''$ is matched by both $p$ and~$q$ (hence \mbox{$\mMTrees(q) \cap
    \mMTrees(p) \not = \emptyset$})
\end{inlinelist}.

\Glspl{pattern set} that contain no \gls{independent.pt} \glspl{pattern} are
known as \gls!{simple.ps}[ \glspl{pattern set}].\!%
%
\footnote{
  In \citeauthor{HoffmannODonnell:1982}'s paper these are called
  \emph{\gls{simple.ps} \gls{pattern} \glspl{forest}}.%
}
%
For example, the \gls{pattern set} consisting of \glspl{pattern}~\patI{} and
\patII{} is \gls{simple.ps} as there exists no \gls{tree} for which both match.
%
As we will see, \gls{simple.ps} \glspl{pattern set} have two important
properties that we will use for generating the lookup tables.


\paragraph{Generating Lookup Tables for Simple Pattern Sets}

In general, the size of each lookup table is exponential to the size of the
\gls{pattern set}, as is the time to generate these tables.
%
But \citeauthor{HoffmannODonnell:1982} recognized that, for \gls{simple.ps}
\glspl{pattern set}, the number of possible \glspl{match set} is equal to the
number of \glspl{pattern}, making it tractable to generate the tables for such
sets.

Furthermore, \citeauthor{HoffmannODonnell:1982} found that each possible \gls{match set}
for a \gls{simple.ps} \gls{pattern set} can be represented using a single
\gls{pattern tree}.
%
The intuition is as follows.
%
If a \gls{pattern}~$p$ \gls{strictly subsume}[s] another
\gls{pattern}~\mbox{$q$\hspace{-.8pt},} then by definition it means that $q$
will appear in every \gls{match set} where $p$ appears.
%
Consequently, $q$ does not need to be explicitly encoded into the \gls{match
  set} since it can be inferred from the presence of~\mbox{$p$\hspace{-.8pt}.}
%
Therefore, for every \gls{match set} $M$ we can select a subset of
\glspl{pattern} in $M$ to encode the entire \gls{match set}.
%
Let us call this subset the \gls!{base} of $M$, which we will denote by
$\mMatchSetBase{M}$.
%
It can be proven that different \glspl{match set} must have different
\glspl{base}, and that all \glspl{pattern} in $\mMatchSetBase{M}$ must be
pair-wise \gls{independent.pt}.
%
However, in \gls{simple.ps} \glspl{pattern set} we have no such \glspl{pattern},
and therefore the \gls{base} of every \gls{match set} must consist of a single
\gls{pattern}.
%
We will call this \gls{pattern} the \gls!{base pattern} of a \gls{match set},
and it is the labels of the \glspl{base pattern} that will appear as entries in
the lookup tables.

The key insight behind labeling is that in order to find the \gls{match set} for
some \gls{expression tree} \mbox{$T = \mSigmaTree{a}{T_1, T_2}$}, it is
sufficient to only consider the \glspl{match set} for $T_1$ and $T_2$ in the
context of $a$ instead of $T$ in its entirety.
%
If the \gls{pattern set} is \gls{simple.ps}, then we know that every \gls{match
  set} has a \gls{base pattern}.
%
Let $p_1$ and $p_2$ denote the \glspl{base pattern} of the \glspl{match set} of
$T_1$ and $T_2$, respectively.
%
With these we can transform $T$ into \mbox{$T' = \mSigmaTree{a}{p_1, p_2}$}, and
finding the \gls{match set} for $T'$ will then be equivalent to finding the
\gls{match set} for $T$\!.
%
Since every entry in a lookup table refers to a \gls{match set} (which is
represented by its \gls{base pattern}), and each \gls{symbol} in $\mSigma$ has
its own table, we can produce the tables simply by finding the \glspl{base
  pattern} of the \gls{match set} for the \gls{tree} represented by every table
entry.
%
For example, if labels~\num{1} and~\num{2} respectively refer to the
\glspl{pattern}~$b$ and \mbox{$\mSigmaTree{a}{v\hspace{-1pt}, v}$}, then the
table entry \mbox{$T_c[2, 1]$} will denote the \gls{tree}
\mbox{$\mSigmaTree{c}{\mSigmaTree{a}{v\hspace{-1pt}, v}, b}$}, and we are then
interested in finding the \gls{match set} for that \gls{tree}.

The next problem is thus to find the \gls{base pattern} of a given \gls{match
  set}.
%
For \gls{simple.ps} \glspl{pattern set} it can be proven that if we have three
distinct \glspl{pattern}~\mbox{$p$\hspace{-.8pt},} \mbox{$p'$\!,}
and~\mbox{$p''$\!,} and~$p$ \gls{subsume}[s] both~$p'$ and~$p''$\!, then it must
hold that either \mbox{$p' \mStrictSubsumes p''$} or \mbox{$p'' \mStrictSubsumes
  p'$}\!.
%
Consequently, for every \gls{match set}~$M$ we can form a \gls{subsumption
  order} among the \glspl{pattern} appearing in~$M$.
%
In other words, if a \gls{match set}~$M$ contains $m$~\glspl{pattern}, then we
can arrange these \glspl{pattern} such that \mbox{$p_1 \mStrictSubsumes p_2
  \mStrictSubsumes \ldots \mStrictSubsumes p_m$}.
%
The \gls{pattern} appearing first in this order (in this case, $p_1$) is the
\gls{base pattern} of $M$ as it \gls{strictly subsume}[s] all other
\glspl{pattern} in~$M$.
%
Hence, if we know the \gls{subsumption order}, then we can easily find the
\gls{base pattern}.

For this purpose we first enumerate all unique \glspl{subtree}, called the
\glspl!{subpattern}, that appear in the \gls{pattern set}.
%
In the case of patterns~\patI{} and \patII, this includes
\mbox{$v$\hspace{-1pt},} \mbox{$b$\hspace{-1pt},}
\mbox{$\mSigmaTree{a}{v\hspace{-1pt}, v}$,}
\mbox{$\mSigmaTree{a}{b\hspace{-1pt}, v}$,} and
\mbox{$\mSigmaTree{a}{\mSigmaTree{a}{v\hspace{-1pt}, v}, b}$,} and we denote the
set of all \glspl{subpattern} as~$S$.
%
We then assign each \gls{subpattern} in $S$ a sequential number, starting
from~\num{0}, which will represent the labels (the order in which these are
assigned is not important).


\paragraph{Building the Subsumption Graph}

\def\mGS{\overline{G}_S}
\def\mGImmS{G_S}

Next we form the \gls!{subsumption graph} for~\mbox{$S$\hspace{-1pt},} denoted
by~$\mGS$, where each \gls{node} $n_i$ represents a \gls{subpattern} \mbox{$s_i
  \in S$} and each \gls{edge}~\mbox{$\mEdge{n_i}{n_j}$} indicates that $s_i$
\gls{subsume}[s]~$s_j$.
%
For our \gls{pattern set}, we get the \gls{subsumption graph} illustrated in
\refFigure{hoffmann-odonnell-example-subsumption-graph},\!%
%
\footnote{%
  For every \gls{subsumption graph}~$\mGS$, there is also a corresponding
  \gls!{immediate.sg}[ \gls{subsumption graph}]~$\mGImmS$.
  %
  In general, $\mGImmS$ is shaped like a \glsdesc{DAG}, but for \gls{simple.ps}
  \glspl{pattern set} it is always a \gls{tree}.%
}
%
which we produce using the algorithm given in
\refAlgorithm{hoffmann-odonnell-subsumption-graph-algorithm}.
%
\begin{algorithm}[t]
  \DeclFunction{BuildSubsumptionGraph}{set $S$ of subpatterns}%
  {%
    $\mGS$ \Assign empty graph\;
    \ForEach{subpattern $s \in S$}{%
      add node $s$ to $\mGS$\;
      add edge $\mEdge{s}{s}$ to $\mGS$\;
    }
    \ForEach{%
      $\text{subpattern $s$}$ \Assign $\mSigmaTree{a}{s_1, \ldots, s_m} \in S$
      in increasing height order%
    }{%
      \ForEach{%
        subpattern $s' \in S$ \st $\text{height of $s'$} \leq \text{height of
          $s$}$%
      }{%
        \If{%
          $s' = v$ \Or $s' = \mSigmaTree{a}{s'_1, \ldots, s'_m}$ \st
          $\forall 1 \leq i \leq m : \text{edge $\mEdge{s_i}{s'_i}$} \in \mGS$%
        }{%
          add edge $\mEdge{s}{s'}$ to $\mGS$\;
        }%
      }%
    }%
  }

  \caption[%
            Algorithm for building the subsumption graph used in
            Hoffmann-O'Donnell%
          ]%
          {%
            Algorithm for building the subsumption
            graph~\cite{HoffmannODonnell:1982}%
          }
  \labelAlgorithm{hoffmann-odonnell-subsumption-graph-algorithm}
\end{algorithm}
%
The algorithm basically works as follows.
%
First, we add a \gls{node} for every \gls{subpattern}
in~\mbox{$S$\hspace{-1pt},} together with a \gls{loop edge}, as every
\gls{pattern} always \gls{subsume}[s] itself.
%
Next we iterate over all pair-wise combinations of \glspl{subpattern} and check
whether one subsumes the other, and add a corresponding edge to $\mGS$ if this
is the case.
%
To test whether a \gls{subpattern}~$q$ \gls{subsume}[s] another
\gls{pattern}~\mbox{$q$\hspace{-.8pt},} we check whether the \glspl{root} of $p$
and $q$ are of the same \gls{symbol} and whether every \gls{subtree} of $p$
\gls{subsume}[s] the corresponding \gls{subtree} of~\mbox{$q$\hspace{-.8pt}.}
%
This last check can be done by checking whether a corresponding \gls{edge}
exists in~$\mGS$ for each combination of \glspl{subtree}.
%
Hence we should iterate this process until $\mGS$ reaches a fixpoint, but we can
minimize the number of checks by first ordering the \glspl{subpattern} in $S$ by
increasing height order and then comparing the \glspl{subpattern} in that order.


\paragraph{Building the Lookup Tables}

Once we have~$\mGS$, we can generate the lookup tables following the algorithm
given in \refAlgorithm{hoffmann-odonnell-lookup-table-generation}.
%
\begin{algorithm}[t]
  \DeclFunction{GenerateTable}%
               {%
                 set $S$ of subpatterns,
                 subsumption graph $\mGS$
                 symbol $a \in \mSigma$%
               }%
  {%
    $T_a$ \Assign matrix of size $|\mSigma| \times |\mSigma|$,
                  initialized to $v \in S$\;
    \ForEach{%
      $\text{subpattern $s$}$ \Assign $\mSigmaTree{a}{s_1, \ldots, s_m} \in S$
      in increasing subsumption order%
    }{%
      \ForEach{%
        $m$-tuple $\mTuple{s'_1, \ldots, s'_m}$ \st $\forall 1 \leq i \leq m :
        s'_i \mSubsumes s_i$%
      }{%
        $T_a[s'_1, \ldots, s'_m]$ \Assign $s$\;
      }%
    }%
  }

  \caption[%
            Algorithm for generating the lookup tables used in
            Hoffmann-O'Donnell%
          ]%
          {%
            Algorithm for generating the lookup
            tables~\cite{HoffmannODonnell:1982}%
          }
  \labelAlgorithm{hoffmann-odonnell-lookup-table-generation}
\end{algorithm}
%
First, we find the \gls{subsumption order} for all \glspl{pattern} by doing a
\gls{topological sort} of the \glspl{node} in~$\mGS$ (see
\refAppendix{graph-definitions} for a definition of \gls{topological sort}).
%
Next, we initialize each entry in the table with the label of the
\gls{subpattern} consisting of a single \gls{nullary symbol}, and then
incrementally update an entry with the label of the next, larger \gls{pattern}
that matches the \gls{tree} corresponding to that entry.
%
By iterating over the \glspl{pattern} in increasing \gls{subsumption order}, the
last assignment to each entry will be that of the largest matching \gls{pattern}
in the \gls{pattern set}.
%
For our example, this results in the tables shown in
\refFigureList{hoffmann-odonnell-example-lookup-table-a,
  hoffmann-odonnell-example-lookup-table-b,
  hoffmann-odonnell-example-lookup-table-v}.

As already stated, since \glspl{pattern} are required to be ordered, we need to
duplicate \glspl{pattern} containing commutative operations by swapping the
\glspl{subtree} of the operands.
%
But doing this yields \glspl{pattern} that are pair-wise \gls{independent.pt},
destroying the property of the \gls{pattern set} being \gls{simple.ps}.
%
In such cases, the algorithm is still able to produce usable lookup tables, but
the resulting match sets will include only one of the commutative
\glspl{pattern} and not the other (which one depends on the \gls{subpattern}
last used during table generation).
%
Consequently, not all \glspl{match} will be found during \gls{pattern matching},
which may in turn prevent \gls{optimal.ps} \gls{pattern selection}.


\paragraph{Compressing the Lookup Tables}
\labelSection{tc-table-compression}

\textcite{Chase:1987} further advanced \citeauthor{HoffmannODonnell:1982}'s
table generation technique by developing an algorithm that compresses the final
lookup tables.
%
The key insight is that the lookup tables often contain redundant information as
many rows and columns are duplicates.
%
For example, this can be seen clearly in $T_a$ from our previous example, which
is also shown in \refTable{table-compression-example-uncompressed}.
%
\begin{table}
  \centering%

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Uncompressed lookup table%
                  \labelTable{table-compression-example-uncompressed}%
                }%
                [46mm]%
                {%
                  \figureFont\figureFontSize%
                  \begin{tabular}{cccccccc}
                    \toprule
                        $\mathtabhead{T_a}$
                      & \tabhead 0
                      & \tabhead 1
                      & \tabhead 2
                      & \tabhead 3
                      & \tabhead 4\\
                    \midrule
                        \tabhead 0
                      & 2 & 2 & 2 & 2 & 2\\
                        \tabhead 1
                      & 3 & 3 & 3 & 3 & 3\\
                        \tabhead 2
                      & 2 & 4 & 2 & 2 & 2\\
                        \tabhead 3
                      & 2 & 4 & 2 & 2 & 2\\
                        \tabhead 4
                      & 2 & 4 & 2 & 2 & 2\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \subcaptionbox{%
                  Compressed lookup table%
                  \labelTable{table-compression-example-compressed}%
                }{%
                  \begin{tikzpicture}[%
                      every node/.style={
                        nothing,
                        node distance=6mm,
                        font=\figureFont\figureFontSize,
                      }
                    ]

                    \node (ta) {%
                      \begin{tabular}{ccc}
                        \toprule
                            $\mathtabhead{\tau_a}$
                          & \tabhead 0
                          & \tabhead 1\\
                        \midrule
                            \tabhead 0
                          & 2 & 2\\
                            \tabhead 1
                          & 3 & 3\\
                            \tabhead 2
                          & 2 & 4\\
                        \bottomrule
                      \end{tabular}%
                    };

                    \node [left=of ta] (u1) {%
                      \begin{tabular}{cc}
                        \toprule
                          \multicolumn{2}{c}{$\mathtabhead{\mu_{a,0}$}}\\
                        \midrule
                          \tabhead 0 & 0\\
                          \tabhead 1 & 1\\
                          \tabhead 2 & 2\\
                          \tabhead 3 & 2\\
                          \tabhead 4 & 2\\
                        \bottomrule
                      \end{tabular}%
                    };

                    \node [above=of ta] (u2) {%
                      \begin{tabular}{ccccccc}
                        \toprule
                            $\mathtabhead{\mu_{a,1}}$
                          & \tabhead 0
                          & \tabhead 1
                          & \tabhead 2
                          & \tabhead 3
                          & \tabhead 4\\
                        \midrule
                          & 0 & 1 & 0 & 0 & 0\\
                        \bottomrule
                      \end{tabular}%
                    };

                    \begin{scope}[data flow]
                      \draw (u1) -- (ta);
                      \draw (u2) -- (ta);
                    \end{scope}
                  \end{tikzpicture}%
                }%
  \hfill%
  \mbox{}

  \caption[Example of lookup table compression]%
          {Example of compressing the lookup table $T_a$~\cite{Chase:1987}}
  \labelTable{table-compression-example}
\end{table}
%
By introducing a set of \glspl!{index map}, the duplicates can be removed by
mapping identical columns or rows in the \gls{index map} to the same row or
column in the lookup table.
%
The lookup table can then be reduced to contain only the minimal amount of
information, as seen in \refTable{table-compression-example-compressed}.
%
By denoting the compressed version of $T_a$ by~$\tau_a$, and the corresponding
\glspl{index map} by $\mu_{a,0}$ and $\mu_{a,1}$, we replace a previous lookup
\mbox{$T_i[l_0, \ldots, l_m]$} for symbol~$i$ with \mbox{$\tau_i[\mu_{i,0}[l_0],
    \ldots, \mu_{i,m}[l_m]]$}.

Table compression also provides another benefit in that, for some \glspl{pattern
  set}, the lookup tables can be so large that they cannot even be constructed
in the first place.
%
But \citeauthor{Chase:1987} discovered that the tables can be compressed as they
are generated, thus pushing the limit on how large lookup tables can be
produced.
%
\textcite{CaiEtAl:1992} later improved the asymptotic bounds of
\citeauthor{Chase:1987}'s algorithm.


\subsection{Optimal Pattern Selection with Dynamic Programming}
\labelSection{optimal-pattern-selection-dp}

Once it became possible to find all \glspl{match set} for the entire
\gls{expression tree} in linear time, techniques started to appear that also
tackled the problem of \gls{optimal.ps} \gls{pattern selection} in linear time.
%
According to the literature, \textcite{Ripken:1977} was the first to propose a
viable method for optimal linear-time \gls{instruction selection}, which is
described in a 1977 technical report.
%
\citeauthor{Ripken:1977} based his method on the \glsdesc{DP} algorithm by
\citeauthor{AhoJohnson:1976} and later extended it to handle more realistic
\glspl{instruction set} with multiple \glspl{register class} and
\glspl{addressing mode}.
%
For brevity, we will henceforth abbreviate \glsdesc!{DP}
as~\glsunset{DP}\gls{DP}.

Although \citeauthor{Ripken:1977} appears to have been the first to propose a
design of an optimal \gls{DP}-based \gls{instruction selector}, it only remained
that -- a proposal.
%
The first \emph{practical} attempt was instead made in 1986 by
\citeauthor{AhoEtAl:1989}~\cite{AhoGanapthi:1985, Tjiang:1986, AhoEtAl:1989}
with the introduction of a \gls{compiler} generator called \gls!{Twig}.


\paragraph{\glsentrytext{Twig}}

As in \citeauthor{Ripken:1977}'s design, \gls{Twig} uses a version of
\citeauthor{AhoJohnson:1976}'s \gls{DP}~algorithm for selecting the
\gls{optimal.ps} set of \glspl{pattern tree} to cover a given \gls{expression
  tree}.
%
The \gls{machine description} is expressed as a \glsshort{normal form.g}
\gls{machine grammar} (see \refSection{tc-glanville-graham}) using a language
called \gls!{CGL}, which as introduced by \textcite{AhoGanapthi:1985} in~1985.
%
An excerpt of such a \gls{machine description} is shown in
\refFigure{twig-example}.
%
\begin{filecontents*}{twig-example.c}
node const mem assign plus ind;
label reg no_value;
reg:const                                       /* Rule 1 */
  {  cost = 2; }
  ={ NODEPTR regnode = getreg( );
     emit(''MOV'', $1$, regnode, 0);
     return(regnode);
  };
no_value: assign(mem, reg)                      /* Rule 3 */
  {  cost = 2+$%1$->cost; }
  ={ emit(''MOV'', $2$, $1$, 0);
     return(NULL);
  };
reg: plus(reg, ind(plus(const, reg)))           /* Rule 6 */
  {  cost = 2+$%1$->cost+$%2$->cost; }
  ={ emit(''ADD'', $2$, $1$, 0);
     return($1$);
  };
\end{filecontents*}
%
\begin{figure}
  \centering%

  \begin{lstpage}{105mm}
    \lstinputlisting{twig-example.c}%
  \end{lstpage}

  \caption[Examples of grammar rules for \glsentrytext{Twig}]%
          {%
            Examples of grammar rules for \glsentrytext{Twig}, written in
            CGL~\cite{AhoEtAl:1989}%
          }%
  \labelFigure{twig-example}
\end{figure}
%
\Gls{Twig} takes this \gls{machine description} and generates an
\gls{instruction selector} that makes three passes over the \gls{expression
  tree}.
%
The first pass is a top-down labeling pass that finds all \glspl{match set} for
every \gls{node} in the \gls{expression tree}%
%
\footnote{%
  Remember that, when using \glspl{machine grammar}, a \gls{pattern} found in
  the \gls{match set} during \gls{pattern matching} corresponds to the
  right-hand side of a \gls{production}.%
}
%
using an implementation of the Aho-Corasick string matching
algorithm~\cite{AhoCorasick:1975}.
%
The second pass is a bottom-up cost computation pass that gives the cost of
selecting a particular \gls{pattern} for a given \gls{node}.
%
As we will see, the costs are computed using \gls{DP}~and hence the computation
constitutes the core of this design.
%
The last pass is a recursive top-down pass that finds the least-cost cover of
the \gls{expression tree}.
%
This pass also executes the \glspl{action} associated with the selected
\glspl{pattern}, which in turn emits the corresponding \gls{assembly code}.

The design is centered around the following assumption.
%
Given a \gls{node}~$n$ in an \gls{expression tree} and a \gls{rule}~$r$\!, the
cost of applying $r$ on $n$ is the cost of $r$ plus the costs of reducing all
\glspl{child} of $n$ to the appropriate \glspl{nonterminal} appearing on the
right-hand side of~$r$\!.
%
If $r$ is a \gls{chain.r} \gls{rule} then the cost is computed as the cost of
$r$ plus the cost of reducing $n$ to the \glsshort{rule result} of~$r$\!.
%
The recursive nature of these costs can be exploited using \glsdesc{DP},
resulting in the algorithm shown in \refAlgorithm{aho-etal-cost-algorithm-2}
which computes the least cost of reducing a given \gls{expression tree} to a
particular \gls{nonterminal}.

\begin{algorithm}[p]
  \DeclFunction{ComputeCosts}%
               {expression tree $T$\!, normal-form grammar $G$}%
  {%
    $S$ \Assign $\mSetBuilder{s}%
                             {\text{$s$ is a nonterminal in $G$}}$\;
    $\mMatrix{C}$ \Assign
      matrix of size $|T| \times |S|$, costs initialized to $\infty$\;
    \Call{ComputeCostsRec}{root node of $T$}\;
    \Return{$\mMatrix{C}$}\;
    \BlankLine
    \DeclFunction{ComputeCostsRec}{node $n$}{%
      \ForEach{child $m$ of $n$}{%
        \Call{ComputeCostsRec}{$m$}\;
      }
      \ForEach{base rule $r \in \text{\Call{FindMatchingRules}{$n$}}$}{%
        $c$ \Assign \Call{ComputeReductionCost}{$n$, $r$}\;
        $l$ \Assign result of $r$\;
        \If{$c < \mMatrix{C}[n][l]$.cost}{%
          $\mMatrix{C}[n][l]$.cost \Assign $c$\;
          $\mMatrix{C}[n][l]$.rule \Assign $r$\;
        }
      }
      \Repeat{no change to $\mMatrix{C}$}{%
        \ForEach{chain rule $r \in G$}{%
          $c$ \Assign \Call{ComputeReductionCost}{$n$, $r$}\;
          $l$ \Assign result of $r$\;
          \If{$c < \mMatrix{C}[n][l]$.cost}{%
            $\mMatrix{C}[n][l]$.cost \Assign $c$\;
            $\mMatrix{C}[n][l]$.rule \Assign $r$\;
          }
        }
      }
    }

    \BlankLine
    \DeclFunction{FindMatchingRules}{node $n$}{%
      $M$ \Assign $\emptyset$\;
      \ForEach{base rule $r \in G$}{%
        \If{$\text{terminal in pattern of $r$} = \text{node type of $n$}$}{%
          $M$ \Assign $M \cup \mSet{r}$\;
        }
      }
      \Return{$M$}\;
    }

    \BlankLine
    \DeclFunction{ComputeReductionCost}{node $n$, rule $r$}{%
      $c$ \Assign cost of $r$\;
      \eIf{$r$ is a chain rule}{%
        $s$ \Assign nonterminal in pattern of $r$\;
        $c$ \Assign $c$ $+$ $\mMatrix{C}[n][s]$.cost
        \cmt*{here cost of node itself is taken instead of its children}
      }{%
        \For{$i$ \Assign $1$ \KwTo number of children for $n$}{%
          $m$ \Assign $i$th child of $n$\;
          $s$ \Assign $i$th nonterminal in pattern of $r$\;
          $c$ \Assign $c$ $+$ $\mMatrix{C}[m][s]$.cost\;
        }
      }
      \Return{$c$}\;
    }
  }

  \caption[%
            Algorithm for computing the optimal sequence of rules that reduces
            the given expression tree to a particular nonterminal%
          ]{%
            Computes the optimal sequence of rules that reduces the given
            expression tree to a particular nonterminal%
          }
  \labelAlgorithm{aho-etal-cost-algorithm-2}
\end{algorithm}

The algorithm works as follows.
%
It first constructs a cost matrix~$\mMatrix{C}$, where rows represent
\glspl{node} in the \gls{expression tree} and columns represent
\glspl{nonterminal} in the \gls{grammar}, which is assumed to be in \gls{normal
  form.g}.\!%
%
\footnote{%
  The algorithm can be adapted to accept any \gls{grammar} by expanding the
  \algStyle{FindMatchingRules} and \algStyle{ComputeReductionCost} functions to
  handle \glspl{rule} of arbitrary form.%
}
%
The cost in each element~\mbox{$\mMatrix{C}[i][j]$} is initialized to infinity,
indicating that there exists no sequence of \glspl{rule reduction} which reduces
\gls{node}~$i$ to \gls{nonterminal}~\mbox{$j$\hspace{-1pt}.}
%
It then computes the costs by traversing the \gls{expression tree} bottom up.
%
At each \gls{node}~$n$ and for each matching \gls{base.r} \gls{rule}~$r$\!, with
\gls{nonterminal}~$s$ as \glsshort{rule result}, it computes the cost~$c$ of
applying $r$ at~$n$ to produce~$s$ according to the scheme stated above.
%
If $c$ is less than the currently recorded cost for reducing $n$
to~\mbox{$s$\hspace{-1pt},} then the cost and \gls{rule} information for $n$ is
updated accordingly.
%
The same is then done for all \gls{chain.r} \glspl{rule} until it reaches a
fixpoint (which must eventually be reached as all \gls{rule} costs are
non-negative and an update only occurs when the cost is strictly less).
%
Since every \gls{node} is also only processed once, the algorithm runs in linear
time with respect to the size of the \gls{expression tree}.

Having computed the costs, the optimal order of \glspl{rule reduction} -- which
is equivalent to the \gls{least-cost.c} \gls{cover} -- can be found using the
algorithm shown in \refAlgorithm{aho-etal-select-algorithm}.
%
\begin{algorithm}[t]
  \DeclFunction{SelectRules}{expression tree $T$\!,
                             goal nonterminal $g$,
                             cost matrix $\mMatrix{C}$}%
  {%
    $n$ \Assign root node of $T$\;
    $r$ \Assign $\mMatrix{C}[n][g]$.rule\;
    \eIf{$r$ is a chain rule}{%
      $s$ \Assign result of $r$\;
      \Call{SelectRules}{$T$\!, $s$, $\mMatrix{C}$}\;
    }{%
      \For{$i$ \Assign $1$ \KwTo number of children for $n$}{%
        $m$ \Assign $i$th child of $n$\;
        $s$ \Assign $i$th nonterminal in pattern of $r$\;
        \Call{SelectRules}{expression tree rooted at $m$\hspace{-.8pt},
                           $s$\hspace{-.8pt},
                           $\mMatrix{C}$}\;
      }
    }
    execute actions associated with $r$\;
  }

  \caption[Algorithm for selecting the optimal sequence of rules]%
          {%
            Selects optimal sequence of rules that reduces a given expression
            tree to a given nonterminal, based on costs computed
            by \refAlgorithm{aho-etal-cost-algorithm-2}%
          }
  \labelAlgorithm{aho-etal-select-algorithm-2}
\end{algorithm}
%
Starting from the \gls{root}, we select the \gls{rule} that reduces this
\gls{node} of the \gls{expression tree} to a particular \gls{nonterminal}.
%
The same is then done recursively for each \gls{nonterminal} that appears on the
pattern in the selected \gls{rule}, acting as the goal for the corresponding
\gls{subtree}.
%
Since it is assumed that the \gls{machine grammar} is in \gls{normal form.g},
every \gls{pattern} is exactly one \gls{node} which makes it trivial to find the
next \gls{subtree} in the \gls{expression tree}.
%
The algorithm also correctly applies the necessary \gls{chain.r} \glspl{rule},
as the use of such a \gls{rule} causes the routine to be reinvoked on the same
\gls{node} but with a different goal \gls{nonterminal}.


\paragraph{DP \versus LR Parsing}

The \gls{DP}~scheme has several advantages over those based on \gls{LR parsing}.
%
First, \glsshort{rule reduction} conflicts are automatically handled by the
cost-computing algorithm, removing the need of ordering the \glspl{rule} which
could affect the code quality yielded by \glspl{LR parser}.
%
Second, \gls{rule} cycles that cause \glspl{LR parser} to get stuck in an
infinite loop no longer need to be explicitly broken.
%
Third, machine descriptions can be made more concise as \glspl{rule} differing
only in cost can be combined into a single \gls{rule}.
%
Again taking the \gls{VAX}~machine as an example, \citeauthor{AhoEtAl:1989}
reported that the entire \gls{Twig} specification could be implemented using
only \num{115}~\glspl{rule}, which is about half the size of
\citeauthor{GanapathiEtAl:1982:AttrGr}'s \gls{attribute}-based \gls{machine
  grammar} for the same \gls{target machine}.

However, the \gls{DP}~approach requires that the \gls{code generation} problem
exhibit properties of optimal substructure, meaning that it is possible to
generate optimal \gls{assembly code} by solving each of its subproblems to
optimality.
%
However, this is not always the case.
%
Some solutions, whose total sum is greater compared to another set of selected
\glspl{pattern}, can actually lead to better \gls{assembly code} in the end.


\paragraph{Further Improvements}
\labelSection{tc-iburg}
\labelSection{tc-olive}

Several improvements of \gls{Twig} were later made by
\textcite{YatesSchwartz:1988} and \textcite{EmmelmannEtAl:1989}.
%
\citeauthor{YatesSchwartz:1988} improved the rate of \gls{pattern matching} by
replacing \gls{Twig}'s top-down approach with the faster bottom-up algorithm
proposed by \citeauthor{HoffmannODonnell:1982}, and also extended the
\gls{attribute} support for more powerful \glspl{predicate}.
%
\citeauthor{EmmelmannEtAl:1989} modified the \gls{DP}~algorithm to be run as the
\glspl{expression tree} are built by the frontend, which also inlines the code
of auxiliary functions directly into the \gls{DP}~algorithm to reduce the
overhead.
%
\citeauthor{EmmelmannEtAl:1989} implemented their improvements in a system
called the \gls!{BEG}, and a modified version of this is currently used in the
\gls!{CoSy} \gls{compiler}~\cite{ACE:2003}.

\textcite{FraserEtAl:1992:IBurg} made similar improvements in a system called
\gls!{IBurg} that is both simpler and faster than \gls{Twig}; \gls{IBurg}
requires only \num{950}~lines of code compared to \gls{Twig}'s \mbox{3,000
  lines} of \gls{C}~code, and generates \gls{assembly code} of comparable
quality at a rate that is \num{25}~times faster.
%
\gls{IBurg} has also been used in several \glspl{compiler}, such as
\gls!{Record}~\cite{LeupersMarwedel:1997, Marwedel:1997} and
\gls!{Redaco}~\cite{KreuzerEtAl:1996}.
%
\citeauthor{GoughLedermann:1997}~\cite{Gough:1995, GoughLedermann:1997} later
extended the \gls{predicate} support of \gls{IBurg} in an implementation called
\gls!{MBurg}.
%
Both \gls{IBurg} and \gls{MBurg} have later been reimplemented in various
programming languages, such as the \gls{Java}-based \gls!{JBurg}~\cite{JBURG},
\gls!{OCamlBurg}~\cite{OCamlBURG}, which is written in~\gls{C--}, and
\gls!{GPBurg}~\cite{Gough:2012}, which is written in \gls{CSharp}.

According to \textcite{LeupersMarwedel:2001} and \textcite{CaoEtAl:2011},
\textcite{Tjiang:1993} later merged the ideas of \gls{Twig} and \gls{IBurg} into
a new implementation called \gls!{Olive} (the name is a spin-off of \gls{Twig}).
%
\citeauthor{Tjiang:1993} also made several additional improvements such as
supporting \glspl{rule} to use arbitrary cost functions instead of fixed,
numeric values.
%
This supports more versatile \gls{instruction selection}, as \glspl{rule} can be
dynamically deactivated by setting infinite costs, which can be controlled from
the current context.
%
\gls{Olive} is also used in \gls!{Spam}~\cite{SudarsanamEtAl:1999} -- a
fixed-point \gls{DSP} \gls{compiler} -- and \textcite{AraujoMalik:1995} employed
it in an attempt to integrate \gls{instruction selection} with scheduling and
\gls{register allocation}.


\paragraph{Code Size-Reducing Instruction Selection}

In 2010, \textcite{EdlerVonKochEtAl:2010} modified the backend in \gls{CoSy} to
perform \gls{code generation} in two stages in order to reduce code size for
architectures with mixed \mbox{16-bit} and \mbox{32-bit} \glspl{instruction},
where the former is smaller but can only access a reduced set of
\glspl{register}.
%
In the first stage, \gls{instruction selection} is performed by aggressively
selecting \mbox{16-bit} \glspl{instruction}.
%
Then, during \gls{register allocation}, whenever a memory spill is required due
to the use of a \mbox{16-bit} \gls{instruction}, the \gls{node} ``causing'' this
spill is annotated with a special flag.
%
Once \gls{register allocation} is finished, another round of \gls{instruction
  selection} is performed but this time no \glspl{node} which have been
annotated are allowed to be covered by \glspl{pattern} originating from
\mbox{16-bit} \glspl{instruction}.
%
Experiments showed that this scheme reduced code size by about \SI{17}{\percent}
on average compared to \gls{CoSy} for the selected target architecture and
benchmark suite.


\paragraph{Combining DP with Macro Expansion}

After arguing that \citeauthor{GlanvilleGraham:1978}'s method attacked the
\gls{instruction selection} problem from the wrong direction -- that is, by
defining the \glspl{instruction} in terms of \gls{IR}~operations --
\textcite{Horspool:1987} developed in 1987 a technique that essentially is an
enhanced form of \gls{macro expansion}.
%
Because \gls{macro}-expanding \glspl{instruction selector} only visit and
execute macros one \gls{IR} \gls{node} at a time (see
\refAppendix{macro-expansion} on \refPageOfSection{me-limitations}), they do not
inherently support \glspl{instruction} where there is an \mbox{$n$-to-1} mapping
between the\gls{IR} \glspl{node} and the \glspl{instruction}.
%
This limitation can be worked around by incorporating additional logic and
bookkeeping into the macro definitions, but doing so by hand often proves to be
infeasible.
%
By including an \gls{edge} labeling step prior to \gls{macro expansion},
\citeauthor{Horspool:1987} found a way of supporting such \glspl{instruction}
while at the same time simplifying the \gls{macro} definitions.

The idea is to first break down every \gls{pattern} into single-\gls{node}
components (see \refFigure{horspool-node-breakdown-example}).
%
\begin{figure}
  \centering%

  \mbox{}%
  \hfill%
  \subcaptionbox{%
                  Original pattern%
                  \labelFigure{horspool-node-breakdown-example-original}%
                }{%
                  \input{%
                    figures/tree-covering/%
                    horspool-node-breakdown-example-original%
                  }%
                }%
  \hfill%
  \subcaptionbox{%
                  Components%
                  \labelFigure{horspool-node-breakdown-example-components}%
                }{%
                  \begin{minipage}{50mm}%
                    \adjustbox{valign=M}{%
                      \input{%
                        figures/tree-covering/%
                        horspool-node-breakdown-example-add1%
                      }%
                    }%
                    \hfill%
                    \adjustbox{valign=M}{%
                      \input{%
                        figures/tree-covering/%
                        horspool-node-breakdown-example-ld%
                      }%
                    }%

                    \vspace{\betweensubfigures}

                    \mbox{}%
                    \hfill%
                    \adjustbox{valign=M}{%
                      \input{%
                        figures/tree-covering/%
                        horspool-node-breakdown-example-int%
                      }%
                    }%
                    \hfill\hfill%
                    \adjustbox{valign=M}{%
                      \input{%
                        figures/tree-covering/%
                        horspool-node-breakdown-example-add2%
                      }%
                    }%
                    \hfill%
                    \mbox{}%
                  \end{minipage}%
                }%
  \hfill%
  \mbox{}

  \caption[Example of breaking down a pattern into single-node components]%
          {%
            Example of breaking down a pattern into single-node components in
            order for it to be supported by a macro-expanding instruction
            selector~\cite{Horspool:1987}%
          }
  \labelFigure{horspool-node-breakdown-example}
\end{figure}
%
As part of the breakdown process the intermediate \glspl{edge} are labeled with
\glspl!{storage class} which serve as a form of glue between the components,
allowing them to be reconnected during \gls{macro expansion}.
%
The same \glspl{storage class} can be used across multiple \glspl{pattern} if
this is deemed appropriate, which is akin to \gls{refactoring} an \gls{machine
  grammar} in order to reduce the number of \glspl{rule}.

The goal is then to label the \glspl{edge} of the \gls{expression tree} with
\glspl{storage class} such that they correspond to a least-cost cover of the
\glsshort{expression tree}, which can be done using dynamic programming (but the
paper does not go into detail about how the component costs should be assigned).
%
Once the \gls{expression tree} has been labeled, the \gls{assembly code} can be
emitted using a straightforward \gls{macro expander} that uses the current
\gls{node}'s type and the \glspl{storage class} of its \glspl{edge} as indices
to a macro table.
%
Since the bookkeeping is essentially lifted into the \glspl{storage class}, the
macro definitions become much simpler compared to those of traditional
\gls{macro}-expanding techniques.
%
Moreover, there is no need to handle backtracking, as such a combination of
\gls{edge} labels would imply an illegal cover of the \gls{expression tree}.

In principle, \citeauthor{Horspool:1987}'s design is comparable to that of
\citeauthor{AhoEtAl:1989}, and should yield similar code quality.
%
However, \citeauthor{Horspool:1987} appears to have had to implement his
\gls{instruction selection} tables by hand whereas \citeauthor{AhoEtAl:1989}
built a tool to do it for them.


\subsection{Faster Pattern Selection with Offline Cost Analysis}
\labelSection{precomputing-cost-computations}

In the \gls{DP}~approach just discussed, the \gls{rule} costs needed for
selecting the \glspl{pattern} are dynamically computed while the \gls{pattern
  matcher} is completely table-driven.
%
It was later discovered that these calculations can also be done beforehand and
represented as tables, improving the speed of the \gls{pattern selector} as it
did for \gls{pattern matching}.
%
We will refer to this aspect as \gls!{offline cost analysis}, which means that
the costs of covering any given \gls{expression tree} are precomputed as part of
generating the \gls{compiler} instead at compile time.


\paragraph{Extending Match Set Labels with Costs}

To make use of \gls{offline cost analysis}, we need to extend the labels to not
only represent \glspl{match set}, but also incorporate the information about
which \gls{pattern} will lead to the lowest covering cost given a specific goal.
%
To distinguish between the two, we refer to this extended form of label as a
\gls{state}.
%
A \gls{state} is essentially a representation of a specific combination of
goals, \glspl{pattern}, and costs, where each possible goal~$g$ is associated
with a \gls{pattern}~$p$ and a relative cost~$c$.
%
A goal in this context typically dictates where the result of an expression must
appear, like a particular \gls{register class} or memory, and in \gls{grammar}
terms this means that each \gls{nonterminal} is associated with a \gls{rule} and
a cost.
%
This combination is such that
%
\begin{enumerate}
  \item for any \gls{expression tree} whose \gls{root} has been labeled with a
    particular \gls{state},
  \item if the goal of the \gls{root} must be~\mbox{$g$\hspace{-1pt},}
  \item then the entire \gls{expression tree} can be covered with minimal cost
    by selecting \gls{pattern}~$p$ at the \gls{root}.
    %
    The relative cost of this covering, compared to the scenario in which the
    goal is something else, is equal to~$c$.
\end{enumerate}

A key point to understand here is that a \gls{state} does not necessarily need
to carry information about how to optimally cover the \emph{entire}
\gls{expression tree}.
%
Indeed, such an attempt would require an infinite number of \glspl{state}.
%
Instead, the \glspl{state} only convey enough information about how to cover the
distinct key shapes that can appear in any \gls{expression tree}.
%
To explain this further, let us observe how most \glspl{target machine}
typically operate.
%
Between the execution of two \glspl{instruction}, the data is synchronized by
storing it in \glspl{register} or in memory.
%
The manner in which some data came to appear in a particular location has in
general no impact on the execution of the subsequent \glspl{instruction}.
%
Consequently, depending on the available \glspl{instruction}, one can often
break a \gls{expression tree} at certain key places without compromising code
quality.
%
This yields a set of many, smaller \glspl{expression tree}, each with a specific
goal at the \gls{root}, which then can be optimally covered in isolation.
%
In other words, the set of \glspl{state} only needs to collectively represent
enough information to communicate where these cuts can be made for all possible
\glspl{expression tree}.
%
This does not mean that the \gls{expression tree} is \emph{actually} partitioned
into smaller pieces before \gls{pattern selection}, but thinking about it in
this way helps us understand why we can restrict ourselves to a finite number of
\glspl{state} and still get \gls{optimal.ps} \gls{pattern selection}.

The algorithm for labeling an \gls{expression tree} using \glspl{state} is given
in \refAlgorithm{burs-labeling-algorithm}.
%
Since a \gls{state} is simply an extended form of a label, this algorithm is
very similar to the one used in Hoffmann-O'Donnell (compare with
\refAlgorithm{hoffmann-odonnell-labeling} on
\refPageOfAlgorithm{hoffmann-odonnell-labeling}).
%
\begin{algorithm}[t]
  \DeclFunction{LabelTree}%
               {expression tree $T$\!, list $L$ of state tables}%
  {%
    $n$ \Assign root node of $T$\;
    $k$ \Assign number of children for $n$\;
    \For{$i$ \Assign $1$ \KwTo $k$}{%
      $m_i$ \Assign $i$th child of $n$\;
      \Call{LabelTree}{expression tree rooted at $m_i$, $L$}\;
    }
    $S$ \Assign $L[\text{terminal corresponding to $n$}]$\;
    $n$.label \Assign $S[m_1.\text{label}, \ldots, m_k.\text{label}]$\;
  }

  \caption[%
            Algorithm for labeling an expression tree using states%
          ]{%
            Labels an expression tree using states%
          }
  \labelAlgorithm{burs-labeling-algorithm}
\end{algorithm}
%
\Gls{pattern selection} and \gls{assembly code} emission is then done as
described in \refAlgorithm{burs-rule-selection-algorithm}.
%
This is more or less identically to the selection algorithm when computing the
costs directly (compare with \refAlgorithm{aho-etal-select-algorithm} on
\refPageOfAlgorithm{aho-etal-select-algorithm}).
%
\begin{algorithm}[t]
  \DeclFunction{Select}{labeled expression tree $T$\!,
                        goal nonterminal $g$,
                        list $L$ of rule lookup tables}%
  {%
    $n$ \Assign root node of $T$\;
    $R$ \Assign $L[n\text{.label}]$\;
    $r$ \Assign $R[g]$\;
    \eIf{$r$ is a chain rule}{%
      $s$ \Assign result of $r$\;
      \Call{Select}{$T$\!, $s$\hspace{-.8pt}, $L$}\;
    }{%
      \For{$i$ \Assign $1$ \KwTo number of children for $n$}{%
        $m$ \Assign $i$th child of $n$\;
        $s$ \Assign $i$th nonterminal in pattern of $r$\;
        \Call{Select}{expression tree rooted at $m$, $s$\hspace{-.8pt}, $L$}\;
      }
    }
    execute actions associated with $r$\;
  }

  \caption[%
            Algorithm for selecting the rules for a labeled expression tree%
          ]%
          {%
            Selects optimal sequence of rules that reduces a given labeled
            expression tree to a given nonterminal%
          }
  \labelAlgorithm{burs-rule-selection-algorithm}
\end{algorithm}
%
However, we have yet to describe how to compute the \glspl{state}.


\paragraph{First Technique to Apply Offline Cost Analysis}

Due to a 1986~paper, \textcite{HatcherChristopher:1986} appear to have been
pioneers in applying \gls{offline cost analysis} to \gls{pattern selection}.
%
\citeauthor{HatcherChristopher:1986}'s design, which is an extension of the work
by \citeauthor{HoffmannODonnell:1982}, can intuitively be described as follows.
%
Given a \gls{expression tree} whose \gls{root} has been assigned a
label~\mbox{$l$\hspace{-.8pt},} find the \gls{rule} to apply such that the
entire \gls{tree} can be reduced to a given \gls{nonterminal} at lowest cost.
%
\citeauthor{HatcherChristopher:1986} argued that for \gls{optimal.ps}
\gls{pattern selection} we can consider each pair of a label~$l$ and
\gls{nonterminal}~$\mNT{N}$, and then always apply the \gls{rule} that will
reduce the largest \gls{expression tree}~$T_l$, which is representative
of~\mbox{$l$\hspace{-.8pt},} to $\mNT{N}$ at the lowest cost.
%
In \citeauthor{HoffmannODonnell:1982}'s design, where there is only one
\gls{nullary symbol} that may match any \gls{subtree}, $T_l$ is equal to the
largest \gls{pattern} appearing in the \gls{match set}.
%
However, to accommodate \glspl{machine grammar}
\citeauthor{HatcherChristopher:1986}'s version includes one \gls{nullary symbol}
per \gls{nonterminal}.
%
This means that $T_l$ has to be found by overlapping all \glspl{pattern}
appearing in the \gls{match set}.
%
We then calculate the cost of transforming a larger \gls{pattern}~$p$ into a
subsuming, smaller \gls{pattern}~$q$ (hence \mbox{$p > q$}) for every pair of
\glspl{pattern}.
%
This cost, which is later annotated to the \gls{subsumption graph}, is
calculated by recursively rewriting $p$ using other \glspl{pattern} until it is
equal to~\mbox{$q$\hspace{-.8pt}.}
%
Hence the cost of this transformation is equal to the sum of all applied
\glspl{pattern}.
%
We represent this cost with a function $\mReduceCost{p}{q}$.
%
With this information, we retrieve the \gls{rule} that leads to the lowest-cost
\glsshort{rule reduction} of $T_l$ to a goal~$g$ by finding the \gls{rule}~$r$
for which
%
\begin{displaymath}
  \mReduceCost{T_l}{g} =
  \mReduceCost{T_l}{\text{pattern tree of $r$}} + \text{cost of $r$}\!.
\end{displaymath}
%
This will select either the largest \gls{pattern} appearing in the \gls{match
  set} of~\mbox{$l$\hspace{-.8pt},} or, if one exists, a smaller \gls{pattern}
that in combination with others has a lower cost.
%
We have of course glossed over many details, but this covers the main idea of
\citeauthor{HatcherChristopher:1986}'s design.

By encoding the selected \glspl{rule} into an additional table to be used during
\gls{pattern matching}, we achieve a completely table-driven \gls{instruction
  selector} which also performs \gls{optimal.ps} \gls{pattern selection}.
%
\citeauthor{HatcherChristopher:1986} also augmented the original algorithm so
that the returned \glspl{match set} contain all \glspl{pattern} that were
duplicated due commutative operations.
%
However, if the \gls{pattern set} contains \glspl{pattern} which are truly
\gls{independent.pt}, then \citeauthor{HatcherChristopher:1986}'s design does
not always guarantee that the \glspl{expression tree} can be optimally covered.
%
It is also not clear whether \gls{optimal.ps} \gls{pattern selection} for the
largest \glspl{expression tree} representative of the labels is an accurate
approximation for \gls{optimal.ps} \gls{pattern selection} for all possible
\glspl{expression tree}.


\paragraph{Generating the States Using BURS Theory}

A different and more well-known method for generating the \glspl{state} was
developed by \textcite{Pelegri-LlopartGraham:1988}.
%
In a seminal paper from~:1988, \citeauthor{Pelegri-LlopartGraham:1988} prove
that the methods of \gls{tree rewriting} can always arranged such that all
rewrites occur at the \glspl{leaf} of the \gls{tree}, resulting in a
\gls!{BURS}.
%
We say that a collection of such \glspl{rule} constitute a \gls!{BURS grammar},
which is similar to the \glspl{grammar} already seen, with the exception that
\glspl{BURS grammar} allow multiple \glspl{symbol} -- including \glspl{terminal}
-- to appear on the left-hand side of a \gls{production}.
%
An example of such a \gls{grammar} is given in \refFigure{burs-example-grammar}.
%
As an extension to the work of \textcite{ZimmermannGaul:1997},
\textcite{DoldEtAl:1998} later developed a method for proving the correctness of
\glspl{BURS grammar} using abstract \glspl{state machine}.

\begin{figure}
  \centering%
  \subcaptionbox{BURS grammar\labelFigure{burs-example-grammar}}%
                {%
                  \figureFont\figureFontSize%
                  \begin{tabular}{rr@{$\; \rightarrow \;$}l}
                    \toprule
                        \tabhead \#
                      & \multicolumn{2}{c}{\tabhead pattern}\\
                    \midrule
                        1
                      & $\mNT{R}$ & $\mNT{Op}$ $\mNT{A}$ $\mNT{A}$\\
                        2
                      & $\mNT{R}$ & \cCode{r}\\
                        3
                      & $\mNT{R}$ & $\mNT{A}$\\
                        4
                      & $\mNT{A}$ & $\mNT{R}$\\
                        5
                      & $\mNT{A}$ & \cCode{c}\\
                        6
                      & $\mNT{A}$ & \cCode{$+$} \cCode{c} $\mNT{R}$\\
                        7
                      & \cCode{c}  & \cCode{0}\\
                        8
                      & $\mNT{X}$ & \cCode{$+$} $\mNT{X}$ \cCode{0}\\
                        9
                      & \cCode{$+$} $\mNT{Y}$ $\mNT{X}$ & \cCode{$+$}
                        $\mNT{X}$ $\mNT{Y}$\\
                        10
                      & $\mNT{Op}$ $\mNT{X}$ $\mNT{Y}$ & \cCode{$+$} $\mNT{X}$
                        $\mNT{Y}$\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \subcaptionbox{%
                  Example of an \glsentrytext{LR graph} based on the expression
                  tree \mbox{\cCode{$+$} \cCode{0} \cCode{$+$} \cVar{c}
                    \cVar{c}} and the grammar on the left-hand side.
                  %
                  Dashed nodes represent subtrees of the expression tree and
                  fully drawn nodes represent goals.
                  %
                  Edges indicate rule applications, with the number of the
                  applied rule appearing next to the edge%
                  \labelFigure{burs-example-lr-graph}%
                }%
                [80mm]%
                {%
                  \input{figures/tree-covering/burs-example-lr-graph}%
                }

  \caption[Example of a BURS grammar and an LR~graph]%
          {%
            Example of a BURS grammar and an
            LR~graph~\cite{Pelegri-LlopartGraham:1988}%
          }
  \labelFigure{burs-example}
\end{figure}

Using \gls{BURS} theory \citeauthor{Pelegri-LlopartGraham:1988} developed an
algorithm that computes the tables needed for \gls{optimal.ps} \gls{pattern
  selection} based on a given \gls{BURS grammar}.
%
The idea is as follows.
%
For a given \gls{expression tree}~$T$\!, a \gls!{LR graph} is formed where each
\gls{node} represents a specific \gls{subtree} appearing in~$T$ and each
\gls{edge} indicates the application of a particular \gls{rewrite.r} \gls{rule}
on that \gls{subtree} (an example is shown in
\refFigure{burs-example-lr-graph}).
%
Setting some \glspl{node} as goals (that is, the desired results of \gls{tree
  rewriting}), a subgraph called the \gls!{UI LR graph} is then selected from
the \gls{LR graph} such that the number of rewrite possibilities is minimized.
%
Each \gls{UI LR graph} then corresponds to a \gls{state}, and by generating all
\glspl{LR graph} for all possible \glspl{expression tree} that can be given as
input, we can find all the necessary \glspl{state}.
%
Since finding a \gls{UI LR graph} is an NP-complete problem,
\citeauthor{Pelegri-LlopartGraham:1988} applied a heuristic that iteratively
removes \glspl{node} which are deemed ``useless'' until a \gls{UI LR graph} is
achieved.


\paragraph{Achieving a Bounded Number of States}

To achieve \gls{optimal.ps} \gls{pattern selection}, the \glspl{LR graph} are
augmented such that each \gls{node} no longer represents a \gls{pattern tree}
but a \mbox{$(p\hspace{-.8pt}, c)$}~pair, where $c$ denotes the minimal cost of
covering the corresponding \gls{subtree} with
\gls{pattern}~\mbox{$p$\hspace{-.8pt}.}
%
This is the information embodied by the \glspl{state} as discussed earlier.
%
A naive approach would be to include the \emph{full} cost of reaching a
particular \gls{pattern} into the \gls{state}, but depending on the rewrite
system this may require an infinite number of \glspl{state}.
%
An example where this occurs is given in
\refFigure{burs-state-explosion-example-unbounded}.

\begin{figure}
  \subcaptionbox{%
                  A BURS grammar that may lead to an unbounded number of
                  states%
                  \labelFigure{burs-state-explosion-example-grammar}%
                }%
                {%
                  \figureFont\figureFontSize%
                  \begin{tabular}{cr@{$\; \rightarrow \;$}lc}
                    \toprule
                        \tabhead \#
                      & \multicolumn{2}{c}{\tabhead pattern}
                      & \tabhead cost\\
                    \midrule
                        1
                      & $\mNT{R}$
                      & \cCode{var}
                      & 1\\
                        2
                      & $\mNT{R}$
                      & \irCode{\irLoadText} $\mNT{R}$
                      & 1\\
                    \bottomrule
                  \end{tabular}%
                }%
  \hfill%
  \subcaptionbox{%
                  Expression tree labeled with states that incorporate the
                    full cost (requires \mbox{$N + 1$} states)%
                  \labelFigure{burs-state-explosion-example-unbounded}%
                }%
                [37mm]%
                {%
                  \input{%
                    figures/tree-covering/%
                    burs-state-explosion-example-unbounded%
                  }%
                }%
  \hfill%
  \subcaptionbox{%
                  Same tree but labeled with states that incorporate the
                    delta costs (requires only two states)%
                  \labelFigure{burs-state-explosion-example-bounded}%
                }%
                [37mm]%
                {%
                  \input{%
                    figures/tree-covering/%
                    burs-state-explosion-example-bounded%
                  }%
                }

  \caption[Example of state explosion]%
          {%
            Example illustrating how incorporating costs into states can result
            in an infinite number of states~\cite{Pelegri-LlopartGraham:1988}%
          }
  \labelFigure{burs-state-explosion-example}
\end{figure}

A better method is to instead account for the \emph{relative} cost of a selected
\gls{pattern}.
%
This is achieved by computing~$c$ as the difference between the cost of~$p$ and
the smallest cost associated with any other \gls{pattern} appearing in the
\gls{LR graph}.
%
This yields the same \gls{optimal.ps} \gls{pattern selection} but the number of
needed \glspl{state} is bounded, as seen in
\refFigure{burs-state-explosion-example-bounded}.
%
This cost is called the \gls!{delta cost} and the augmented \gls{LR graph} is
thus known as a \gls!{d-LR graph}.
%
To limit the memory footprint when generating the \glspl{d-LR graph},
\citeauthor{Pelegri-LlopartGraham:1988} used an extension of
\citeauthor{Chase:1987}'s table compression algorithm~\cite{Chase:1987} (which
we discussed in \refSection{tc-table-compression}).

During testing, \citeauthor{Pelegri-LlopartGraham:1988} reported that their
implementation yielded \gls{state} tables only slightly larger than those
produced by \gls{LR parsing}.
%
They also reported that it generated \gls{assembly code} of quality comparable
to \gls{Twig}'s but at a rate that was about five times faster.


\paragraph{BURS $\nLeftrightarrow$ Offline Cost Analysis}

Since \citeauthor{Pelegri-LlopartGraham:1988}'s 1988 paper, many later
publications mistakenly associate to the idea of \gls{offline cost analysis}
with~\gls{BURS} theory, typically using terms like \glspl!{BURS state}, when
these two aspects are in fact orthogonal to each other.
%
Although the work by \citeauthor{Pelegri-LlopartGraham:1988} undoubtedly led to
making \gls{offline cost analysis} an established aspect of modern
\gls{instruction selection}, the application of \gls{BURS} theory is only
\emph{one} means to achieving \gls{optimal.ps} \gls{pattern selection} using
tables.

For example, in 1990 \textcite{BalachandranEtAl:1990} introduced an alternative
method for generating the \glspl{state} that is both simpler and more efficient
than that of \citeauthor{Pelegri-LlopartGraham:1988}.
%
At its heart their algorithm iteratively creates new \glspl{state} using those
already committed to appear in the \gls{state} tables.
%
Remember that each \gls{state} represents a combination of \glspl{nonterminal},
\glspl{rule}, and costs, where the costs have been normalized such that the
lowest cost of any \gls{rule} appearing in that \gls{state} is~\num{0}.
%
Hence two \glspl{state} are identical if the \glspl{rule} selected for all
\glspl{nonterminal} and costs are the same.
%
Before a new \gls{state} is created it is first checked whether it has already
been seen -- if not, then it is added to the set of committed \glspl{state} --
and the process repeats until no new \glspl{state} can be created.
%
We will go into more detail shortly.

Compared to \citeauthor{Pelegri-LlopartGraham:1988}, this algorithm is less
complicated and also faster as it directly generates a smaller set of
\glspl{state} instead of first enumerating all possible \glspl{state} and then
reducing them.
%
In addition, \citeauthor{BalachandranEtAl:1990} expressed the
\glspl{instruction} as a more traditional \gls{machine grammar} -- like those
used in the \gls{Glanville-Graham approach} -- instead of as a \gls{BURS
  grammar}.


\paragraph{A Work Queue Approach for State Table Generation}

Another \gls{state}-generating algorithm similar to the one by
\citeauthor{BalachandranEtAl:1990} was proposed by
\citeauthor{Proebsting:1992:BURS}~\cite{Proebsting:1992:BURS,
  Proebsting:1995:BURS}.
%
This algorithm was also implemented by \textcite{FraserEtAl:1992:Burg} in a
renowned \gls{code generation} system called \gls!{Burg}.\!%
%
\footnote{%
  The keen reader will notice that \citeauthor{FraserEtAl:1992:IBurg} also
  implemented the \gls{DP}-based system \gls{IBurg} which was introduced in
  \refSection{tc-iburg}.
  %
  The connection between the two is that \gls{IBurg} began as a testbench for
  the \gls{grammar} specification to be used as input to \gls{Burg}.
  %
  \citeauthor{FraserEtAl:1992:Burg} later recognized that some of the ideas for
  the testbench showed some merit in themselves, and therefore improved and
  extended them into a stand-alone generator.
  %
  Unfortunately the authors neglected to say in their papers what these acronyms
  stand for.
  %
  The author's tentative guess is that \gls{Burg} was derived from the
  \gls{BURS} acronym and stands for \emph{Bottom-Up Rewrite Generator}.%
}
%
Since its publication in 1992, the paper has sparked a naming convention within
the \gls{compiler} community which we call the \gls!{BURGer phenomenon}.\!%
%
\footnote{%
  During the research for this dissertation, the author came across the
  following systems, all with equally creative naming schemes:
  %
  \begin{inlinelist}[itemjoin={, }, itemjoin*={, and}]
    \item \gls{Burg}~\cite{FraserEtAl:1992:Burg}
    \item \gls{CBurg}~\cite{ScharwaechterEtAl:2007}
    \item \gls{DBurg}~\cite{Ertl:1999}
    \item \gls{GBurg}~\cite{FraserProebsting:1999}
    \item \gls{GPBurg}~\cite{Gough:2012}
    \item \gls{HBurg}~\cite{HBURG}
    \item \gls{IBurg}~\cite{FraserEtAl:1992:IBurg}
    \item \gls{JBurg}~\cite{JBURG}
    \item \gls{LBurg}~\cite{HansonFraser:1995}
    \item \gls{MBurg}~\cite{Gough:1995, GoughLedermann:1997}
    \item \gls{OCamlBurg}~\cite{OCamlBURG}
    \item \gls{WBurg}~\cite{ProebstingWhaley:1996}
  \end{inlinelist}.%
}
%
Although \citeauthor{BalachandranEtAl:1990} were first, we will continue with
studying \citeauthor{Proebsting:1992:BURS}'s algorithm as it is better
documented.
%
More details are also available in \citeauthor{Proebsting:1992:Thesis}'s
doctoral dissertation~\cite{Proebsting:1992:Thesis}.

The idea for computing the \glspl{state} -- which will only be described briefly
-- works as follows.
%
For each \gls{terminal} representing a $k$-argument \gls{operation}, an
\mbox{$k$-dimensional} matrix is maintained.
%
This is called the \gls{terminal}'s \gls!{state table}, which indicates the
state to assign such \glspl{node} given the labels of its \glspl{child}.
%
First the states for all leaf \glspl{node} are built, considering only
\gls{base.r} \glspl{rule} with a single \glspl{terminal} on the right-hand side
in the \gls{production}.
%
The costs and \gls{rule} decisions are computed using the same logic as in
\refAlgorithm{aho-etal-cost-algorithm-2}, lines~\mbox{\num{8}--\num{21}}.
%
The leaf \gls{state} are then pushed onto a queue.
%
Each popped \gls{state} is used as the $i$th \gls{child} to all \gls{base.r}
\glspl{rule} with a non-leaf \glspl{terminal} in combination with all other
existing \glspl{state} (see \refFigure{new-state-creation}).
%
\begin{figure}
  \centering%
  \input{figures/tree-covering/new-state-creation}

  \caption{Creation of a new state}
  \labelFigure{new-state-creation}
\end{figure}
%
If any combination gives rise to a new set of costs and \gls{rule} decisions,
then a new \gls{state} is created and pushed onto the queue after having updated
the \glspl{state table}.
%
This process continues until the queue is empty, whereupon all necessary
\glspl{state} have been built.


\paragraph{Further Improvements}

The time required to generate the \gls{state} tables can be decreased if the
number of committed \glspl{state} can be minimized.
%
According to \textcite{Proebsting:1992:BURS}, the first attempts to do this were
made by \textcite{Henry:1989}, whose methods were later improved and generalized
by \citeauthor{Proebsting:1992:BURS}~\cite{Proebsting:1992:BURS,
  Proebsting:1995:BURS}.
%
\citeauthor{Proebsting:1992:BURS} developed two methods for reducing the number
of generated \glspl{state}:
%
\begin{inlinelist}[itemjoin={; }, itemjoin*={; and}]
  \item \gls!{state trimming}, which extends and generalizes
    \citeauthor{Henry:1989}'s ideas
  \item a new technique called \gls!{chain rule trimming}
\end{inlinelist}.
%
Without going into details, \gls{state trimming} increases the likelihood that
two created \glspl{state} will be identical by removing the information about
\glspl{nonterminal} that can be proven to never take part in a least-cost
covering.
%
\Gls{chain rule trimming} then further minimizes the number of \glspl{state} by
attempting to use the same \glspl{rule} whenever possible.
%
This technique was later improved by
\citeauthor{KangChoe:1995}~\cite{KangChoe:1995, Kang:2004}, who exploited
properties of common \glspl{machine description} to decrease the amount of
redundant \gls{state} testing.


\paragraph{More Applications}

The approach of extending \gls{pattern selection} with \gls{offline cost
  analysis} has been applied in numerous \gls{compiler}-related systems.
%
Some notable applications that we have not already mentioned include
\gls!{UNH-Codegen}~\cite{HatcherTuller:1988},
\gls!{DCG}~\cite{EnglerProebsting:1994}, \gls!{LBurg}~\cite{HansonFraser:1995},
and \gls!{WBurg}~\cite{ProebstingWhaley:1996}.
%
\Gls{Burg} is also available as a \gls{Haskell} clone called
\gls!{HBurg}~\cite{HBURG}, and has been adapted by \textcite{Boulytchev:2007} to
assist \gls{instruction set} selection.
%
\gls{LBurg} was developed to be used in the \gls!{LCC}~\cite{HansonFraser:1995},
and was adopted by \textcite{BrandnerEtAl:2007} in designing an architecture
description language from which the \glspl{instruction} can automatically be
inferred.
%
\gls{LBurg} was also extended by \textcite{FarfelederEtAl:2006} to support
certain \gls{multi-output.ic} \glspl{instruction} by adding an additional,
handwritten pass in the \gls{pattern matcher}.


\subsection{Generating States Lazily}

The two main approaches for achieving \gls{optimal.ps} \gls{pattern selection}
-- those that dynamically compute the costs as the \gls{function} is compiled,
and those that rely on statically computed costs via \gls{state} tables -- both
have their respective advantages and drawbacks.
%
The former have the advantage of being able to support dynamic costs (meaning
the \gls{pattern} cost is not fixed but depends on the context), but they are
also considerably slower than their purely table-driven counterparts.
%
The latter yield faster but larger \glspl{instruction selector} due to the use
of \gls{state} tables, which are also very time-consuming to generate -- for
pathological \glspl{grammar} this may even be infeasible -- and they only
support \gls{grammar} \glspl{rule} with fixed costs.


\paragraph{Combining the Best of State Tables and DP}

In 2006, \textcite{ErtlEtAl:2006} introduced a method that allows the
\gls{state} tables to be generated lazily and on demand.
%
The intuition is that instead of generating the \glspl{state} for \emph{all}
possible \glspl{expression tree} in advance, one can get away with only
generating the \glspl{state} needed for the \glspl{expression tree} that
actually appear in the \gls{function}.

The scheme can be outlined as follows.
%
As the \gls{instruction selector} traverses a \gls{expression tree}, the
\glspl{state} required for covering its \glspl{subtree} are created using
dynamic programming.
%
Once the \glspl{state} have been generated, the \gls{subtree} is labeled and
\glspl{pattern} are selected using the familiar table-driven techniques.
%
Then, if an identical \gls{subtree} is encountered elsewhere -- either in the
same \gls{expression tree} or in another \gls{tree} of the \gls{function} -- the
same \glspl{state} can be reused.
%
This allows the cost of \gls{state} generation to be amortized as the
\gls{subtree} can now be optimally covered faster than if it had been processed
using a purely \gls{DP}-based \gls{pattern selector}.
%
\citeauthor{ErtlEtAl:2006} reported the overhead of \gls{state} reuse was
minimal compared to purely table-driven implementations.
%
They also reported that the time required to first compute the \glspl{state} and
then label the \glspl{expression tree} was on par with selecting \glspl{pattern}
using ordinary \gls{DP}-based techniques.
%
Moreover, by generating the \glspl{state} lazily it is possible to handle larger
and more complex \glspl{machine grammar} which otherwise would require an
intractable number of \glspl{state}.

\citeauthor{ErtlEtAl:2006} also extended this design to support dynamic costs by
recomputing and storing the \glspl{state} in hash tables whenever the costs at
the \gls{expression tree} \glspl{root} differ.
%
The authors noted that while this incurs an additional overhead, their
\gls{instruction selector} was still faster than a purely \gls{DP}-based
\gls{instruction selector}.


\section{Other Tree-Based Approaches}
\labelSection{tc-other-tree-based-approaches}

So far we have discussed the conventional methods of covering \glspl{tree}:
%
\begin{inlinelist}[itemjoin={, }, itemjoin*={, and}]
  \item \gls{LR parsing}
  \item top-down recursion
  \item dynamic programming
  \item the use of \gls{state} tables
\end{inlinelist}.
%
In this section we will look at other designs which also rely on \glspl{tree},
but solve the \gls{instruction selection} problem using alternative methods.


\subsection{Techniques Based on Formal Frameworks}

\paragraph{Homomorphisms and Inversion of Derivors}

In order to simplify the \glspl{machine description} and enable formal
verification, \textcite{GeigerichSchmal:1988} proposed in 1988 an algebraic
framework intended to support all aspects of \gls{code generation}, including
\gls{instruction scheduling} and \gls{register allocation}.
%
In brief terms \citeauthor{GeigerichSchmal:1988} reformulated the
\gls{instruction selection} problem into a ``problem of a hierarchic derivor,''
which essentially entails the specification and implementation of a mechanism
%
\begin{displaymath}
  \gamma: T(Q) \rightarrow T(Z),
\end{displaymath}
%
where $T(Q)$ and $T(Z)$ denote the term algebras for expressing \glspl{function}
in an intermediate language and \gls{target machine} language, respectively.
%
Hence $\gamma$ can be viewed as the resulting \gls{instruction selector}.
%
Most \glspl{machine description}, however, are typically expressed in terms
of~$Z$ rather than~$Q$.
%
We therefore view the machine specification as a \gls{homomorphism}
%
\begin{displaymath}
  \delta: T(Z) \rightarrow T(Q),
\end{displaymath}
%
and the task of an \gls{instruction selection}-generator is thus to derive
$\gamma$ by inverting~$\delta$.
%
Usually this is achieved by resorting to \gls{pattern matching}, but for optimal
\gls{instruction selection} the generator must also interleave the construction
of the inverse $\delta^{-1}$ with a \gls!{choice function}~$\xi$ whenever some
\mbox{$q \in T(Q)$} has several \mbox{$z \in T(Z)$} such that \mbox{$\delta(q) =
  z$}.
%
Conceptually this gives us the following functionality:
%
\begin{displaymath}
  \def\annotArrow#1{\xrightarrow{\makebox[6mm]{\small$#1$}}}
  T(Q) \annotArrow{\delta^{-1}} 2^{T(Z)} \annotArrow{\xi} T(Z).
\end{displaymath}
%
In the same paper, \citeauthor{GeigerichSchmal:1988} also demonstrate how some
other methods, such as \gls{tree parsing}, can be expressed using this
framework.
%
A similar scheme based on rewriting techniques was later proposed by
\citeauthor{DesplandEtAl:1990}~\cite{DesplandEtAl:1987, DesplandEtAl:1990} in an
implementation called \gls!{Pagode}~\cite{CanaldaEtAl:1995}.


\paragraph{Equational Logic}

Shortly after \citeauthor{GeigerichSchmal:1988}, \textcite{Hatcher:1991}
developed a design similar to that of \citeauthor{Pelegri-LlopartGraham:1988}
that relies on \gls!{equational logic}~\cite{ODonnell:1985} instead of
\gls{BURS} theory.
%
The two are closely related in that both apply a set of predefined \glspl{rule}
to rewrite the \gls{expression tree} into a single goal term.
%
However, an equational specification has the advantage that all such
\glspl{rule} -- which are derived from the \glspl{instruction} and axiomatic
transformations -- are based on a set of so-called \glspl!{built-in operation}.
%
Each \gls{built-in operation} has a cost and implicit semantics, expressed as
\gls{assembly code} emission.
%
The cost of a \gls{rule} is then equal to the sum of all \glspl{built-in
  operation} it applies, removing the need to set the \gls{rule} costs manually.
%
In addition, no \glspl{built-in operation} are predefined, but are instead given
as part of the equational specification, providing a very general mechanism for
describing \glspl{target machine}.
%
Experimental results with an implementation called \gls!{UCG}%
%
\footnote{%
  The paper does not say what this acronym stands for.%
}
show that it could, for a selected set of problems, generate \gls{assembly code}
of comparable quality to that of contemporary techniques but in less time.


\subsection{More Tree Rewriting-Based Methods}

We have already discussed numerous techniques which perform \gls{instruction
  selection} by rewriting the \gls{expression tree} such that it finally reaches
a particular goal.
%
For completeness we will in this section examine the remaining such designs, but
without going into much detail.


\paragraph{%
  Using Finite Tree Automata, Series Transducers, and Pushdown Automata%
}

\textcite{Emmelmann:1992:Rewriting} introduced in 1992 a technique that relies
on the theories of \glspl{finite tree automaton} (see for
example~\cite{GecsegSteinby:1984} for an overview), which was later extended by
\textcite{FerdinandEtAl:1994}.
%
In their 1994~paper, \citeauthor{FerdinandEtAl:1994} demonstrate how
\glspl{finite tree automaton} can be used to solve both \gls{pattern matching}
and \gls{pattern selection} -- greedily as well as \gls{optimal.ps}[ly] -- and
also present algorithms for how to produce these automata.
%
An experimental implementation demonstrated the feasibility of this technique,
but the results were not compared to those of other techniques.
%
Similar designs were later proposed by \textcite{Borchardt:2004} and
\textcite{JanousekJaroslav:2014}, who made use of \glspl!{tree series
  transducer} (see for example~\cite{EngelfrietEtAl:2001} for an overview) and
\glspl{pushdown automaton}, respectively.


\paragraph{Rewriting Strategies}

In 2002, \citeauthor{BravenboerVisser:2002} presented a design where
\gls{rule}-based \gls{function} transformation systems~\cite{Visser:2005} are
adapted to \gls{instruction selection}.
%
Through a system called \gls!{Stratego}~\cite{Visser:2001}, a \gls{machine
  description} can be augmented by \gls{pattern selection} strategies, allowing
the \gls{pattern selector} to be tailored to that particular \gls{target
  machine}.
%
\citeauthor{BravenboerVisser:2002} refer to this as providing a \gls{rewriting
  strategy}, and their system supports modeling of several strategies such as
exhaustive search, \gls{maximum munch}, and dynamic programming.
%
Purely table-driven techniques, however, do not seem to be supported at the time
of writing, which excludes the application of \gls{offline cost analysis}.
%
In their paper, \citeauthor{BravenboerVisser:2002} argue that this setup allows
several \gls{pattern selection} techniques to be combined, but they do not
provide an example of where this would be beneficial.


\subsection{Techniques Based on Genetic Algorithms}
\labelSection{tc-genetic-algorithms}

To solve the \gls{pattern selection} problem, \textcite{ShuEtAl:1996} employed
the theories of \glspl!{GA}, which mimic the process of natural selection (see
for example \cite{Reeves:2010} for an overview).\!%
%
\footnote{%
  On a related note, \textcite{WuLi:2006} applied \gls!{ant colony optimization}
  -- a meta-heuristic inspired by the shortest-path searching behavior of
  various ant species \cite{DorigoStutzle:2010} -- to improve overall code size
  by alternating between \glspl{instruction set} on a per-function basis.%
}
%
The idea is to formulate a solution as a string, called a \gls!{chromosome} (or
\gls!{gene}), and then mutate it in order to hopefully end up with a better
solution.
%
For a given \gls{expression tree} whose \glspl{match set} have been found using
an \mbox{$\mBigO(nm)$} \gls{pattern matcher}, \citeauthor{ShuEtAl:1996}
formulated each \gls{chromosome} as a binary bit string where a~\num{1}
indicates the selection of a particular \gls{pattern}.
%
Likewise, a~\num{0} indicates that the \gls{pattern} is not used in the
\gls{tree covering}.
%
The length of a \gls{chromosome} is therefore equal to the sum of the number of
\glspl{pattern} appearing in all \glspl{match set}.
%
The objective is then to find the \gls{chromosome} which maximizes a
\gls!{fitness function}~$f$\!, which \citeauthor{ShuEtAl:1996} defined as%
%
\def\numPats#1{p_{#1}}%
\def\multCov#1{n_{#1}}%
%
\begin{displaymath}
  f(c) = {\frac{1}{k * \numPats{c} + \multCov{c}}},
\end{displaymath}
%
where $k$ is a tweakable constant greater than~\num{1}, $\numPats{c}$ is the
number of selected \glspl{pattern} in the \gls{chromosome}~$c$, and
$\multCov{c}$ is the number of \glspl{node} in $c$ which are covered by more
than one \gls{pattern}.
%
Hence \glspl{pattern} are allowed to overlap in covering the \gls{expression
  tree}.
%
First, a fixed number of \glspl{chromosome} is randomly generated and evaluated.
%
The best ones are kept and subjected to standard \gls{GA}~operations -- such as
fitness-proportionate reproduction, single-point crossover, and one-bit
mutations -- in order to produce new \glspl{chromosome}, and the process repeats
until a termination criterion is reached.
%
The authors claim to be able to find \gls{optimal.ps} \gls{tree} covers in
``reasonable'' time for medium-sized \glspl{expression tree}, but these include
at most \num{50}~\glspl{node}.
%
Moreover, due to the nature of \glspl{GA}, optimality cannot be guaranteed for
all \glspl{expression tree}.
%
A similar technique was devised by \textcite{ErikssonEtAl:2008}, which also
incorporates \gls{instruction scheduling}, for generating \gls{assembly code}
for clustered \gls{VLIW} architectures.


\subsection{Techniques Based on Trellis Diagrams}
\labelSection{tc-trellis-diagrams-trees}

The last \gls{instruction selection} technique that we will examine in this
appendix is a rather unusual design by \citeauthor{Wess:1992}~\cite{Wess:1992,
  Wess:1995}.
%
Specifically targeting \glsdesc{DSP}[s], \citeauthor{Wess:1992}'s design
integrates \gls{instruction selection} with \gls{register allocation} through
the use of \glspl{trellis diagram}.

A \gls!{trellis diagram} is a graph where each \gls{node} consists of an
\gls!{OVA}.
%
\def\mTL{\mathit{TL}}%
\def\mRA{\mathit{RA}}%
%
An element in an \gls{OVA} represents that the data is stored either in
memory~(\cVar*{m}) or in a particular \gls{register}~(\cVar*{r}[x]), and its
value indicates the lowest accumulated cost from the \glspl{leaf} to the
\gls{node}.
%
An example is shown in \refTable{ova-example}, where $\mTL$ denotes the target
location of the data produced at a given \gls{node}, and $\mRA$ denotes the set
of \glspl{register} that may be used when producing the data.
%
\begin{table}
  \figureFont\figureFontSize%
  \begin{displaymath}
    \begin{array}{c}
      \toprule
        \begin{array}{@{}cccccc@{}}
              \mathtabhead{i}
            & \text{0}
            & \text{1}
            & \text{2}
            & \text{3}
            & \text{4} \\
          \cline{2-6}
              \text{\tabhead OVA}
            & \multicolumn{1}{|m{10mm}}{}
            & \multicolumn{1}{|m{10mm}}{}
            & \multicolumn{1}{|m{10mm}}{}
            & \multicolumn{1}{|m{10mm}}{}
            & \multicolumn{1}{|m{10mm}|}{} \\
          \cline{2-6}
              \mathtabhead{\mTL}
            & \text{\cVar{m}}
            & \text{\cVar{r}[1]}
            & \text{\cVar{r}[1]}
            & \text{\cVar{r}[2]}
            & \text{\cVar{r}[2]} \\
              \mathtabhead{\mRA}
            & \mSet{\text{\cVar{r}[1]}, \text{\cVar{r}[2]}}
            & \mSet{\text{\cVar{r}[1]}}
            & \mSet{\text{\cVar{r}[1]}, \text{\cVar{r}[2]}}
            & \mSet{\text{\cVar{r}[2]}}
            & \mSet{\text{\cVar{r}[1]}, \text{\cVar{r}[2]}}
        \end{array} \\
      \bottomrule
    \end{array}
  \end{displaymath}

  \caption[Example of an OVA]%
          {%
            Example of an OVA for a target machine with two registers
            \cVar*{r}[1] and \cVar*{r}[2]~\cite{Wess:1992}%
          }
  \labelTable{ova-example}
\end{table}
%
The cost is computed similarly as in the \gls{DP}-based techniques.
%
To facilitate the following discussion, let us denote by
\mbox{$\mTL(i\hspace{-.8pt}, n)$} and \mbox{$\mRA(i\hspace{-.8pt}, n)$} the
target location and set of available \glspl{register}, respectively, for the
$i$th element in the \gls{OVA} of \gls{node}~\mbox{$n$\hspace{-.8pt}.}

We create the \gls{trellis diagram} using the following scheme.
%
For each \gls{node} in the \gls{expression tree}, a new \gls{node} representing
an \gls{OVA} is added to the \gls{trellis diagram}.
%
For the \glspl{leaf} an additional \gls{node} is added in order to handle
situations where the values first need to be transferred to another location
before being used (this is needed for example if the value resides in memory).
%
Next we add the \glspl{edge}.
%
Let us denote by \mbox{$e(i\hspace{-.8pt}, n)$} the $i$th~element in the
\gls{OVA} of a \gls{node}~\mbox{$n$\hspace{-.8pt}.}
%
For a unary operation \gls{node}~$n$ with a
\gls{child}~\mbox{$m$\hspace{-.8pt},} we add an \gls{edge} between
\mbox{$e(i\hspace{-.8pt}, n)$} and \mbox{$e(j\hspace{-1pt}, m)$} if there exists
a sequence of \glspl{instruction} that implements the operation
of~\mbox{$n$\hspace{-.8pt},} stores the result in \mbox{$\mTL(i\hspace{-.8pt},
  n)$}, takes as input the value stored in \mbox{$\mTL(j\hspace{-1pt}, m)$,} and
exclusively uses the \glspl{register} in \mbox{$\mRA(i\hspace{-.8pt}, n)$.}
%
Similarly, for a binary operation \gls{node}~$o$ with two \glspl{child}~$n$
and~$m$, we add an \gls{edge} pair from \mbox{$e(i\hspace{-.8pt}, n)$} and
\mbox{$e(j\hspace{-1pt}, m)$} to \mbox{$e(k\hspace{-.8pt}, o)$} if there exists
a sequence of \glspl{instruction} that implements the operation of~$o$, stores
the result in \mbox{$\mTL(k\hspace{-.8pt}, o)$,} takes as input the two values
stored in \mbox{$\mTL(i\hspace{-.8pt}, n)$} and \mbox{$\mTL(j\hspace{-1pt},
  m)$,} and exclusively uses the \glspl{register} in
\mbox{$\mRA(k\hspace{-.8pt}, o)$.}
%
This can be generalized to \mbox{$n$-ary} operations.
%
An example is given in \refFigure{trellis-example}.

\begin{figure}
  \begin{minipage}[b]{40mm}
    \centering%
    \subcaptionbox{Expression tree\labelFigure{trellis-example-expr-tree}}%
                  [\linewidth]%
                  {%
                    \input{figures/tree-covering/trellis-example-expr-tree}%
                  }

    \vspace{\betweensubfigures}

    \subcaptionbox{%
                    Instruction set.
                    %
                    All instructions are assumed to have equal cost.
                    %
                    Note that this is not a machine grammar%
                    \labelFigure{trellis-example-instruction-set}%
                  }%
                  [\linewidth]%
                  {%
                    \begin{tabular}{rr@{$\; \leftarrow \;$}l}
                      \toprule
                           \tabhead \#
                         & \multicolumn{2}{c}{\tabhead production}\\
                      \midrule
                          1
                        & \cVar{r}[1]
                        & \cCode{\cVar{r}[1] \irMulText{} \cVar{r}[2]}\\
                          2
                        & \cVar{r}[1]
                        & \cCode{\cVar{r}[1] \irMulText{} \cVar{m}}\\
                          3
                        & \cVar{r}[1]
                        & \cCode{\cVar{r}[1] \irSubText{} \cVar{m}}\\
                          4
                        & \cVar{r}[1]
                        & \cCode{\irSubText{} \cVar{r}[1]}\\
                          5
                        & \cVar{r}[1]
                        & \cVar{m}\\
                          6
                        & \cVar{r}[2]
                        & \cVar{m}\\
                          7
                        & \cVar{m}
                        & \cVar{r}[1]\\
                          8
                        & \cVar{m}
                        & \cVar{r}[2]\\
                          9
                        & \cVar{r}[2]
                        & \cVar{r}[1]\\
                          10
                        & \cVar{r}[1]
                        & \cVar{r}[2]\\
                      \bottomrule
                    \end{tabular}%
                  }%
  \end{minipage}%
  \hfill%
  \subcaptionbox{%
                  Trellis diagram.
                  %
                  Gray edges represent available paths, and black edges indicate
                  the (selected) optimal path%
                  \labelFigure{trellis-example-diagram}%
                }%
                [78mm]%
                {%
                  \input{figures/tree-covering/trellis-example-diagram}%
                }

  \caption[Example of Trellis diagram]%
          {%
            A trellis diagram corresponding to an expression
            \mbox{\cCode{\irSubText{}(a \irMulText{} b) \irSubText{} c}} for a
            two-register target machine.
            %
            The variables~\cVar{a}, \cVar{b}, and~\cVar{c} are assumed to
            initially be stored in memory.
            %
            Note that two instructions are selected for the root as the result
            is also required to be stored in memory~\cite{Wess:1992}%
          }
  \labelFigure{trellis-example}
\end{figure}

The \glspl{edge} in the \gls{trellis diagram} thus correspond to the possible
combinations of \glspl{instruction} and \glspl{register} that implement a
particular operation in the \gls{expression tree}.
%
A path from every \gls{leaf} in the \gls{trellis diagram} to its \gls{root} thus
represents a selection of such combinations.
%
By keeping track of the costs, we can get the optimal \gls{instruction} sequence
by selecting the path which ends at the \gls{OVA}~element with the lowest cost
in the \gls{root} of the \gls{trellis diagram}.

The strength of \citeauthor{Wess:1992}'s design is that \glspl{target machine}
with asymmetric \glspl{register class} -- where different \glspl{instruction}
are needed for accessing different \glspl{register} -- are easily handled as
\gls{instruction selection} and \gls{register allocation} is done
simultaneously.
%
The drawback is that the number of \glspl{node} in the \gls{trellis diagram} is
exponential in the number of \glspl{register}.
%
This problem was mitigated by \textcite{FrohlichEtAl:1999}, who augmented the
algorithm to build the \gls{trellis diagram} in a lazy fashion.
%
However, both schemes nonetheless require a \mbox{1-to-1} mapping between the
\glspl{node} in a \gls{trellis diagram} and the \glspl{instruction} in order to
be effective.

This, in combination of how \glspl{instruction} are selected, makes one wonder
whether these techniques actually conform to the \glspl{principle} of
\glsshort{tree covering} and \gls{DAG covering}.
%
The author certainly struggled with deciding how to categorize them, and finally
opted against creating a separate \gls{principle}, as that would indeed be a
very short appendix.


\section{Limitations of Tree Covering}
\labelSection{tc-limitations}

While \gls{tree covering} enables use of more complex \glspl{pattern} compared
to \gls{macro expansion}, \gls{tree covering} has several disadvantages of its
own.

% LAYOUT FIX:
% Prevent paragrah below from being broken across two pages
\newpage

\begin{filecontents*}{tree-limitation-example.c}
x = a + b
y = x + x
\end{filecontents*}
%
\begin{inParFigure}{17mm}[r]
  \centering

  % LAYOUT FIX:
  % Put figure in the middle of paragraph
  \vspace{1.25\baselineskip}

  \begin{lstpage}{\linewidth}%
    \lstinputlisting{tree-limitation-example.c}%
  \end{lstpage}%
\end{inParFigure}%
%
The first disadvantage of \glspl{tree} has to do with expression modeling.
%
Due to the definitions of \glspl{tree}, common subexpressions cannot be properly
modeled in a \gls{expression tree}.
%
For example, the inlined code cannot be modeled directly without applying one of
the following workarounds:
%
% LAYOUT FIX:
% Prevent the second item from being split across two pages
\enlargethispage{.8ex}
%
\begin{enumerate}
  \item Repeating the shared operations, which in \gls{Polish notation} results
    in
    \begin{center}
      \irCode*{$=$ y $+$ $+$ a b $+$ a b}.
    \end{center}
  \item Splitting the expression, which results in
    \begin{center}
      \irCode*{$=$ x $+$ a b}\\
      \irCode*{$=$ y $+$ x x}.
    \end{center}
\end{enumerate}

% LAYOUT FIX:
% The lower paragraph cannot be joined with the one above or else the in-par
% figure will interfere (for some reason)

\noindent
The first approach leads to additional \glspl{instruction} in the \gls{assembly
  code}, while the second hinders the use of more complex \glspl{instruction}.
%
Hence code quality is compromised in both cases.

The second disadvantage is limited \gls{instruction set} support.
%
For example, since \glspl{tree} only allow a single \gls{root},
\gls{multi-output.ic} \glspl{instruction} cannot be represented as
\glspl{pattern tree} as such \glspl{instruction} would require multiple
\glspl{root}.
%
Even \gls{disjoint-output.ic} \glspl{instruction}, where each individual
operation can be modeled as \glspl{tree}, cannot be selected because \gls{tree
  covering}-based \glspl{instruction selector} can only consider a single
\gls{pattern tree} at a time.

The third disadvantage is that \glspl{expression tree} typically cannot model
control flow.
%
For example, a for~loop statement requires a cyclic \gls{edge} between
\glspl{block}, which violates the definition of \glspl{tree}.
%
For this reason, \gls{tree}-based \glspl{instruction selector} are limited to
selecting \glspl{instruction} for a single \gls{expression tree} at a time,
which is known as \gls!{local.is}[ \gls{instruction selection}].
%
Moreover, handling of control flow must be done separately, which excludes
\glsshort{matching problem} and \glsshort{selection problem} of
\gls{inter-block.ic} \glspl{instruction}, whose behavior incorporates control
flow.

To summarize, although the \gls{principle} of \gls{tree covering} greatly
improves code quality over the \gls{principle} of pure \gls{macro expansion}
(ignoring \gls{peephole optimization}, that is), the inherent restrictions of
\glspl{tree} prevent exploitation of many \glspl{instruction} provided by modern
\glspl{target machine}.


\section{Summary}
\labelSection{tc-summary}

In this appendix, we have looked at numerous techniques that are based on the
\gls{principle} of \gls{tree covering}.
%
In contrast to \gls{macro expansion}, \gls{tree covering} enables use of more
complex \glspl{pattern}, allowing a wider range of \glspl{instruction} to be
selected.
%
By applying \glsdesc{DP}, \gls{optimal.ps} covers can be found in linear time,
thereby further improving the quality of the generated \gls{assembly code}.
%
Several techniques also incorporate \gls{offline cost analysis} into the
\gls{instruction selector} generator to reduce compilation time.
%
In other words, this kind of implementation is very fast and efficient while
also supporting a wide array of \glspl{target machine}.
%
Consequently, \gls{tree covering} has become the most known -- although perhaps
no longer the most applied -- \gls{principle} of \gls{instruction selection}.

Restricting oneself to \glspl{tree}, however, has several inherent
disadvantages, and in the next appendix we will look at a more general
\gls{principle} that addresses some of these issues.
